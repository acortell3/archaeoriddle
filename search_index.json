[["index.html", "Archaeoriddle, the bookdown Preface", " Archaeoriddle, the bookdown by the Computational and Digital Archaeology Laboratory from Cambridge University last compiled: 2024-11-06 17:14:55 Preface This bookdown has two main objectives: documents all the steps, models, functions and results done during the archaeoriddle project generate your own fake world with specific parameters and a fake archaeological records To use this book you can follow it on this online version and copy paste the chunks on your local computer, or you can clone the source at the git repository here and compile the bookdown on your local machine. Dependencies If you want this to be successful, you will need to install a few additional packages by running the command in the chunk below, which will check if all packages are installed, install them if not and then load them. needed=c(&quot;terra&quot;,&quot;sf&quot;,&quot;igraph&quot;,&quot;devtools&quot;) success=sapply(needed,require,character.only=T) sapply(needed[!success],install.packages) ## named list() success=sapply(needed,require,character.only=T) terra::terraOptions(progress=0) #by default terra show a progress bar when applying function to large raster, this messes with our output so we set it to zero to remove it. These packages will very likely need other packages that will depend on your operating system, so check the error messages from the previous commands. You will also need to install and load the package associated with the bookdown by doing devtools::install_github(&quot;acortell3/archaeoriddle&quot;) library(archaeoriddle) Compiling the book If you want to compile the book yourself, you will need bookdown and a few more packages used to render the book. But first, you will need to clone the repository: git clone https://github.com/acortell3/archaeoriddle/ cd archaeoriddle Once done you can then load the package using the local source: devtools::load_all() ## ℹ Loading archaeoriddle And then install the specific packages to render the book. compneeded=c(&quot;DiagrammeR&quot;,&quot;bookdown&quot;,&quot;igraph&quot;,&quot;gifski&quot;) success=sapply(compneeded,require,character.only=T) sapply(needed[!success],install.packages) success=sapply(compneeded,require,character.only=T) Using load_all() instead of install(...) will allow you to modify the source code and reload the package after any change, thus making test and integration of your own implementation way easier. Once the package has been loaded or installed, you can then render the book locally. Note that this may take a long time as it generate a full new world, and run a couple of simulations and examples. bookdown::render_book() #you need to be in the folder &#39;doc/bookdown/&#39; "],["introduction.html", "1 Introduction 1.1 What is archaeoriddle? 1.2 What will you find in this bookdown? 1.3 What can you do with this?", " 1 Introduction 1.1 What is archaeoriddle? Archaeoriddle is a collaborative project at the Computational and Digital Archaeology Laboratory (CDAL) from the Department of Archaeology, University of Cambridge. The project started with an idea to develop an ABM to simulate an interaction process in the imaginary world of Rabbithole between two fictitious groups: the hunter-gatherers called rabbitskinners and the farmers called poppychewers. Through this world, the project could explore archaeological use of data and methods in a controlled environment to learn more about the act of research. The project has also produced a large amount of media content for dissemination and engagement, most of which can be found on the various social accounts linked to the project (here or here). There you will also find all of the information related to the project (or most of it anyway), but in brief, the idea behind it is to create a virtual world where we, as developers, know what happened, but where only partial data are available to the public. With this, there are then some questions which interested participants have to answer with their own methods. In particular, we have created a map with 100 grid squares, of which only five were shared commonly, and each participant had the right to request five more squares of their own preference. Additionally, all data had gone through processes of creation and loss, emulating the archaeological record. The aim was that each researcher tried to understand what happened so that the results of the different proposed methodologies could be compared. The project has been presented at several conferences (dedicated presentations at CAA conferences 2022 and 2023, as well as mentioned in several others), culminating with a thought-provoking discussion at the dedicated workshop in the European Association of Archaeologists conference in Belfast (2023) where interested participants presented their proposals. 1.2 What will you find in this bookdown? This bookdown document provides an explanation of the model used in the creation of the project. Everything has been developed using R. By downloading the package “archaeoriddle” from GitHub, you can reproduce the world of the Rabbithole or adapt it to create your own using the code provided in this bookdown document. This might be useful not only to create a virtual world from scratch while attending to different archaeological constraints, but also provides an excellent exercise to practice making R-based ABMs. Finally, all the data, this includes the data that we produceed (including what’s on the website) or the one that you produce from this is free, and you are free modify to suit your differing needs. We have structured the following chapters as follows: Introduction: this introduction Chapter 2 Landscape &amp; resources: functions and methods to create ‘realistic’ artificial environments. Chapter 3 Population structure &amp; growth: functions &amp; methods to initialise sites location, population structure and growth. Chapter 4 Migration &amp; conflicts: functions &amp; methods that handle movements of populations and their interactions. Chapter 5 Main simulation: the main function that wrap together the previous elements to simulate the interactions processes. Chapter 6 Record formation: functions and methods that generate archaeological record given the population of each site generated during the main simulation. Chapter 7 Record loss: functions and methods to simulate loss of the record through time. Chapter 8 Generate Final archaeological record: functions and methods that wrap together the main simulation, the record formation and loss, and generate shareable files for people to explore Chapter 9 Archaoriddle: the original challenge: The full description of the parameters, runs and data generated, as well as the 5 proposal gather for the original challenge 1.3 What can you do with this? Well, many things! We have mentioned that the data generated in the original challenge (which can be freely downloaded here) can be used for your own research purposes or as an educational tool. We encourage you to use the code in this bookdown to generate your own Rabbithole and pose your own research questions with synthetic data. Additionally, this book is also a good document for the development of ABMs in R. All in all, our purpose is not only to bring awareness to the strengths and weaknesses of archaeological methodology, but also to provide material (e.g. methods, theory, and code) for interested researchers so that they can reproduce their own virtual worlds and do their own experiments. Thus, we are proposing a tool that can be used for research, but which is also an excellent testing ground for educational and training purposes. Download things and toy around! Don’t worry, you won’t break anything!! "],["env.html", "2 Landscape &amp; Resources 2.1 Autocorrelated noise generation 2.2 Slope and Elevation 2.3 Environmental resources 2.4 Manually Select Resources", " 2 Landscape &amp; Resources To simulate the interaction between two groups, we need an environment in which they can interact in. This chapter walks you through how to generate a landscape and an environment for your simulated groups to explore. An important part of the simulation is to generate a landscape and an environment. Environmental variable are often autocorrelated. For example, The temperature at a given time is never extremly different to the temperature slightly before and slightly after this time. Same is true with the slope, or the elevation of a place, 2.1 Autocorrelated noise generation An interesting way to generate such auto-correlated variable is to use coloured noise. This use specific distribution to define the relation between frequencies found in your signal with the power of these frequencies. In pink noise (or \\(\\frac{1}{f}\\)-noise) for example, the power of each frequencis will be invertly proportional to the frequencies (ie you will have, low frequencies with high power, high frequencies with low ones). We defined an environment function, which in turns uses TK95 to generate a power law noise. To generate a single auto-correlated variable (like temperature), who’s spectrum decomposition ( power wrt frequency) has a slope of -3 and a standard deviation of 1.5: tstep &lt;- 1000 faketemp &lt;- environment(tstep, omega=3, delta=1.5) + 1.5 plot(faketemp, type=&quot;l&quot;, ylab=&quot;temperature change&quot;, xlab=&quot;time&quot;,lwd=3) You can inspect the code of environment and TK95 functions here: Show code Code ## Function 10. Power law noise from Kimmer &amp; Koening TK95 &lt;- function(N, alpha = 1){ f &lt;- seq(from=0, to=pi, length.out=(N/2+1))[-c(1,(N/2+1))] # Fourier frequencies f_ &lt;- 1 / f^alpha # Power law RW &lt;- sqrt(0.5*f_) * rnorm(N/2-1) # for the real part IW &lt;- sqrt(0.5*f_) * rnorm(N/2-1) # for the imaginary part fR &lt;- complex(real=c(rnorm(1), RW, rnorm(1), RW[(N/2-1):1]), imaginary=c(0, IW, 0, -IW[(N/2-1):1]), length.out=N) # Those complex numbers that are to be back transformed for # Fourier Frequencies 0, 2pi/N, 2*2pi/N, ..., pi, ..., 2pi-1/N # Choose in a way that frequencies are complex-conjugated and symmetric around pi # 0 and pi do not need an imaginary part reihe &lt;- fft(fR, inverse=TRUE) # go back into time domain return(Re(reihe)) # imaginary part is 0 } ## Function 11. Environment generator environment &lt;- function(N, omega, delta, vt=NULL){ ts &lt;- TK95(N, omega) ts &lt;- delta * ts/sd(ts) if(!is.null(vt)) { ts = ts + vt*1:N } return(ts) } This generates autocorrelated time series that can be used to define temporal change in climate (rain, temperature, etc….). But we haven’t used such changes for the Archaeoriddle challenge. Coloured noise remains a very straightforward way to simulated factor and could be easily added in the simulations. 2.2 Slope and Elevation An important aspect of the simulation is a way to generate elevation and slope. To generate 2D autocorrelation we used a slightly methods known as perlin noise ; using the implementation found on this stackoverflow.com post. And you can see the code here: Show code Code ## Function 12. Perlin noise perlin_noise &lt;- function( n = 5, m = 7, N = 100, M = 100 ) { # For each point on this n*m grid, choose a unit 1 vector vector_field &lt;- apply( array( rnorm( 2 * n * m ), dim = c(2,n,m) ), 2:3, function(u) u / sqrt(sum(u^2)) ) f &lt;- function(x, y) { # Find the grid cell in which the point (x,y) is i &lt;- floor(x) j &lt;- floor(y) stopifnot( i &gt;= 1 || j &gt;= 1 || i &lt; n || j &lt; m ) # The 4 vectors, from the vector field, at the vertices of the square v1 &lt;- vector_field[, i, j] v2 &lt;- vector_field[, i+1, j] v3 &lt;- vector_field[, i, j+1] v4 &lt;- vector_field[, i+1, j+1] # Vectors from the point to the vertices u1 &lt;- c(x,y) - c(i, j) u2 &lt;- c(x,y) - c(i+1, j) u3 &lt;- c(x,y) - c(i, j+1) u4 &lt;- c(x,y) - c(i+1, j+1) # Scalar products a1 &lt;- sum( v1 * u1 ) a2 &lt;- sum( v2 * u2 ) a3 &lt;- sum( v3 * u3 ) a4 &lt;- sum( v4 * u4 ) # Weighted average of the scalar products s &lt;- function(p) 3 * p^2 - 2 * p^3 p &lt;- s( x - i ) q &lt;- s( y - j ) b1 &lt;- (1-p)*a1 + p*a2 b2 &lt;- (1-p)*a3 + p*a4 (1-q) * b1 + q * b2 } xs &lt;- seq(from = 1, to = n, length = N+1)[-(N+1)] ys &lt;- seq(from = 1, to = m, length = M+1)[-(M+1)] return(outer( xs, ys, Vectorize(f) )) } Various parameters for the Perlin noise will result in either a higher or lower level of definition for our future DEM raster. Our aim is to find a balance between having the environment sufficiently detailed across a relatively wide area, while also ensuring it remains manageable in terms of memory usage. require(terra) require(sf) a &lt;- 0.6 k &lt;- 8 m &lt;- perlin_noise(2, 2, 2^k, 2^k) for ( i in 2:k ) m &lt;- m + a^i * perlin_noise(2^i, 2^i, 2^k, 2^k) If we change k (that dictates the total resolution of the DEM), the size of the raster grow explonentially and would be quickly hard to manage. allsizes &lt;- sapply(1:7,function(k) { print(k) mt &lt;- perlin_noise(2, 2, 2^k, 2^k) for ( i in 2:k ) mt &lt;- mt + 0.6^i * perlin_noise(2^i, 2^i, 2^k, 2^k) object.size(mt) }) ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 plot(2^(1:7),allsizes/1000,type=&quot;b&quot;,pch=20,col=&quot;red&quot;,cex=2,ylab=&quot;size of raster (kb)&quot;, xlab=&quot;resolution&quot;) To enhance realism, slight adjustments were made to the extremes in order to create seas and mountains that more closely resemble those found in an real DEM. height &lt;- (m[,])*20 #height[height&lt;50]=0 height &lt;- height^3 height &lt;- height+abs(mean(height)) height[height&lt;min(height)*.25] &lt;- min(height)*.25 We can then visualise the result as a 2D raster. We setup the total extent of the raster to define exactly where it is on our GIS using a latlon projection. We also define the mean high as the water level, all the thing below are then considered under water. height.ras &lt;- rast(height, extent=c(-3.5, 0.5, -0.5, 3.5), crs=&quot;+proj=latlon&quot;) maxh &lt;- max(height.ras[], na.rm=T) plot(height.ras, col=col_ramp(20), legend=F, reset=F) height.wat &lt;- height.ras height.wat[height.wat&gt;mean(height.wat[])] &lt;- NA plot(height.wat, col=&quot;lightblue&quot;, add=T, legend=F) This has been put in a function plotMap plotMap(height.ras,height.wat) The actual raster used for the original archaeoriddle is available in data_original/east_narnia4x.tif and looks like original.ras &lt;- rast(&quot;data_original/east_narnia4x.tif&quot;) original.wat &lt;- original.ras original.wat[original.wat&gt;mean(original.wat[])] &lt;- NA plotMap(original.ras,original.wat) If you really want to feel like being in a video game, you can play with rayshader, but getting the right parameters to have a nice rendering is tricky, slow, and it’s not unlikely you’ll loose a lot of time ¯\\_(ツ)_/¯ require(rayshader) height_map &lt;- raster_to_matrix(raster::raster(height)) hm_shade &lt;- sphere_shade(height_map, texture = &quot;imhof1&quot;, zscale=5) hm_ray &lt;- add_shadow(hm_shade, ray_shade(height_map, zscale = 19), 0.8) hm_ambiant &lt;- add_shadow(hm_ray, ambient_shade(height_map), 0,5) hm_lamb &lt;- add_shadow(hm_ambiant, lamb_shade(height_map), 0) plot_3d( hm_lamb,height_map, zscale = 10, fov = 0, theta = 20, zoom = .5, phi = 45, windowsize = c(1000, 800),water = TRUE, waterdepth = 0, wateralpha = 1, watercolor = &quot;lightblue&quot;, waterlinecolor = &quot;lightblue&quot;, waterlinealpha = .7, baseshape=&quot;hex&quot;) #HQ render, very slow #render_highquality( samples=256, clear = TRUE,light=TRUE) 2.3 Environmental resources We will generate zones of higher resources/fitness that will attract settlements and support higher carrying capacity ie population size. The resources will be located at given hotspots with an area around in which they decay with a logistic function. We provide a very basic function logisticdecay which allow to define multiple source with different decay array around them. Here’s the code for the generation of resources under logistic decay Show code Code ## Fucntion 13. Logistic decay for resource generation logisticdecay &lt;- function(pt, rast, L=1, k=0.0001, x0=60000){ ds &lt;- terra::distance(rast, pt) logdec = L-L/(1+exp(-k*(ds-x0))) return(logdec) } For the original archaeoriddle, these hotspots and their radius were manually selected given the topography of Rabbit Hole. These manual setups are given below: ##resource geolocalisation set manually goodresources &lt;- vect( cbind( x=c(-0.2300711, -3.1455282, -0.5086485, -1.9639755, -0.4077843, 0.019688, -3.116710), y=c(3.6423000, -0.2551019, -0.7440748, 1.1303214, 1.0248567, 0.2194895, 2.0267718) ) ) #spread of resources areas &lt;- 4 * c(100, 2000, 200, 300, 100, 200, 400) #speed of ressource decay: ks &lt;- c(0.0002, 0.0001000, 0.0001600, 0.0001800, 0.00040, .0002, 0.0002)/4 crs(goodresources) &lt;- crs(original.ras) The function logisticdecay generates individual rasters for all the identified hotspots, which are then merged to create an aggregate raster referred to as ‘resources’. When considering the selection of hotspots, one may choose to retain the one utilized for the archaeoriddle but to adjust these resources center to your own topography ; one can use locator() to find the coordinates of the hotspot by clicking on the map. An example of that is given in this section of the chapter These resource span along all the map but we want to restrict them to place where peoplewill be able to settle Given this new landscape we can now generate sites in specific locations. We first need to define where the sites can appear (not in water nor too high in the mountains) ; so we first create a raster ‘viable’ which will give a binary maps where group of human can leave or not. height.groups &lt;- height.ras # taking back the raster created before height.groups[height.groups&lt;mean(height.groups[])] &lt;- NA #replace values that are below mean level (under water) height.groups[height.groups&lt;(maxh*.7)] &lt;- 1 # values below 70% of the maximum height are set as 1 height.groups[height.groups&gt;(maxh*.7)] &lt;- 200 # value above at 200 (high mountains) height.groups[is.na(height.groups)] &lt;- -1 # NA values (below waters) are set to -1 height.poly &lt;- as.polygons(height.groups) # convert height.groups as polygons that will be of three type: -1 =&gt; under water, 1, viable, and 200, high mountaines viable &lt;- makeValid(height.poly[2,]) # select polygon with &#39;1&#39; and make it a valid polygon as it may have some loop and problems We now have also have a polygon viable representing the space where sites can be create. This space is representing by the green area on the map below. plot(viable,col=adjustcolor(&quot;chartreuse&quot;,.8)) We then loop over all center manually defined previously and apply the logicitc decay within the viable raster. allres &lt;- lapply( seq_along(goodresources), function(i){ logisticdecay(goodresources[i], mask(height.ras, viable), x=areas[i], k=ks[i] ) } ) allna &lt;- sapply(allres, function(i)any(is.na(values(i)))) allres &lt;- allres[!allna] And when the add all the ressource raster, which gives us a final ress raster as below: ress &lt;- allres[[1]] for(i in 2:length(allres)) ress &lt;- ress + allres[[i]] ress &lt;- mask(ress,viable) plot(ress) 2.4 Manually Select Resources Below some useful code to manually select the resource origins on the map (by clicking on the screen) and explore different decays around them plotMap(height.ras, height.wat, paste0(&quot;year &quot;, 0)) new_site &lt;- vect(as.data.frame(locator()), geom=c(&quot;x&quot;, &quot;y&quot;)) # select point on the screen crs(new_site) &lt;- crs(height.ras) d2 &lt;- logisticdecay(new_site, height.ras, x=20000) # generate a logistic decay around the point plot(d2) w &lt;- (0.7*d2 + 0.3*ress) / (0.7*minmax(d2)[2] + 0.3*minmax(ress)[2]) plot(w) plot(mask(w*logisticdecay(new_site, height.ras, k=0.00002, x=1), territory)) multisamp &lt;- sapply( 1:1000, function(i){ spatSample(x=w, size=1, method=&quot;weights&quot;, xy=T)[1:2] } ) "],["popgrowth.html", "3 Population structure &amp; growth 3.1 Settlements position 3.2 Initializing Site Population and Culture 3.3 Population Growth : \\(Pop\\)-protocol", " 3 Population structure &amp; growth 3.1 Settlements position n &lt;- 20 # we will create 20 sites size &lt;- rexp(n) #assign a size for each site following an exponential distribution Following what we set up during the previous chapter, we can use spatSample from package terra to randomly select n points (20 for this simulation) on the viable polygon we created. sites &lt;- spatSample(viable, n) # generate n random points within the viable polygon and assign to variable sites plot(height.ras, col=col_ramp(20), legend=F, reset=F) plot(height.wat, col=&quot;lightblue&quot;, add=T, legend=F) plot(sites, cex=2, pch=21, add=T, bg=rainbow(length(sites), alpha=.6)) # get only the non submerged actual land and plot it above_level &lt;- height.ras &gt; mean(height.ras[]) coastline &lt;- st_as_sf(as.polygons(above_level))[2,] plot(coastline, col=NA, bgc=adjustcolor(&quot;cyan&quot;, 0.1), add=T) In the context of the archaeoriddle challenge, the files that describe this newly generated environment (the DEM and the coastline) as well as the resources raster, needs to be given to the participant. This can be done as: foldname=&quot;data_toshare&quot; #We will store all data that can/will be shared with participant in data_toshare if(!file.exists(foldname))dir.create(foldname) st_write(dsn=file.path(foldname,&quot;coastline.shp&quot;), coastline,append=F) # Write the coastline as a shapefile ## Writing layer `coastline&#39; to data source ## `data_toshare/coastline.shp&#39; using driver `ESRI Shapefile&#39; ## Writing 1 features with 1 fields and geometry type Multi Polygon. writeRaster(filename=file.path(foldname,&quot;dem_raster.tiff&quot;), height.ras, overwrite=T) #write the DEM as a raster writeRaster(filename=file.path(foldname,&quot;resources.tiff&quot;), ress, overwrite=T) #write the resources as a raster The original data, saved and shared for the archaeoriddle are stored in data_original/ 3.2 Initializing Site Population and Culture Each site is then assigned as either belonging to the hunter-gather (HG) or farmer (F) culture. To generate a more realistic distribution, all sites are initially, designated as hunter-gatherer sites, except for the one located in the most southwest position. Please note that this decision was based on the specific shape of the initial archaeoriddle’s challenge. If needed, you may want to manually select the site culture for this specific location. cultures &lt;- rep(&quot;HG&quot;,length(sites)) sw &lt;- which.min(sqrt((crds(sites)[,&quot;x&quot;] - min(crds(sites)[,&quot;x&quot;]))^2 + (crds(sites)[,&quot;y&quot;] - min(crds(sites)[,&quot;y&quot;]))^2)) cultures[sw]=&quot;F&quot; sites$culture &lt;- cultures plotMap(height.ras,height.wat) points(crds(sites),bg=as.factor(cultures),pch=21,cex=1.3) Each site will then be assigned a carrying capacity (how many people can leave on the site), probabilistically defined using the resource raster and the culture of the site. Kbase &lt;- c(&quot;HG&quot;=45,&quot;F&quot;=120) #difference in base-K for the two cultures # The base Ks are then modulated given the ressource raster sites$Ks &lt;- initKs(Kbase, sites, ress, sizeexp=&quot;F&quot;) Using these carrying capacities, we then set initial number of people, and the population structure for each sites: # Population size at initialisation, a number close to Ks INs &lt;- round(runif(length(sites), 0.85, 0.95)*sites$Ks) # initialise population structure for all sites Ips &lt;- lapply(INs, initpopstruc) ts &lt;- 250 # We put the population structure across the years in a matrix: each row will represent a time step of the simulation ; each column a sites Nts &lt;- initlistsites(Ips, ts=ts) We can look at the population structure of the biggest site: biggest &lt;- which.max(INs) pop.biggest &lt;- Ips[[biggest]] poplist=sapply(c(F=&quot;F&quot;,M=&quot;M&quot;),function(s)table(cut(pop.biggest[pop.biggest[,2]==s,1],breaks=seq(0,40,4)))) par(mar=c(1,10,1,1)) barplot(-poplist[,1],beside=T,horiz=T,col=c(&quot;blue&quot;),axes=F,las=1) par(mar=c(1,1,1,10)) barplot(unname(poplist[,1]),beside=T,horiz=T,col=c(&quot;red&quot;),axes=F) You can see the code of initKs, initpopstruct and initlistsites functions here: Show code Code ## Function 14. Initial carrying capacity initKs &lt;- function(Kbase=c(&quot;HG&quot;=30,&quot;F&quot;=120), sites, ressources, sizeexp=NULL, rate=.5){ Ks &lt;- round(Kbase[sites$culture] + rnorm(length(sites), 0, 10)) while(any(Ks&lt;1)){ Ks &lt;- round( Kbase[sites$culture] + rnorm(length(sites), 0, 10) ) } #Ks[sites$culture==&quot;F&quot;]=Ks[sites$culture==&quot;F&quot;]*runif(sum(sites$culture==&quot;F&quot;),1,1) tmp &lt;- Ks * (1 + extract(ressources, sites)[, 2]) if(!is.null(sizeexp)){ tmp[sites$culture==sizeexp] &lt;- ( (Ks[sites$culture==sizeexp]) * (1 + rexp(sum(sites$culture==sizeexp), rate=rate) * extract(ressources, sites[sites$culture==sizeexp])[,2]) ) } tmp } ## Function 15. Create population matrix initpopstruc &lt;- function(n=100, ages=10:30, p_sex=c(0.5, 0.5)){ initpop = data.frame( &quot;Age&quot; = sample(ages, n, ages, replace = TRUE), &quot;Sex&quot; = sample(c(&quot;M&quot;, &quot;F&quot;), n, prob = p_sex, replace = TRUE)) return(initpop) } ## Function 16. Initialize initial list of sites initlistsites &lt;- function(list_sites, ts=200){ Nts &lt;- matrix(0, nrow=ts+1, ncol=length(list_sites)) Nts[1,] &lt;- sapply(list_sites, nrow) return(Nts) } 3.3 Population Growth : \\(Pop\\)-protocol This is a central aspect of the model, and defines several parameters to control how population growth and reach a maximum population \\(\\sim K\\). It is based on three functions: The first function, Gpd(), is the core of the protocol. It receives a population matrix or data.frame with a specific number of individuals (nrow), their age (df[,1]) and their sex (df[,2]) and produces another object with the same format but with a different population based on the current one. Introduced in a loop, this function produces a time series of population, which ensures the Markov property of the stochastic process \\(p(x_n+1|x_n)\\). However, this function also relies on two the other functions: death() and K_lim(). death() is the process of people dying each year. It is based on an age-structured probability matrix, where infants and old people have more probabilities of deceasing. The matrix is provided, but the user can set its own. K_lim() limits the population when it reaches \\(K\\). The user should specify the appropriate value for \\(K\\) according to the size of the site, its type, the group economy, etc. Also, the user can change the probability that exceeding people dies with parameter prob. Default is 0.8. These base function will then be used in higher level functions to update the population size given the size and ages defined and described in the previous section. A simple function Pop_stoch implement a simple stochastic growth that uses these functions without the migrations and war mechanismes describe in this chapter. We can then use it to check and see the type of growth are generate given the default parameters (prob define the probability of people dying when the population has reached carrying capacity) set.seed(1234) plot(1,1,ylim=c(0,180),xlim=c(0,50),type=&quot;n&quot;,xlab=&quot;time&quot;,ylab=&quot;pop size&quot;) na=replicate(10, lines(Pop_stoch(pop_size=50, ts=50, K=150, W_fert_age = c(10, 45), M_fert_age = c(15, 55), p_offspring = 0.3, prob = 0.8))) To know more about these functions and how their parameters are used, you can look at the manual given in the package: ?Gpd ?death ?K_lim ?Pop_stoch You can also use this shiny app online to explore how these parameters interact to generate different growth curves. Source If you want to delve into the code of the functions you can open it here: Show code Code ## Function 1. Generation of population dynamics Gpd &lt;- function(x, K, W_fert_age = c(10, 45), M_fert_age = c(15, 55), p_offspring = 0.3, prob = 0.8, ...){ ## Process of having offspring # Female fertile population W &lt;- x[x[,2]==&quot;F&quot;, ] W_fert &lt;- W[W$Age &gt; W_fert_age[1] &amp; W$Age &lt; W_fert_age[2], ] # Male fertile population M &lt;- x[x[,2]==&quot;M&quot;, ] M_fert &lt;- M[M$Age &gt; M_fert_age[1] &amp; M$Age &lt; M_fert_age[2], ] ## Probability of having descendance per woman # Penalisation in case there are too few men pen &lt;- round(nrow(M_fert)*2 / nrow(W_fert), 2) # Assumes one man can have two women pen[pen&gt;1] &lt;- 1 ## The men penalisation can never multiply the birth rate per woman # Probability of a woman having a son per year p_offspring &lt;- p_offspring*pen ## Aging process. They get one year older x$Age &lt;- x$Age+1 ## Births are new population that&#39;s added n_offspring &lt;- sum(rbinom(nrow(W_fert), 1, p_offspring)) new_pop &lt;- data.frame( &quot;Age&quot; = rep(0, n_offspring), &quot;Sex&quot; = sample(c(&quot;M&quot;,&quot;F&quot;), n_offspring, prob=c(0.5, 0.5), replace=TRUE)) x &lt;- rbind(x, new_pop) ## Process of dying vec_d &lt;- apply(x, 1, death, ...) x &lt;- x[vec_d==0, ] ## Apply carrying capacty restrictions x &lt;- K_lim(x, K = K, prob = prob) return(x) } ## Function 2. Simulation of death process death &lt;- function(x, pd=data.frame(&quot;Age&quot; = c(0:99), &quot;P_d&quot; = c(rep(0.14,1), rep(0.16,4), rep(0.05,5), rep(0.01,24), rep(0.03,14), rep(0.1,10), rep(0.3,42)))){ age &lt;- as.numeric(x[1]) return(rbinom(1, 1, prob=pd[pd$Age==age, 2])) } ## Function 3. Simulation of carrying capacity limitation K_lim &lt;- function(x, K, prob=0.8){ p &lt;- nrow(x) if (p&gt;K){ o &lt;- rbinom(p-K, 1, prob) o &lt;- sum(o[o==1]) o[o==0] &lt;- 1 # Avoids problem eliminating all the df if remove == 0 x &lt;- x[-sample(1:nrow(x), o, replace=FALSE), ] } return(x) } ## Function 4. Stochastic population generation Pop_stoch &lt;- function(pop_size, K, ts, prob = 0.8, ...){ ## Create initial population pop_matrix &lt;- data.frame( &quot;Age&quot; = sample(10:30, pop_size, 10:30, replace = TRUE), &quot;Sex&quot; = sample(c(&quot;M&quot;,&quot;F&quot;), pop_size, prob = c(0.5,0.5), replace = TRUE) ) ## Initialize vector with population size for each year pop &lt;- vector(length=ts) pop[0] &lt;- nrow(pop_matrix) ## Run stochastic process for (i in 1:ts){ pop_matrix &lt;- Gpd(pop_matrix, K=K, prob=prob, ...) pop[i] &lt;- nrow(pop_matrix) } return(pop) } "],["conflict.html", "4 Migration &amp; Conflict 4.1 Fission 4.2 Migration 4.3 Conflict and War 4.4 DEPRECATED – Network, Site Size And Climate", " 4 Migration &amp; Conflict 4.1 Fission Although the protocol described in the previous chapter tends to keep the population of a site close to its carrying capacity, it often happens that the population size still exceeds the carrying capacity. When this occurs, the surplus individuals have the opportunity to migrate and establish a new settlement. This process is influenced by two culture-dependent parameters: + minimals: This defines the minimum number of individuals, in relation to the carrying capacity, needed to create a new settlement. + prob_split: This defines the probability that, given the minimum number of individuals is reached, the group will actually split and relocate. It is also culture-dependent. When this occurs, the group will search for a suitable location to relocate. The selection of this space depends on its availability, its proximity to the original settlement, and the resources available at the new location. The criteria for available space are determined by areas on the map that are neither too elevated, submerged in water, nor within the vicinity of pre-existing settlements. This vicinity range is also influenced by cultural factors—reflecting the cultural differences in terms of mobility—and by the population size of the settlement. The latter reflects the settlement’s capability to dispatch individuals to defend its territory and its increasing needs for resources. Once all the available spaces have been identified, a friction surface is created. This surface determines the probability of moving to any square of the raster. This probability is defined as a function of the distance from the original settlement and the availability of resources. To do so we create a friction surface using a logistic decay function defined as: \\[ \\text{logdec}(ds,k,L) = L - \\frac{L}{1 + e^{-k(ds - x_0)}} \\] where: + \\(L\\): The maximum value, set as one to give a probability + \\(k\\): Growth rate, here set as 0.0001 + \\(ds\\): raster representing the distance to the focal settlement + \\(x_0\\): The midpoint of the logistic function, set as 20000, ie 20km from the origin, modulated by cult_ext, a culture-dependent parameters reflecting the technological abilities of the culture to move. We then combine this first friction surface with the ressource raster to compute another friction map representing weighted average probability, where the distance to origin account for 70% of the final probability and available ressources 30%. This final friction surface can be summarised as: \\[ w = \\frac{0.7 \\cdot d_2 + 0.3 \\cdot \\text{ressources}}{0.7 \\cdot \\text{max}(d_2) + 0.3 \\cdot \\text{max}(\\text{ressources})} \\] Where: \\(d_2\\) is the result of the logistic decay function applied to a given site and its parameters. ressources represents available resources. max() return the maximal value of a raster, to normalise value as probabilites (it’s implemented by terra::minmax() function) When a site position is chosen, a new settlement is created, using the functions described in the previous chapter, to generate the carrying capacity of the new settlement and it’s new population structure This is wrapped within the function run_simulation, that will be described in details in the next chapter. Here is a simplified version of the section of the code that handle this migration events: migrants &lt;- newN - round(Ks[[s]]*0.9) #possible migrants, new_site &lt;- NULL tmp &lt;- Nts[i-1,] tmp[Nts[i,] &gt; 0] &lt;- Nts[i, Nts[i,] &gt; 0] havemoved &lt;- F if (migrants &gt;= (minimals[sites$culture[s]]*sites$Ks[s]) &amp; runif(1)&lt;prob_split[sites$culture[s]] ){ #if supopulation &gt; 10 people, 10% chance of creation of a new city #mean of area of influence infarea &lt;- (sqrt(tmp)+penal_cul[cultures]) * buffersettl buffersize &lt;- rnorm(length(infarea), infarea, infarea * 0.1) buffersize[tmp==0] &lt;- 0.00001 territory &lt;- erase(viable, buffer(sites, buffersize)) if( length(territory)&gt;0 ){ ##select a new site given its distance to the old one and the ressourcesource available in ressources d2 &lt;- logisticdecay( sites[s], dem, x=20000*cul_ext[sites$culture[s]]) w &lt;- (0.7 * d2 + 0.3*ressources) / (0.7*minmax(d2)[2] + 0.3*minmax(ressources)[2]) new_site &lt;- spatSample( x=mask( w * logisticdecay(sites[s], dem, k=0.00002, x=20000*cul_ext[sites$culture[s]]),territory), size=1, method=&quot;weights&quot;, xy=T)[1:2] new_site &lt;- vect(new_site, geom=c(&quot;x&quot;,&quot;y&quot;)) if ( length(new_site)&gt;0 &amp; all(!is.na(crds(new_site))) ){ ##add new site to site listes Ips[[length(Ips)+1]] &lt;- initpopstruc(n=migrants) #initialise a fake population, will be updated by real migrants later new_site$culture &lt;- sites$culture[s] new_site$Ks &lt;- round( initKs( Kbase, sites=new_site, ressources, sizeex=&quot;F&quot;, rate=0.45)) sites &lt;- rbind(sites, new_site) Ks[length(Ks)+1] &lt;- new_site$Ks city &lt;- length(Ips) Nts &lt;- cbind(Nts, rep(0,ts+1)) Nts[i, city] &lt;- migrants cultures &lt;- c(cultures, cultures[s]) havemoved &lt;- T } if( havemoved ){ Ips[c(s,city)] &lt;- changePopSize(loosingPop=Ips[[s]], winingPop=Ips[[city]], size=migrants) newN &lt;- newN - migrants } } } This uses a couple of internal function some we already describe like: - initpopstruc - initKs and specific one like: - changePopSize: Change population size by moving population from one to another; or remove a group of people 4.2 Migration If the fission event fails, people will then move to another settlement. The choice of the new settlement is based on a combination of the distance to the new settlement, its ‘available space,’ which correspond to how far from carrying capacity the population will be once the migrant move in, and the resources available at this site. Each settlement is assigned a probability, given these elements, used to randomly choose the site of migration, following this equation: \\[ \\text{attractivity} = \\frac{e^{\\left(\\text{resources} \\times \\text{space} \\times \\log(\\text{distance})\\right)} }{ \\sum \\exp\\left(\\text{resources} \\times \\text{space} \\times \\log(\\text{distance})\\right)} \\] Here is a simplified version of the actual implementation available in run_simulation: att &lt;- extract(ressources,sites)[,2] space &lt;- sites$Ks - (Nts[i-1,] + migrants) dis &lt;- extract(logisticdecay(sites[s], dem, k=0.00002, x=1), sites)[,2] attractivity &lt;- att * space * dis #attractivity=attractivity*(1+10*(sites$culture[s]==sites$culture)) #4 times more likely to go to similar culture attractivity[s] &lt;- min(attractivity)-1 attractivity &lt;- exp(attractivity)/sum(exp(attractivity)) attractivity[Nts[i-1,]&lt;10] &lt;- 0 attractivity[sites$culture!=sites$culture[s]] &lt;- 0 if(any(is.na(attractivity))) attractivity[is.na(attractivity)] &lt;- 0 city &lt;- sample(size=1, x=seq_along(sites), prob=attractivity) Nts[i,city] &lt;- Nts[i-1,city] + migrants 4.3 Conflict and War the culture of the settlements, During each time steps of the simulation, each settltement have the opportunity to fight against another settlement. This again, will depend on multiple factor such as the culture of the settlements and their population sizes. Here’s a breakdown of how war is simulated according to the provided code snippet: Identification of Aggressors: In the archaeoriddle simulation, only Farmer have a aggressive behavior and engage in conflict. Thus during a first stepo, farmers ettlements with a population size greater than 50 are identified as potential aggressors (potentialfighters). From this subset of the Farmer’s settlement,10% of them will actually try to engage in warfare. Selecting the Target for Attack: Settlements within a radius from the agressor and with a different culture are identified as potential victims. The radius is defined by theparameters bufferatack. Then: If there is only one clash candidate, that candidate is selected as the target. If there are multiple candidates, one is randomly selected as the target of the attack. Conducting the Battle and Calculating Losses: The battle is simulated through the fightbetterloss function, which calculates the new population sizes for both the aggressor and the victim after the conflict. The casualties from the battle are calculated, and these are used to determine the size of the war event. To calculate who win and what are the loss we defined a function fightbetterloss that simulates a battle between two settlements and calculates their post-conflict population sizes based on their initial population sizes. The outcome of who wins the battle is determined probabilistically, influenced by the relative sizes of each settlement’s population. The battle outcome between two settlements, A and B, is determined by: \\[ P(\\text{Victory for A}) = \\frac{N_A}{N_A + N_B} \\] where \\(N_A\\) and \\(N_B\\) are the population sizes of settlements A and B, respectively. This probability is used to decide who the victor (v) is and who the loser (l) is. The new population sizes of the victor and the loser after the battle are determined by binomial distributions, reflecting the randomness of conflict outcomes: For the victor (\\(v\\)): \\[ N_{v,\\text{new}} = \\text{Binomial}\\left(n = N_v, p = 1 - \\frac{N_l}{N_v + N_l}\\right) \\] For the loser (\\(l\\)): \\[ N_{l,\\text{new}} = \\text{Binomial}\\left(n = N_l, p = 1 - \\frac{N_v}{N_v + N_l}\\right) \\] In these equations, \\(N_{v,\\text{new}}\\) and \\(N_{l,\\text{new}}\\) are the new population sizes for the victor and the loser; \\(n\\) is the size parameter of the binomial distribution, which is the pre-battle population size; and \\(p\\) is the success probability of surviving the battle. The success probability for each side inversely depends on the ratio of the opponent’s population size to the total population involved in the conflict, reflecting the idea that the larger the opponent’s population, the higher the casualties and thus, the lower the survival probability. This process incorporates cultural differences, population sizes, and proximity to dictate when and how wars occur between settlements, simulating the nuanced and multifaceted nature of conflict in a simplified manner. A quick illustration of how the outcome of conflicts between two settlements of varying size: compare &lt;- do.call(&quot;rbind&quot;,lapply(seq(10,100,3),function(b) t(rbind(b/100,replicate(100,c(100,b)-fightbetterloss(c(100,b),1,2)))) )) plot(compare[,1]+runif(100,0,0.01),compare[,2],col=adjustcolor(&quot;red&quot;,.6),pch=20,main=&quot;number of loss&quot;,xlab=&quot;ratio A/B&quot;,ylab=&quot;number of loss&quot;) points(compare[,1]+.015+runif(100,0,0.01),compare[,3],col=adjustcolor(&quot;blue&quot;,.6),pch=20) legend(&quot;topleft&quot;,legend=c(&quot;loss for A&quot;,&quot;loss for B&quot;),col=c(&quot;red&quot;,&quot;blue&quot;),pch=20) cls=heat.colors(length(unique(compare[,1]))) names(cls)=unique(compare[,1]) plot(compare[,2]+runif(100),compare[,3]+runif(100),bg=cls[as.character(compare[,1])],pch=21,ylab=&quot;losses for B&quot;,xlab=&quot;losses for A&quot;,xlim=c(0,80),ylim=c(0,80),cex=2) legend(&quot;bottomrigh&quot;,legend=names(cls)[seq(1,length(cls),length.out=9)],fill=cls[seq(1,length(cls),length.out=9)],ncol = 3,title=&quot;ratio A/B&quot;) abline(a=1,b=1,col=&quot;green&quot;,lwd=3) Figure 4.1: Simulation run with population A with 100 people, and populaiton B with a growing populatoin size from 10 to 100, represent fraction of population a from 10% to same population size. Noise has been added to visualise alldata The model with generate loss in both sides, but on average less for the biggest populaiton, while stil allowing cases where the smaller settlement win the fight and generate more damage that it receives. These difference get smoothed when the size of the two settlements become similar. Here is a simplified version of the actual implementation available in run_simulation: potentialfighters &lt;- which(sites$culture==&quot;F&quot; &amp; Nts[i,]&gt;50) for (s in sample(x=potentialfighters, size=round(length(potentialfighters)*probfight))){ buff &lt;- bufferatack clash &lt;- whotouch(s, sites, Ne=Nts[i,], buffersize=buff) if(length(clash)&gt;0 &amp;&amp; !is.na(clash)){ if(length(clash) == 1){ attack &lt;- clash } else { attack &lt;- sample(clash, 1) } newns &lt;- fightbetterloss(Ne=Nts[i,], a=s, b=attack) casualties &lt;- sum(Nts[i, c(s,attack)] - newns[c(s,attack)]) warcasualties[i] &lt;- casualties sizew &lt;- casualties^2/4000 warpoints(sites, s, attack, Ne=Nts[i,], buffersize=buff, sizewar=sizew+0.5,plot=wplot) #effectively kill people in population (should be done taking into account age pyramid to be more realistic) Ips[[s]] &lt;- changePopSize(loosingPop=Ips[[s]], size=(Nts[i,s] - newns[s])) Ips[[attack]] &lt;- changePopSize(loosingPop=Ips[[attack]], size=(Nts[i, attack] - newns[attack])) Nts[i,] &lt;- newns } } whotouch : is a central elements of the conflict has it return the list of sites that are within the reach of the settlements. A couple of adjustment have been manually done for the original challenge: only Farmer attacks, and they do so only if they have at least 50 individuals. Among the settlement that have the potential to attach another, they will do so with a probability of probfight. This has been set to 0.1 in the original challenge in order to ensure that settlement don’t fight everyone all the time, which was leading to way to much wars. warpoint : is here used to randomly select a point where the two settlement can potentially meet and thus where the conflict happened. It is so far only used to show conflicts on the map, but could be used to deposity evidences of these conflicts. Here are defined lot of the assumptions that will be tested by the research questions proposed in the original challenge described in the Chapter 9. 4.4 DEPRECATED – Network, Site Size And Climate The following steps were designed but never used ; they can be fun and useful. Create a network between sites (requires igraph) library(igraph) subsites &lt;- sites crs(subsites) &lt;- &quot;+proj=lonlat&quot; subsites &lt;- sites[sample(nrow(sites), 20), ] ig &lt;- graph_from_adjacency_matrix( as.matrix(distance(subsites)), weighted=T, mode=&quot;undirected&quot;) ig &lt;- set_graph_attr(ig, &quot;layout&quot;, crds(subsites)) V(ig)$label &lt;- &quot;&quot; V(ig)$size &lt;- (size+1)*5 V(ig)$frame.color &lt;- &quot;black&quot; V(ig)$frame.width &lt;- .5 E(ig)$weight &lt;- 1/(E(ig)$weight^2) E(ig)$width &lt;- exp(E(ig)$weight)+.6 allcomu &lt;- cluster_louvain(ig)$memberships[1,] V(ig)$color &lt;- allcomu E(ig)$color &lt;- allcomu[head_of(ig,E(ig))] plot(height.ras, col=col_ramp(20), legend=F, reset=F) plot(height.wat, col=&quot;lightblue&quot;, add=T, legend=F) plot(ig, add=T, rescale=F, alpha=.2) Visualise sites on the 3D plot plot_3d( hm_lamb,height_map, zscale=20, fov=0, theta=20, zoom=1, phi=45, windowsize=c(1000, 800), water=TRUE, waterdepth=mean(height), wateralpha=1, watercolor=&quot;lightblue&quot;, waterlinecolor=&quot;lightblue&quot;, waterlinealpha=.7 ) render_points( extent=raster::extent(raster::raster(height.ras)), lat=2.13828, long=-1.685547, altitude=extract(height.ras,sites)[,1]+20, zscale=20, size=1, col=categorical_pal(max(allcomu))[allcomu], clear_previous=TRUE ) render_points( extent=raster::extent(raster::raster(height.ras)), lat=crds(sites)[,&quot;y&quot;], long=crds(sites)[,&quot;x&quot;], altitude=extract(height.ras, sites)[,1]+20, zscale=20, size=1, col=categorical_pal(max(allcomu))[allcomu], clear_previous=TRUE ) render_highquality(point_radius=5, samples=256, clear=TRUE, light=TRUE) let’s say the climate modulate site size in a naive way: for(i in seq(1, tstep, length.out=100)){ layout(matrix(1:2, nrow=2, ncol=1), heights=c(.8, .2)) par(mar=c(1, 2, 2, 1)) image(m, ann=F, axes=F, main=i) mtext(round(i), 3, 1) nsize &lt;- size*faketemp[i] + size + 1 points(crds(sites), bg=&quot;green&quot;, pch=21, cex=nsize) par(mar=c(2, 2, 0, 1)) plot( 1:tstep, faketemp, type=&quot;l&quot;, ylab=&quot;temp&quot;, xlab=&quot;time&quot;, ylim=range(faketemp), ann=F, frame.plot=F) points(i, faketemp[i], col=&quot;red&quot;, pch=20) } Population will then grow, for years and years. This implies multiple things: population need resources and a way to grow with regard to these resources and their own technologies.We initialised the ressource layer in the previous chapter in the section about ressources. In the following sections we will use this ressource raster to initialise site size et define how ressource are used by the different culture. "],["main-simu.html", "5 Main Simulation 5.1 Initial Parameters 5.2 Preparation 5.3 Algorithm 5.4 Explore simulation", " 5 Main Simulation 5.1 Initial Parameters We now have sites with two cultures, a environment and function to make population growth, migrate and engages into wars. Each function depends on a lot of paramaters, some have been reviewd before other are explain in the help of each function. Often the parameters can differs between the two cultures. He we gave a list of parameters, close to those used during the Archaeoriddle challenge: 5.2 Preparation We will create antoher older data_tmp where we will store useful file, that won’t be shared to everyone but will be used at some point or another. foldtmp=&quot;data_tmp&quot; #We will store all data that can/will be shared with participant in data_toshare if(!file.exists(foldtmp))dir.create(foldtmp) We setup all paramaters #initialisation Kbase=c(&quot;HG&quot;=45,&quot;F&quot;=120) #difference in K for the two cultures use and defined before ; will be use in simulation to define new site # spatial penality for cultural extentions, ie for population to move out of initial site : lower, bigger penality cul_ext &lt;- c(&quot;HG&quot;=7, &quot;F&quot;=6) # penality of occupational area ie how hard it is to come close to your site ; if low, other sites can come close penal_cul &lt;- c(&quot;HG&quot;=4, &quot;F&quot;=5) # proba to give birth every year prob_birth &lt;- c(&quot;HG&quot;=0.3, &quot;F&quot;=0.5) # proba to die when pop &gt; K prob_survive &lt;- c(&quot;HG&quot;=0.8, &quot;F&quot;=0.6) # proba to create new settlement when Ne &gt; K prob_split &lt;- c(&quot;HG&quot;=0.2, &quot;F&quot;=0.6) # how big the group of migrant should be to create a new city vs # migrate to a existing one minimals &lt;- c(&quot;HG&quot;=0.14, &quot;F&quot;=0.20) # prob to migrate to existing settlement when Ne &gt; K prob_move &lt;- c(&quot;HG&quot;=0.2,&quot;F&quot;=0.1) Let’s see our raster map again, and put the sites on top, with their ids and showing there initial population size: plotMap(height.ras,height.wat,paste0(&quot;year &quot;, 0)) plot(sites, pch=21, add=T, bg=rainbow(2, alpha=0.6)[as.factor(sites$culture)],cex=(1+Nts[1,]/100)) text(sites,pos=3) The run_simulation function will take the raster data and parameters for defined before for the two different cultures and run for the specified number of years ts (250). It will then call the functions described above to model contacts between sites, the outcomes of fights and migrations. If you want ?run_simulation ?whotouch ?fightbetterloss ?changePopSize ?warpoints 5.3 Algorithm The general organisation of the algorithm is as follow: initialisation for all sites s: N_s &lt;- growth(N_s) if(N_s&gt;K_s): moving people: if( N_s - K-S &gt; min_s * K_s and probsplit_s): create new city: find the closest most attractive spot and move there else if(proba move ): migration war(s) 1. ** initialisation ** 2. for (i in 2:(ts+1)) inactives &lt;- (Nts[i-1,]==0) for ( s in sample(seq_along(sites)[!inactives]) : #among all active site N_s &lt;- Gpd() if $N_s&gt; K_s$ : migrants &lt;- newN - round(Ks[[s]]*0.9) if (migrants &gt;= (minimals[sites$culture[s]]*sites$Ks[s]) &amp; runif(1)&lt;prob_split[sites$culture[s]] ){ #if subpopulation &gt; 10 people, 10% chance of creation of a new city infarea &lt;- (sqrt(tmp)+penal_cul[cultures]) * buffersettl buffersize &lt;- rnorm(length(infarea), infarea, infarea * 0.1) buffersize[tmp==0] &lt;- 0 territory &lt;- erase(viable, buffer(sites, buffersize)) if( length(territory)&gt;0 ){ ##select a new site given its distance to the old one and the ressourcesource available in ressources d2 &lt;- logisticdecay( sites[s], dem, x=20000*cul_ext[sites$culture[s]]) w &lt;- (0.7 * d2 + 0.3*ressources) / (0.7*minmax(d2)[2] + 0.3*minmax(ressources)[2]) new_site &lt;- spatSample( x=mask( w * logisticdecay(sites[s], dem, k=0.00002, x=20000*cul_ext[sites$culture[s]]), territory), size=1, method=&quot;weights&quot;, xy=T)[1:2] new_site &lt;- vect(new_site, geom=c(&quot;x&quot;,&quot;y&quot;)) if ( length(new_site)&gt;0 &amp; all(!is.na(crds(new_site))) ){ ##add new site to site listes Ips[[length(Ips)+1]] &lt;- initpopstruc(n=migrants) #initialise a fake populaition, will be updated by real migrants later new_site$culture &lt;- sites$culture[s] new_site$Ks &lt;- round(initKs( Kbase, sites=new_site, ressources, sizeex=&quot;F&quot;, rate=0.45)) sites &lt;- rbind(sites, new_site) Ks[length(Ks)+1] &lt;- new_site$Ks } } } ## if no creation of new city happen, there is a certain probability that people will move if( length(new_site)==0 &amp;&amp; runif(1) &lt; prob_move[sites$culture[s]] ){ #migration to existing site att &lt;- extract(ressources,sites)[,2] space &lt;- sites$Ks - (Nts[i-1,] + migrants) dis &lt;- extract(logisticdecay(sites[s], dem, k=0.00002, x=1), sites)[,2] attractivity &lt;- att * space * dis #attractivity=attractivity*(1+10*(sites$culture[s]==sites$culture)) #4 times more likely to go to similar culture attractivity[s] &lt;- min(attractivity)-1 attractivity &lt;- exp(attractivity)/sum(exp(attractivity)) attractivity[Nts[i-1,]&lt;10] &lt;- 0 attractivity[sites$culture!=sites$culture[s]] &lt;- 0 if(any(is.na(attractivity))){ print(attractivity) attractivity[is.na(attractivity)] &lt;- 0 } city &lt;- sample(size=1, x=seq_along(sites), prob=attractivity) Nts[i,city] &lt;- Nts[i-1,city] + migrants } if( havemoved ){ Ips[c(s,city)] &lt;- changePopSize( loosingPop=Ips[[s]], winingPop=Ips[[city]], size=migrants) newN &lt;- newN - migrants } } Nts[i,s] &lt;- newN } ## WAR ======================= potentialfighters &lt;- which(sites$culture==&quot;F&quot; &amp; Nts[i,]&gt;50) for (s in sample(x=potentialfighters, size=round(length(potentialfighters)*0.1))){ buff &lt;- bufferatack potentialvictims &lt;- which(sites$culture !=sites$culture[s] &amp; Nts[i,]&gt;0) clash &lt;- whotouch(s, sites, Ne=Nts[i,], buffersize=buff) if(length(clash)&gt;0 &amp;&amp; !is.na(clash)){ if(length(clash) == 1){ attack &lt;- clash } else { attack &lt;- sample(clash, 1) } newns &lt;- fightbetterloss(Ne=Nts[i,], a=s, b=attack) casualties &lt;- sum(Nts[i, c(s,attack)] - newns[c(s,attack)]) warcasualties[i] &lt;- casualties sizew &lt;- casualties^2/4000 warpoints(sites, s, attack, Ne=Nts[i,], buffersize=buff, sizewar=sizew+0.5) #effectively kill people in population (should be done taking into account age pyramid to be more realistic) Ips[[s]] &lt;- changePopSize(loosingPop=Ips[[s]], size=(Nts[i,s] - newns[s])) Ips[[attack]] &lt;- changePopSize(loosingPop=Ips[[attack]], size=(Nts[i, attack] - newns[attack])) Nts[i,] &lt;- newns } } return(list(Nts=Nts, warcasualties=warcasualties, Ips=Ips, sites=sites)) } We wrapped all this in a function, run_simulation that return a list of object. Let see one simple example: ts &lt;- 150 # we keep using 250 time steps. onesimu &lt;- run_simulation( sites=sites, viable=viable, dem=height.ras, ressources=ress, water=height.wat, foldervid=NULL, visu=F, visumin=T, log=F, ts=ts, #length of simulation in year Kbase=c(&quot;HG&quot;=35, &quot;F&quot;=110), #difference in K for the two cultures cul_ext=c(&quot;HG&quot;=7, &quot;F&quot;=6), #spatial penality to extent: lower, bigger penality penal_cul=c(&quot;HG&quot;=4, &quot;F&quot;=5), #penality of occupational area: low, other sites can cam close prob_birth=c(&quot;HG&quot;=0.4, &quot;F&quot;=0.75), #proba of giving birth every year prob_survive=c(&quot;HG&quot;=0.8, &quot;F&quot;=0.65), #proba of dying when pop &gt; K prob_split=c(&quot;HG&quot;=0.5, &quot;F&quot;=0.6), #proba of creating a new settlement when Ne &gt; K minimals=c(&quot;HG&quot;=0.14,&quot;F&quot;=0.20), #how big the group of migrant should be to create a new city vs migrate to a existing one probfight=0.5, #this has been added for the bookdown to increase conflict occurences ; it is of 0.1 in the original challenge, it allows to modulate the probability that any settlement in a position of conflict actually goes to war. bufferatack=1000, #distance max around which settlement can fight prob_move=c(&quot;HG&quot;=0.2, &quot;F&quot;=0.1) #proba of migrating to existing settlement when Ne &gt; K ) Figure 5.1: quick animation showing the simulation run in the previous chunk. Size o the dot represent the size of the settlement, the color its culture, where red represents farmers and blue hunter-gatherer. Flame represent fight between settlement, the size of the flame being proportional to the total number of casualties during the conflict (both sides included). Note that we here use a larger bufferatack that generates more conflicts and higher birth rates to make this simulation To create your own video of the simulation you can replace foldervid=NULL by foldervid=foldtmp or any other folder you want. this will save all output in the folder you specify, and you can use the image to generate your video using, for exemple, ffmpeg. We also save the different object generated by run_simulation to be able to re-load it, share it, analyse it, later. #write the terra SpatVect that stores all site sites location terra::writeVector(onesimu$sites,file.path(foldtmp,&quot;allsites.shp&quot;),overwrite=TRUE) #write the matrix with all the population for all sites and all time steps saveRDS(file=file.path(foldtmp,&quot;popSizeMatrix.RDS&quot;),onesimu$Nts) #write a list that stores all the population structure for the sites at the end of the simulation saveRDS(file=file.path(foldtmp,&quot;popStructList.RDS&quot;),onesimu$Ips) #write a vector that stores the number of death due to conflict at each time step saveRDS(file=file.path(foldtmp,&quot;wardeath.RDS&quot;),onesimu$warcasualties) Source code Check the code of the simulation functions here: Show code Code ## Function 17. Change population sizes changePopSize &lt;- function(loosingPop, size, winingPop=NULL, new=F, method=&quot;random&quot;, probs=dnorm, prob.option=list(&quot;sd&quot;=10, &quot;mean&quot;=22)) { #print(dim(loosingPop)) #if(!is.null(winingPop)) # print(dim(winingPop)) #if(length(size)==0 || size==0)return(data.frame(Age=numeric(),Sex=character())) if(nrow(loosingPop)==0){ kill &lt;- 0 }else if(method==&quot;random&quot;){ kill &lt;- tryCatch( sample(x=1:nrow(loosingPop), size=size, prob=probs(loosingPop$Age, mean=prob.option$mean, sd=prob.option$sd)), error=function(e){ print(paste0(&quot;problem with population replacement for settlement of size:&quot;, nrow(loosingPop), &quot; need to loose &quot;, size));0 } ) } #print(paste(&quot;diff&quot;,nrow(popdistrib)-size,&quot;new&quot;,size)) if(!is.null(winingPop)){ winingPop &lt;- rbind(winingPop, loosingPop[kill,]) } loosingPop &lt;- loosingPop[-kill,] if(!is.null(winingPop)) return(list(loosingPop, winingPop)) else return(loosingPop) } ## Function 18. Check sites touching whotouch &lt;- function(i, sites, Ne, homophily=F, buffersize=200){ touch &lt;- st_intersects( st_make_valid(st_as_sf(buffer(sites[i], Ne[i] * buffersize+0.00001))), st_make_valid(st_as_sf(buffer(sites, Ne * buffersize+0.00001)))) if( length(touch) &gt; 0 ){ enemies &lt;- unlist(touch) if(homophily){ enemies &lt;- enemies[enemies != i] } else { enemies &lt;- enemies[sites$culture[enemies] != sites$culture[i]] } } else { enemies &lt;- NA } #after adding a small number to zero, does that means that often some zero size group may be taken as fighter? checking below if this happen if(sum(Ne[enemies]==0)&gt;0){ print(&quot;some already dead enemies&quot;) print(which(Ne[enemies]==0)) enemies=enemies[Ne[enemies]&gt;0] } return(enemies) } ## Function 19. Model a simple fight simplefight &lt;- function(Ne, a, b){ if(runif(1) &lt; Ne[a] / (Ne[a] + Ne[b])){ v &lt;- a l &lt;- b } else{ v &lt;- b l &lt;- a } # Keep the original pop sizes for reporting outcome one &lt;- Ne # Update population sizes using a binomial Ne[v] &lt;- rbinom(n=1, prob=0.9, size=Ne[v]) Ne[l] &lt;- rbinom(n=1, prob=0.4, size=Ne[l]) print( paste(&quot;victory&quot;, v, &quot;(&quot;, one[v], &quot;-&quot;, Ne[v],&quot;) over&quot;, l, &quot;(&quot;,one[l],&quot;-&quot;,Ne[l],&quot;), total of: &quot;, (one[v]-Ne[v]) + (one[l]-Ne[l]), &quot;people&quot;)) return(Ne) } ## Function 20. Model fight with better probabilities fightbetterloss &lt;- function(Ne,a,b,log=F){ if( runif(1) &lt; Ne[a]/(Ne[a] + Ne[b]) ){ v &lt;- a l &lt;- b } else{ v &lt;- b l &lt;- a } one &lt;- Ne Ne[v] &lt;- rbinom(n=1, prob=1 - Ne[l]/(Ne[v] + Ne[l]), size=Ne[v]) Ne[l] &lt;- rbinom(n=1, prob=1 - Ne[v]/(Ne[v] + Ne[l]), size=Ne[l]) if(log)print(paste0(&quot;victory &quot;, v, &quot;(&quot;, one[v], &quot;-&quot;, Ne[v],&quot;) over &quot;, l, &quot; (&quot;, one[l], &quot;-&quot;, Ne[l], &quot;), tot: &quot;, (one[v]-Ne[v]) + (one[l]-Ne[l]), &quot;losses&quot;)) return(Ne) } ## Function 21. Draw a war symbol where two clans are fighting warpoints &lt;- function(sites, a, b, Ne, buffersize=300, plot=T, sizewar=2){ meetpoints &lt;- crop( buffer(sites[a], 1+Ne[a] * buffersize), buffer(sites[b], 1+Ne[b] * buffersize) ) if( length(meetpoints)&gt;0 ){ p &lt;- terra::spatSample(meetpoints, 1) if(plot &amp; length(p)&gt;0){ plot(p, add=T, bg=&quot;red&quot;, pch=&quot;🔥&quot;, cex=sizewar+1, col=adjustcolor(&quot;yellow&quot;, 0.1)) plot(p, add=T, bg=&quot;yellow&quot;, pch=&quot;⚔️&quot; ,cex=sizewar) } return(p) } else return(NULL) } ## Function 22. Run simulation run_simulation &lt;- function(cultures=NULL, viable=NULL, sites=NULL, dem=NULL, ressources=NULL, water=NULL, foldervid=NULL, visu=FALSE, visumin=TRUE, probfight=0.1, log=F, ts=20000, Kbase=c(&quot;HG&quot;=35, &quot;F&quot;=120), cul_ext=c(&quot;HG&quot;=7, &quot;F&quot;=6), penal_cul=c(&quot;HG&quot;=4, &quot;F&quot;=5), prob_birth=c(&quot;HG&quot;=0.3, &quot;F&quot;=0.5), prob_survive=c(&quot;HG&quot;=0.8, &quot;F&quot;=0.6), prob_split=c(&quot;HG&quot;= .2, &quot;F&quot;=0.6), prob_move=c(&quot;HG&quot;=0.2, &quot;F&quot;=0.1), minimals=c(&quot;HG&quot;=.14, &quot;F&quot;=.20), bufferatack=400, buffersettl=2000, Nts=NULL, Ips=NULL ){ ## Run stochastic process wplot &lt;- F if(visumin || visu) wplot &lt;- T Ks &lt;- sites$Ks cultures &lt;- sites$culture if(is.null(Nts)){ INs &lt;- round(runif(length(sites), 0.85, 0.95) * sites$Ks) #Population size at initialisation Ips &lt;- lapply(INs, initpopstruc) #initialise population structure for all sites Nts &lt;- initlistsites(Ips, ts=ts) frame &lt;- 0 mint &lt;- 2 } else {##should check and test howto start back a simulation mint &lt;- nrow(Nts) frame &lt;- nrow(Nts) } ### visualisation ===== if(visu &amp;&amp; !is.null(foldervid) &amp;&amp; !dir.exists(foldervid)) dir.create(foldervid) ### warcasualties &lt;- vector(&quot;integer&quot;, ts) for (i in 2:(ts+1)){ countcult &lt;- table(sites$culture[Nts[i-1, ] &gt; 0]) if ( length(countcult) != 2 ) { return( list(Nts=Nts[,1:i], warcasualties=warcasualties[1:i], Ips=Ips, sites=sites ) ) } if(log){ print( paste(&quot;year&quot;, i, &quot;total&quot;, sum(sapply(Ips,nrow)), &quot;with&quot;, length(sites), &quot;sites (&quot;, paste0(paste(names(countcult), countcult, sep=&quot;:&quot;), collapse=&quot;,&quot;), &quot;)&quot;)) } if (visumin){ ### visualisation ===== frame &lt;- frame+1 if(!is.null(foldervid)){ filename &lt;- sprintf(&quot;map_%06d.png&quot;, frame) png(file.path(foldervid,filename), width=800, height=800, pointsize=20) } plotMap(dem, water, paste0(&quot;year &quot;,i)) ######## } inactives &lt;- (Nts[i-1,]==0) for ( s in sample(seq_along(sites)[!inactives]) ){ if ( visu ) { ### visualisation ===== frame &lt;- frame+1 filename &lt;- sprintf(&quot;map_%08d.png&quot;, frame) png(file.path(foldervid,filename), width=800, height=800, pointsize=20) plotMap(dem,water,paste0(&quot;year &quot;, i)) ######## } city &lt;- NULL Ips[[s]] &lt;- Gpd( #compute new population for the sites Ips[[s]], K = Ks[[s]], p_offspring = prob_birth[sites$culture[s]], prob = prob_survive[sites$culture[s]] ) newN &lt;- nrow(Ips[[s]]) #count population size if(newN &gt;= (Ks[[s]])){ #if new population is more than carrying capacity: migration scenario migrants &lt;- newN - round(Ks[[s]]*0.9) ##Creation of new city new_site &lt;- NULL #if(sites$culture[s]==&quot;F&quot;)print(paste(&quot;possib&quot;,migrants, (minimals[sites$culture[s]]*sites$Ks[s]))) tmp &lt;- Nts[i-1,] tmp[Nts[i,] &gt; 0] &lt;- Nts[i, Nts[i,] &gt; 0] #tmp=tmp+sqrt(sites$Ks) havemoved &lt;- F if (migrants &gt;= (minimals[sites$culture[s]]*sites$Ks[s]) &amp; runif(1)&lt;prob_split[sites$culture[s]] ){ #if supopulation &gt; 10 people, 10% chance of creation of a new city #print(paste(&quot;look for new spot for &quot;,migrants, &quot;from site&quot;,s,&quot;culture&quot;,sites$culture[s])) #mean of area of influence infarea &lt;- (sqrt(tmp)+penal_cul[cultures]) * buffersettl buffersize &lt;- rnorm(length(infarea), infarea, infarea * 0.1) buffersize[tmp==0] &lt;- 0.00001 territory &lt;- erase(viable, buffer(sites, buffersize)) if( length(territory)&gt;0 ){ #print(paste(&quot;found new spot&quot;,migrants)) ##select a new site given its distance to the old one and the ressourcesource available in ressources d2 &lt;- logisticdecay( sites[s], dem, x=20000*cul_ext[sites$culture[s]] ) w &lt;- (0.7 * d2 + 0.3*ressources) / (0.7*minmax(d2)[2] + 0.3*minmax(ressources)[2]) new_site &lt;- terra::spatSample( x=mask( w * logisticdecay(sites[s], dem, k=0.00002, x=20000*cul_ext[sites$culture[s]]), territory), size=1, method=&quot;weights&quot;, xy=T)[1:2] new_site &lt;- vect(new_site, geom=c(&quot;x&quot;,&quot;y&quot;)) if ( length(new_site)&gt;0 &amp; all(!is.na(crds(new_site))) ){ ##add new site to site listes ##initialise population struc of new site #print(paste(&quot;total sites:&quot;,length(Ips))) #print(paste(&quot;dim Nts:&quot;,dim(Nts)[2])) #print(paste(&quot;site sf Nts:&quot;,length(sites))) Ips[[length(Ips)+1]] &lt;- initpopstruc(n=migrants) #initialise a fake populaition, will be updated by real migrants later new_site$culture &lt;- sites$culture[s] new_site$Ks &lt;- round(initKs( Kbase, sites=new_site, ressources, sizeex=&quot;F&quot;, rate=0.45)) if(log){ print(paste0(&quot;new settlement (&quot;, sites$culture[s], &quot;) of K &quot;, new_site$Ks, &quot; and pop &quot;, migrants)) } sites &lt;- rbind(sites, new_site) Ks[length(Ks)+1] &lt;- new_site$Ks city &lt;- length(Ips) Nts &lt;- cbind(Nts, rep(0,ts+1)) Nts[i, city] &lt;- migrants cultures &lt;- c(cultures, cultures[s]) #print(paste(&quot;new site sf Nts:&quot;,length(sites))) #print(paste(&quot;new dim Nts:&quot;,dim(Nts)[2])) #print(paste(&quot;new total sites:&quot;,length(Ips))) havemoved &lt;- T } } } ## if no creation of new city happen, there is a certain probability that people will move if( length(new_site)==0 &amp;&amp; runif(1) &lt; prob_move[sites$culture[s]] ){ #getj att &lt;- extract(ressources,sites)[,2] space &lt;- sites$Ks - (Nts[i-1,] + migrants) dis &lt;- extract(logisticdecay(sites[s], dem, k=0.00002, x=1), sites)[,2] attractivity &lt;- att * space * dis #attractivity=attractivity*(1+10*(sites$culture[s]==sites$culture)) #4 times more likely to go to similar culture attractivity[s] &lt;- min(attractivity)-1 attractivity &lt;- exp(attractivity)/sum(exp(attractivity)) attractivity[Nts[i-1,]&lt;10] &lt;- 0 attractivity[sites$culture!=sites$culture[s]] &lt;- 0 if(any(is.na(attractivity))){ if(log)print(attractivity) attractivity[is.na(attractivity)] &lt;- 0 } city &lt;- sample(size=1, x=seq_along(sites), prob=attractivity) Nts[i,city] &lt;- Nts[i-1,city] + migrants if(log){ print(paste(migrants, &quot;migrant from&quot;, sites$culture[s], &quot;to&quot;, sites$culture[city])) } havemoved &lt;- T } if( havemoved ){ #print(paste(&quot;old spot&quot;,migrants,&quot; for &quot;,nrow(Ips[[s]]))) #print(paste(&quot;old new spot&quot;,migrants,&quot; for &quot;,nrow(Ips[[city]]))) #if(city&gt;length(Ips))print(paste(&quot;problem, migrants:&quot;,migrants)) #print(paste(&quot;the other:&quot;,city)) Ips[c(s,city)] &lt;- changePopSize( loosingPop=Ips[[s]], winingPop=Ips[[city]], size=migrants ) newN &lt;- newN - migrants #print(paste(&quot;loosing &quot;,newN,&quot; vs &quot;,nrow(Ips[[s]]))) #print(paste(&quot;wining &quot;,newN,&quot; vs &quot;,nrow(Ips[[city]]))) } } Nts[i,s] &lt;- newN if (visu){ ###visualisation========= sitescols &lt;- rep(1,length(sites)) siteslwd &lt;- rep(1,length(sites)) ii=NULL if(!is.null(city)){ sitescols[s] &lt;- &quot;yellow&quot; sitescols[city] &lt;- &quot;red&quot; siteslwd[s] &lt;- 3 siteslwd[city] &lt;- 3 ii &lt;- st_cast(st_combine(st_as_sf(sites[c(s, city)])), &quot;LINESTRING&quot;) } if (!is.null(ii)){ plot(ii ,add=T) } tmp &lt;- Nts[i-1,] tmp[Nts[i,]&gt;0] &lt;- Nts[i,Nts[i,]&gt;0] plot(sites, cex=(as.integer(Nts[i,]&gt;0) * 0.3 + Nts[i,]/200), pch=21, add=T, bg=rainbow(2, alpha=0.6)[as.factor(sites$culture)], lwd=siteslwd, col=sitescols) dev.off() ###======================= } } if(visumin){ plot(sites, cex=(as.integer(Nts[i,]&gt;0) * 0.3 + Nts[i,]/200), pch=21, add=T, bg=rainbow(2, alpha=0.6)[as.factor(sites$culture)]) } potentialfighters &lt;- which(sites$culture==&quot;F&quot; &amp; Nts[i,]&gt;50) for (s in sample(x=potentialfighters, size=round(length(potentialfighters)*probfight))){ buff &lt;- bufferatack potentialvictims &lt;- which(sites$culture !=sites$culture[s] &amp; Nts[i,]&gt;0) clash &lt;- whotouch(s, sites, Ne=Nts[i,], buffersize=buff) clash=clash[!is.na(clash)] if(length(clash)&gt;0 ){ if(length(clash) == 1){ attack &lt;- clash } else { attack &lt;- sample(clash, 1) } newns &lt;- fightbetterloss(Ne=Nts[i,], a=s, b=attack,log=log) casualties &lt;- sum(Nts[i, c(s,attack)] - newns[c(s,attack)]) warcasualties[i] &lt;- casualties sizew &lt;- casualties^2/4000 if(wplot){ #warplot could be use to deposit evidence of conflicts warpoints(sites, s, attack, Ne=Nts[i,], buffersize=buff, sizewar=sizew+0.5,plot=wplot) } #effectively kill people in population (should be done taking into account age pyramid to be more realistic) Ips[[s]] &lt;- changePopSize(loosingPop=Ips[[s]], size=(Nts[i,s] - newns[s])) Ips[[attack]] &lt;- changePopSize(loosingPop=Ips[[attack]], size=(Nts[i, attack] - newns[attack])) Nts[i,] &lt;- newns if(log){ print(paste0(&quot;fight : #&quot;, s, &quot; (&quot;, cultures[s], &quot;) left with &quot;, Nts[i,s], &quot; (bef:&quot;, Nts[i-1,s], &quot;) ind., attacked: #&quot;, attack, &quot; (&quot;, cultures[attack], &quot;) left with &quot;, Nts[i,attack], &quot; (bef:&quot;, Nts[i-1,attack],&quot;) ind., #death=&quot;,casualties)) } } } if(visumin &amp;&amp; !is.null(foldervid)) dev.off() } return(list(Nts=Nts, warcasualties=warcasualties, Ips=Ips, sites=sites)) } 5.4 Explore simulation The list onesimu contains a few interesting info: Nts &lt;- onesimu$Nts # population at each timestep warcasualties &lt;- onesimu$warcasualties #death by war at each time step Nts is a \\(n \\times m\\) matrix where n is the number of time step of the simulaiton, and m the total number of sites that appears during the simulation, including the one that diseapers. With this Nts matrix we can already see a couple of thing: ncol(Nts)= 154, the total number of place occupied and other in the next chunk: indmax=which(max(Nts)==Nts,arr.ind = T) indmax[1] #timestep when biggest settlement was the biggest ## [1] 5 indmax[2] #id of the biggest settlement ## [1] 3 max(Nts) #max number of people during the simulation ## [1] 152 We can also look at the dynamics throught time, and explore how each individual sites are growing: i &lt;- ts plot(1, 1, type=&quot;n&quot;, xlim=c(0,i), ylim=c(0,max(Nts)), xlab=&quot;time&quot;, ylab=&quot;popsize&quot;) nill &lt;- lapply(1:ncol(Nts),function(j)lines(Nts[,j])) plot(apply(Nts, 1, sum)[1:i], xlab=&quot;time&quot;, ylab=&quot;popsize&quot;) Figure 5.2: left: Growth of each site, right: overall growth Combing onesimu$site with Nts we can compare the initial condition with the state of the worldat the end: plot(onesimu$site, cex=(Nts[1,]/max(Nts))*10, pch=21, bg=rainbow(2, alpha=0.6)[as.factor(onesimu$site$culture)]) plot(onesimu$site, cex=(Nts[nrow(Nts),]/max(Nts))*10, pch=21, bg=rainbow(2, alpha=0.6)[as.factor(onesimu$site$culture)]) Figure 5.3: left: sites at initialisation, right: sites at the end of the simulation Or look at the dynamics through time but divided by culture, and comparing with the war casualties plot(warcasualties[1:(i-1)], lwd=2, col=&quot;green&quot;, type=&quot;h&quot;, yaxt=&quot;n&quot;, ylab=&quot;&quot;) # Plot war casualties par(new=T) plot(1, 1, type=&quot;n&quot;, xlim=c(0,i), ylim=c(0,max(Nts)), xlab=&quot;time&quot;, ylab=&quot;popsize&quot;) na=lapply(1:ncol(Nts), function(i)lines(Nts[,i], col=c(&quot;red&quot;,&quot;blue&quot;)[as.factor(sites$culture)[i]])) plot(warcasualties[1:(i-1)], lwd=2, col=&quot;green&quot;, type=&quot;h&quot;, yaxt=&quot;n&quot;, ylab=&quot;&quot;) # Plot war casualties par(new=T) growF &lt;- apply(Nts[1:(i-1), sites$culture==&quot;F&quot;,drop=F], 1, sum) # Sum of &#39;F&#39; culture values excluding last row growHG &lt;- apply(Nts[1:(i-1), sites$culture==&quot;HG&quot;,drop=F], 1, sum) # Sum of &#39;HG&#39; culture values excluding last row plot(growF, col=&quot;red&quot;, type=&quot;l&quot;, lwd=2, ylim=c(0, max(growF, growHG))) # Plot growth of &#39;F&#39; culture points(growHG, col=&quot;blue&quot;, lwd=2, type=&quot;l&quot;) # Add points for growth of &#39;HG&#39; culture legend(&quot;topleft&quot;,legend=c(&quot;F&quot;,&quot;HG&quot;,&quot;war&quot;),col=c(&quot;red&quot;,&quot;blue&quot;,&quot;green&quot;),lwd=1,title=&quot;culture&quot;) Figure 5.4: left: Growth of eachsite for both culturles, right: combined growth The dynamics of the simulations will heavily depend on the initial conditions and the parameters. As an example, with these parameters, it is not rare that the Farmers, due to the conflict mechanisms that we described in Chapter 4, will destroy themselves by going to war too often, as one can see in the Figure 5.6 below. This can be avoided with the current implementation by setting a small probfight value to 0.1; however, it is important to remember that this could have been achieved in various ways. A more clever approach would have been for Farmers to ‘estimate’ the likelihood of them losing the battle, perhaps by fighting only safe battles, against much smaller settlements. In order to find the right simulation, the one that will show the dynamics you want your participants to detect, you will need to play with these parameters and run several replications of the simulation in order to find the right one. The script scriptmini.R was designed to do so for the original challenge, here we adapted it for a more generic need, to re-run simulation using the world you have generated throughout this book. par(mar=c(0,0,0,0)) height.ras=rast(&quot;data_toshare/dem_raster.tiff&quot;) height.wat=height.ras height.wat[height.wat&gt;mean(height.wat[])]=NA height.groups=height.ras maxh=max(height.ras[],na.rm=T) height.groups[height.groups&lt;mean(height.groups[])]=NA height.groups[height.groups&lt;(maxh*.7)]=1 height.groups[height.groups&gt;(maxh*.7)]=200 height.groups[is.na(height.groups)]=-1 height.poly=as.polygons(height.groups) viable=makeValid(height.poly[2,]) sites=vect(&quot;data_tmp/allsites.shp&quot;) ts=10 allsim=list() #we will store te multiplesimulation, in this list to play with it, with a proper setup we stored results in files as simulation are too big to be kept in memory. for(expname in paste0(&quot;exp&quot;,1:9)){ print(paste0(&quot;Starting simulation &quot;,expname)) onesimu=run_simulation ( sites=sites, viable=viable, dem=height.ras, ressources=ress, water=height.wat, foldervid=NULL, visu=F, visumin=F, log=F, ts=ts, Kbase=c(&quot;HG&quot;=35, &quot;F&quot;=110), cul_ext=c(&quot;HG&quot;=7, &quot;F&quot;=6), penal_cul=c(&quot;HG&quot;=4, &quot;F&quot;=5), prob_birth=c(&quot;HG&quot;=0.4, &quot;F&quot;=0.75), prob_survive=c(&quot;HG&quot;=0.8, &quot;F&quot;=0.65), prob_split=c(&quot;HG&quot;=0.5, &quot;F&quot;=0.6), minimals=c(&quot;HG&quot;=0.14,&quot;F&quot;=0.20), probfight=0.5, bufferatack=1000, prob_move=c(&quot;HG&quot;=0.2, &quot;F&quot;=0.1) ) Nts=onesimu$Nts warcasualties=onesimu$warcasualties sites=onesimu$sites i=min(ts,which(apply(Nts,1,sum)==0)) saveRDS(file=file.path(foldtmp,paste0(expname,&quot;_all.RDS&quot;)),onesimu) saveRDS(file=file.path(foldtmp,paste0(expname,&quot;_sitesRast.RDS&quot;)),onesimu$sites) plotMap(height.ras,height.wat,paste0(&quot;year &quot;,i)) plot(sites,cex=(as.integer(Nts[i,]&gt;0)*0.3+Nts[i,]/200),pch=21,add=T,bg=rainbow(2,alpha=.6)[as.factor(sites$culture)]) growF=apply(Nts[1:i,sites$culture==&quot;F&quot;],1,sum) growHG=apply(Nts[1:i,sites$culture==&quot;HG&quot;],1,sum) growT=apply(Nts[1:i,],1,sum) allsim[[expname]]=cbind.data.frame(growF,growHG,growT,warcasualties[1:i],rep(expname,i)) } ## [1] &quot;Starting simulation exp1&quot; ## [1] &quot;Starting simulation exp2&quot; ## [1] &quot;Starting simulation exp3&quot; ## [1] &quot;Starting simulation exp4&quot; ## [1] &quot;Starting simulation exp5&quot; ## [1] &quot;Starting simulation exp6&quot; ## [1] &quot;Starting simulation exp7&quot; ## [1] &quot;Starting simulation exp8&quot; ## [1] &quot;Starting simulation exp9&quot; Figure 5.5: Final state of multiple simulations with the exact same parameters. gnmax=max(sapply(allsim,function(i)max(i$growT))) plot(1,i,xlim=c(0,10),ylim=c(0,gnmax),type=&quot;n&quot;,main=&quot;Overal growth&quot;,ylab=&quot;number of individual&quot;,xlab=&quot;time&quot;) na=lapply(allsim,function(i)lines(i$growT)) gnmax=max(sapply(allsim,function(i)max(i[,c(&quot;growF&quot;,&quot;growHG&quot;)]))) plot(1,i,xlim=c(0,10),ylim=c(0,gnmax),type=&quot;n&quot;,main=&quot;Growth per culture&quot;,ylab=&quot;number of individual&quot;,xlab=&quot;time&quot;) na=lapply(allsim,function(i){lines(i$growHG,col=&quot;red&quot;);lines(i$growF,col=&quot;green&quot;)}) gnmax=max(sapply(allsim,function(i)max(i$warcasualties))) plot(1,i,xlim=c(0,10),ylim=c(0,gnmax),type=&quot;n&quot;,main=&quot;War Casualties&quot;,ylab=&quot;number of individual&quot;,xlab=&quot;time&quot;) na=lapply(allsim,function(i)lines(i$warcasualties)) Figure 5.6: Comparing multiple runs Once you found the right simulation we can use the populations this simulaiton created to generated an archaeological records, based on law we will describe in the next chapter. "],["recgen.html", "6 Record formation 6.1 Theoretical model 6.2 Implementation 6.3 Minimal example and simulation", " 6 Record formation 6.1 Theoretical model We have produced a computational model, replicating the process of formation of the archaeological record in a specific site, accounting for different variables. In order to consider the possible relation between those variables, we have decided to express them as a Directed Acyclic Graph (DAG), following the ECS-DAG methodology proposed by Ferguson et al. (2020). In this case, however, and because our goal is to obtain an accurate initial template for the development of the model, we have not considered setting an exposure variable, but rather to propose different theoretical directional and causal relationships without relying on the conventional exposure-outcome assessment. Therefore, highlights in the graph below do not intend to differentiate among confounders, mediators, etc., but to emphasize the possible weight that each variable has for producing the outcome (produced waste by \\(m^2\\)). ## Loading required package: DiagrammeR ## ## Attaching package: &#39;DiagrammeR&#39; ## The following objects are masked from &#39;package:igraph&#39;: ## ## count_automorphisms, get_edge_ids The above graph depicts a very schematic way to represent the formation of the archaeological record (\\(W/m^2\\)). In this case, we are interested in radiocarbon dating, focusing on bone samples; that is the number (and range) of potential 14C dates produced in a site, and this is key to understand the way in which we produced the model. We are aware that generating a specific archaeological palimpsest depends on a large quantity of variables, some of which may be controlled by field archaeologists and some of which may not (or are not). Therefore, the graph above must be read as follows. Asides from the outcome variable, the exposure variables, Pop (group size), Anthropogenic deposition rates and Natural deposition rates (depicted in orange), are designed in a way that they can be activated for the model to infer them, they can be provided by the archaeologist, or they can be simply ignored. In the most elementary case, the archaeologist could introduce directly the outcome variable (\\(W/m^2\\)) without taking into account anything else, if she/he is confident to provide an accurate value. However, in most cases, this information will not be known to the field researcher, and in this situation, these variables can be modeled to different extents of complexity. While, for example, in the case of the most simple simulation, the field archaeologist could ignore deposition rates. In this case, and because we are only considering 14C dates, then the quantity of the sample becomes broadly proportional to the number of people present in a site, and thus it could be modeled like this. However, deposition rates are difficult to ignore if we want to account for the formation process of the archaeological record. If these are known to the archaeologists that developed fieldwork, then this would be the behaviour of the model for creating \\(W/m^2\\). Finally, if these are not known to the archaeologist, but they want to model them, then we would go back to the first DAG. In this case, keep reading for the specificities of the model. This is how each variable works specifically: Waste produced (\\(W/m^2\\) or just \\(W\\)) (outcome variable): Archaeological record present per \\(m^2\\). Since we are considering C14 dates coming from bone, this is the quantity of C14 samples present a time t and at a depth d. Thus, this variable depends on time and depth. \\[W(t,d)=function(t,d)\\]. We have considered this a discrete variable (number of potential samples) with range \\([0,\\propto K]\\). Defined in code as W. The variables time (\\(T\\)) and depth (\\(D\\)) must also be defined. Thus T describes the time, discretely, in years, of existence of the site. For the simulation, this time will be known. Thus, in the simulation \\(T_{birth}=(T=0)\\) and \\(T_{max}=T_{death}\\). Values of \\(T\\) are represented by \\(t\\). D is the depth, in cms, where each specific sample is deposited at a time \\(t\\). This is a continuous variable with range \\([0,\\infty]\\), and for any \\(t\\), we know that \\(0\\leq D(t)\\leq d_0\\) and that \\[D(t)=max\\{D(t-1)-r,0\\} \\tag{1},\\\\ t=1,2,...,T_{max},\\] where \\(r\\) is the natural deposition rates. Population (group size) (\\(P\\)): Discrete variable with range \\([0,\\sim K]\\). Population group size is key to determine anthropogenic deposition rates (which in turn determine \\(W\\)) but it can also be used as a proxy to determine \\(W\\) in case deposition rates are not accounted for since, in any case, \\(W \\propto Pop\\). The model includes the possibility to create a stochastic demographic process (see below), but the user can also set fix values for population or set the parameters in a way that the stochastic process is constrained. In any case, the user does have to provide a value for the carrying capacity (\\(K\\)) as this will limit population growth. Defined in code as Pop. Anthropogenic deposition (\\(A\\)): Amount of bone kilogram/year deposited by each group at a time \\(t\\). This is a continuous variable, with range \\([0,\\infty]\\), measured in Kg. See Anthropogenic deposition protocol for full definition. Natural deposition rates (\\(r\\)): This is a continuous variable, with range \\([0,0.5]\\), measured in cm/year. Because, under normal circumstances, bones are thicker than any possible amount of earth deposited per year, even in a perfectly uncorrupted archaeological sequence (which is what is being modelled here), those would occupy microstrats belonging to different years. This variable is one of the responsibles for how many microstrats will each bone be occupying. See Depth protocol to understand how it works. Defined in code as r. Site function: Has not been considered as a variable per se but, rather, as a group of variables, which are defined by the functionality of the site, and that condition the outcome of \\(A\\). See Anthropogenic deposition protocol to understand how these are modelled. Site type: Site type (whether a site is a cave, a rockshelter or an open-air camp) can heavily condition the deposition rates at any archaeological location. However, in order to reduce uncertainty and not make an overcomplex model, we have decided to embed this condition, and the possible expert knowledge related to it, within the definition of the variable \\(r\\). 6.2 Implementation This simulation for the creation of the archaeological record has been divided in three phases, or protocols. The first one has been described in the previous chapter, where we populated the site, according to a stochastic demographic process on top of which migration and war events occurred. Here we focus on how these populations generate waste, and how this waste will be stored in the archaeological record and where. This is done in two steps ; we first generate a sample of waste for each time \\(t\\) and, in second step, we distribute the amount of waste for each depth \\(d\\). In order to fully understand the process, first we go through the protocols and functions used. 6.2.1 Population \\(Pop\\)-protocol Before anything, in order to account for the chronological occupation of the site, a demographic process must be produced ie sites need to be occupied. From these occupation a record sample is generated accordingly. This is what his done and described in the previous chapter, but here we will use the function Pop_stoch to generate a simple stochastic growth and apply our loss function to it in order to describe and test the other protocols. ?Pop_stoch 6.2.2 Anthropogenic deposition \\(A\\)-protocol This generates the amount of human waste produced for each \\(t\\) (In kilograms of bone). It is defined as: \\[A(t)= Pop(t)\\times O(t)\\times F(t) \\times G(t),\\\\ \\text{with}\\ t = 1,2,...,T_{death} \\tag{2}\\] Where: \\(Pop(t)\\) is a discrete variable representing the number of living people at a time \\(t\\). \\(O(t)\\) is the proportion of time of the year that the group spends in the camp for each time \\(t\\). It is a continuous variable with range \\([0,1]\\). In the code, this is defined as the parameter in_camp_stay and it is provided as the number of weeks, considering 52 the maximum number of weeks for a year. \\(F(t)\\) is the proportion of bone for the full weight of each Kg of an animal consumed for a time \\(t\\). This is a unit interval variable \\([0,1]\\). \\(G(t)\\) is the quantity, in Kg, of a (full-weight) animal consumed in a site by each individual at a time \\(t\\). This is a continuous variable with range \\([0,\\propto K]\\). This is a hierarchical variable, defined as follows: \\[G(t)=S(t)\\times\\frac{C(t)}{R(t)} = S(t)\\times\\frac{B(t)\\times M(t)}{R(t)} \\tag{3}\\] Where: \\(S(t)\\) is the proportion of meat consumed within the camp. \\([0,1]\\) \\(R(t)\\) is the quantity of kilocalories produced for each kilogram of meat. This is a continuous with range \\([1,2.5]\\). Total kilocalories have been divided by 1000 for easier computation. And \\(C(t)\\) is the quantity of kilocalories, extracted from meat, consumed by person for each time \\(t\\). It is in turned is defined as \\(C(t)=B(t) \\times M(t)\\), where \\(B(t)\\) is the quantity of kilocalories consumed by an individual for each \\(t\\), and it has a range \\([1.5,2.5]\\times 365\\). For easier computation, the actual average of kilocalories consumed per person/day has been divided by 1000. \\(M(t)\\) is the proportion of kilocalories coming from meat relating to the total calories consumed per each individual. \\([0,1]\\). As it can be seen, the above definitions embed the functionality of the site, which can be considered by the user. In practice, setting the variables \\(O(t) = 1\\) and \\(S(t) = 1\\) would eliminate site functionality. This would assume that a site is inhabited by all the group during the full year, and that this group consumes all the food in the site. Up to now, we have obtained the value for the variable \\(A\\), which is the amount of kilograms of bone produced at each time \\(t\\). If we consider 4 grams of non-heated bones per sample, we can extract the number of samples at time \\(t\\), which we cal \\(W(t)\\) \\[W(t)=\\mbox{Integer part of } \\Bigg(\\frac{1000 \\times A(t)}{4} \\Bigg) \\tag{4}\\] This protocol is captured in the function A_rates: You can inspect the A_rates code here: Show code Code ## Function 5. Simulate anthropogenic deposition A_rates &lt;- function(x, kcalpers = 2, kcalmeat_eat = 0.45, kcalmeat_prod = 1.5, in_camp_eat = 0.55, in_camp_stay = 13, kg = 0.07){ if (is.data.frame(x) == TRUE){ P &lt;- nrow(x) } else if (length(x) == 1){ P &lt;- x } else { P &lt;- length(x) } # Check variable values are within the defined ranges if (1.5 &lt;= kcalpers &amp; kcalpers &lt;= 2.5){ B &lt;- kcalpers*365 } else { stop(&#39;kcalpers must be within [1.5, 2.5]&#39;) } if (0 &lt;= kcalmeat_eat &amp; kcalmeat_eat &lt;= 1) { M &lt;- kcalmeat_eat } else { stop(&#39;kcalmeat_eat must be within [0, 1]&#39;) } if (1 &lt;= kcalmeat_prod &amp; kcalmeat_prod &lt; 2.5) { R &lt;- kcalmeat_prod } else { stop(&#39;kcalmeat_prod must be within [1, 25]&#39;) } if (0 &lt;= in_camp_eat &amp; in_camp_eat &lt;= 1){ S &lt;- in_camp_eat } else { stop(&#39;in_camp_eat must be within [0, 1]&#39;) } if (in_camp_stay &lt;= 52){ O &lt;- round(in_camp_stay/52,2) } else { stop(&#39;A year cannot have more than 52 weeks&#39;) } C &lt;- B*M G &lt;- (C * S) / R ## Quantity (in kg) of animal consumed per person in camp during year t A &lt;- P * O * kg * G ## kilograms of meat consumed within a camp by the group W &lt;- round((1000 * A) / 4) ## samples extracted from that meat return(W) } 6.2.3 Depth \\(D\\)-protocol This protocol is designed to respond to the fact that, despite a bone is deposited at time \\(t\\), it can occupy the estratigraphic space of many subsequent years until it is fully buried. The amount of years it takes to be fully buried depends on the thickness of the bone and the natural deposition rates \\(r\\). Since the thickest bones are usually the minority within a sample, while the majority is usually conformed of smaller bones, we have considered that bone presence decreases exponentially from \\(t_0\\), the year when it was deposited. Thus, this process is modeled as follows: \\[W_b(t_u,t_l)=W(t=t_l)\\times(1-e^{-\\lambda(t_u-t_l)}),\\ t_u &lt; t_l \\tag{5}\\] Where \\(W\\) as defined before, it is the number of samples deposited at a time \\(t_l\\) \\(W_b\\): is the number of samples buried at a time \\(t_u\\) from the ones generated at \\(t_l\\). Thus, \\(t_u \\geq t_l\\) \\(t_l\\) is the moment in time \\(t\\) when the samples \\(W\\) are deposited. \\(t_u\\) is each time \\(t\\), \\(t \\geq t_l\\), when the sample is being buried according to the parameter \\(r\\). At \\(t_{u_{max}}\\), the sample is completely buried. \\(t_{u_{max}}&gt;=t_u&gt;=t_l\\) \\(t_{u_{max}}\\) depends on two factors: \\(L\\) is maximum thickness of the thickest bone in the direction of the deposition, in cm. \\(r\\) is the natural deposition rate, in \\(\\mbox{cm/year}\\). For this model, we have considered the values 0.1, 0.2, 0.3, 0.4 and 0.5 Thus we can calculate the time it takes for a sample to be fully buried as \\[r = \\frac{L}{t_{u_{max}}} \\Rightarrow t_{u_{max}}=\\frac{L}{r} \\tag{6}\\] Estimation of \\(\\lambda\\) Now we need to estimate the exponential rate \\(\\lambda\\) in equation \\((5)\\). In order to do so, we consider the state of the model when \\(t_l=0\\) and \\(t_u=t_{u_{max}}\\), which marks the time at which the first samples deposited are completely buried. And consider the following \\[\\frac{W_b(t_u,t_l)}{W(t=t_l)} \\tag{7}\\] is the proportion of buried samples in \\(t_{u}\\) from the ones that were deposited in \\(t_l\\). Thus, if we substitute the time points specified above and plug in equation 5, we have that \\[\\frac{W_b(t_u=t_{u_{max}},t_l)}{W(t_l=1)}=1-e^{-t_{u_{max}}\\lambda} \\tag{8}\\] However, since we consider that all of the samples buried in \\(t_{u_{max}}\\), then the proportion on the left hand side is 1, and in this situation \\(\\lambda \\rightarrow \\infty\\) so we cannot use the equation. We can especify an error margin, \\(\\theta_e\\) to the proportion of buried samples in \\(t_{u_{max}}\\), say \\(99\\%\\). In this case, \\(\\lambda\\) can be easily computed as follows \\[\\theta_e=1-e^{-\\lambda t_{u_{max}}}\\] and thus, solving for \\(\\lambda\\) and taking logarithms on both sides, \\[\\lambda=\\frac{-log(1-\\theta_e)}{t_{u_{max}}} \\tag{9}\\] Where \\(\\theta_b&lt;1\\) is the proportion of buried samples in \\(t_{u_{max}}\\), which is calculated in equation 6. At the end, the function Rec_c runs D_along over a period of time, effectively producing an archaeological record. In the function Rec_c the user can consider whether to divide the total sample by the area of the site, or not. In addition, the user can use the modeled values (with the functions above) as an input to this function, or introduce them directly, for example, in the case that fix population rates per year were preferred. The function D_along captures this process. As usual, you can learn all about the parameters of D_rates and Rec_c ?D_along ?Rec_c As before, you can check the code here: Show code Code ## Function 6. Depth protocol deposition D_along &lt;- function(W_t, r, max_bone_thickness = c(&quot;m&quot;, &#39;s&#39;, &#39;l&#39;, &#39;vl&#39;), prop_buried = .9999){ # Define parameter r if(r &gt; 0.5) stop(&quot;values &gt; 0.5 are not accepted for param &#39;r&#39;&quot;) r &lt;- round(r, 1) # Constraints for parameter Pb if (prop_buried &gt;= 1) stop(&quot;Pb must be lower than 1&quot;) # Define parameter Max_bone_thickness (L) max_bone_thickness = match.arg(max_bone_thickness) if (max_bone_thickness == &#39;s&#39;){ L &lt;- 2.5 } else if (max_bone_thickness == &#39;m&#39;){ L &lt;- 5 } else if (max_bone_thickness == &#39;l&#39;){ L &lt;- 10 } else if (max_bone_thickness == &#39;vl&#39;){ L &lt;- 20 } # Define tmax tm &lt;- L/r # Estimate lambda l &lt;- -log(1 - prop_buried) / tm ss &lt;- rep(0, round(tm)) ## Vector to distribute samples over tl &lt;- 0 # Year where the sample is deposited tu &lt;- 1 # Year when it is covered for (i in 1:tm){ Wb &lt;- W_t * (1 - exp(-l*(tu-tl))) # Apply formula for tu Wbprev &lt;- W_t * (1 - exp(-l*((tu-1)-tl))) # Calculate for previous to tu ss[i] &lt;- round(Wb - Wbprev) # Number of samples for each year tu &lt;- tu + 1 } return(ss) } ## 7. Archaeological deposition record Rec_c &lt;- function(x, area, ts, InitBP, persqm = FALSE, ...){ ## Whether sqm division must be included or not if (persqm == TRUE){ x &lt;- x / area } ## Spread dates along different depths matdim &lt;- length(x) mat &lt;- matrix(nrow=matdim, ncol=matdim) for (i in 1:matdim){ new &lt;- D_along(x[i], ...) st &lt;- i - 1 pos &lt;- c(rep(0, st), new) pos &lt;- pos[1:matdim] mat[, i] &lt;- pos } mat[is.na(mat)] &lt;- 0 ## Names for columns (each year) years &lt;- seq(InitBP, InitBP-ts+1) colnames(mat) &lt;- paste(years,&quot;BP&quot;) ## Names for rows (each depth) # Extract arguments as a list extract_args &lt;- function(x, ...){ extras &lt;- list(...) return(list(extras=extras)) } dr &lt;- extract_args(D_along, ...) dr &lt;- dr$extras$r d &lt;- rev(cumsum(rep(dr, nrow(mat)))) ## computes depths rownames(mat) &lt;- paste0(&quot;d = &quot;, d, &quot; cm&quot;) return(mat) } 6.3 Minimal example and simulation With the above functions, the user can apply to any generative demographic process: (1) how many samples would a group of people produce in a site, according to different constraints and (2) how those dates distribute along the depth of a site under the assumption that there has not been any kind of record loss or post-depositional process (this will be modelled in a second layer of modelling). We can create a very basic and simple simulation to illustrate these processes following the parameters seen below (or defaults in function definitions above) following these steps: \\(Pop\\)-protocol. Using Pop_stoch, we generate a stochastic population dynamic that will give us the number of individuals for each year. \\(A\\)-protocol. Knowing the population size per year, we apply A_rates to each of them and generate the anthropogenic deposition, and get the number of samples per year. \\(D\\)-protocol. Knowing how many samples are generates each year, using Rec_c, we distribute them along the depth record. set.seed(1234) # 1. Pop-protocol Ps &lt;- Pop_stoch(pop_size=100, ts=100, K=150) # 2. A-protocol samp &lt;- unlist(lapply(Ps, A_rates)) ## Extract sample per time steps # 3. D-protocol Rec &lt;- Rec_c(samp, InitBP = 7500, ts = 100, r = 0.2, max_bone_thickness = &quot;m&quot;) With this we obtain, among other things, the distribution of the samples deposited for each date along the depth of the sit, and its comparison to the total population. For better comprehension, we have decided not to include all of the dates in the plot below, but only some of them as an example. The user can also check how the model works and its results here. The total number of population has been adapted for visualisation Notice the exponential decay in the amount of samples per depth layer for the same year. "],["recloss.html", "7 Record loss 7.1 Theoretical model 7.2 Implementation 7.3 Usage", " 7 Record loss 7.1 Theoretical model 7.1.1 General purpose Our main interest is showing and testing the effect of post-depositional processes on the archaeological record. The most obvious consequence of these effects relates to the direct loss of the record, while other implications can result in vertical and horizontal movements. Such movements, in turn, distort our comprehension of an archaeological site. Although the horizontal disposition of archaeological waste is important to understand different aspects, such as the activities deployed in a site, the function of that site, or it can even bring information on demographic processes, it will not be taken into account here. The reason for this is that modelling horizontal post-depositional movements (1) requires specific types of information which are frequently not available for field researchers, (2) adds complexity layers, the payoff of which, given its cost in terms of computation and comprehension of the model, may be insufficient and (3) as the focus of this model is on the reliability of 14C dates, vertical movements, which can obscure the construction of consistent stratigraphies, have been given preference. However, we understand that horizontal movement is a key aspect of the record formation. For example, it is frequent that hydric, gullying or sloping processes concentrate the archaeological waste in specific parts of the site. This would bias our perception of record concentration both horizontally, but also vertically. This record loss model assumes and normalises this when \\(W(t)\\) is retrieved from the previous record formation model. Nevertheless, this needs to be taken into account both if the user wants to introduce the values for \\(W(t)\\), and for the analysis with real archaeological data. There is not a single solution to control for this, as it largely depends on the excavated area and whether this is representative for the complete site or not. For example, many surveys spread across a site could actually be representative of that site’s record, whereas a single small survey in that same site is likely prone to present some specific bias due to post-depostional conditions. In this case, and in order to propose a correct variable, the user needs to make the proper calculations according to the site under study, but always bearing in mind that the model will work over the variable \\(W(t)\\) (waste produced), and can be computed to be expressed by \\(m^2\\). 7.1.2 Model layers Although in the future we will introduce additional layers which account for how the record moves vertically along the sequence, at this point, we are only considering the processes of record loss. 7.1.2.1 Loss of the record The are two main components for this part of the model. As shown in the graph below, the first one is time-dependent taphonomic loss, usually referring to long term process affecting the record for every \\(t\\) from the moment of its depositions to the present. The second one is direct action, which consider high-impact short term processes, such as human or animal action. These two variables can be applied together (advised), separately or not at all. These variables are modelled as follows: Long-term loss (\\(V_l(t)\\): Attending to the proxy we are considering (bones), we understand this variable as the processes which on the short term have little to none impact on bone preservation, but which can have severe consequences in the long term. These can refer, for example to the pH of the soil of the site, temperature and humidity changes, etc, and must be considered by the researcher or the user. We can subsume this information as a single probability \\(\\theta_l\\) within a binomial distribution. Then, the number of archaeological samples currently surviving from all the \\(W(t)\\) generated at a time \\(t\\) follows the distribution \\[V_l(t) \\sim Bi(W(t), \\theta_l) \\tag{1}\\] Although mathematically this computation is straightforward, it can be quite expensive computationally. Therefore, because we can assume independence of the effect altering the sample for each \\(t\\), and for computational efficiency, we compute \\(\\theta_l\\) as \\[\\theta_l = \\theta^{t+1950} \\tag{2}\\] Where \\(\\theta_l\\) is the considered probability, \\(t\\) is the time period before present where the sample was deposited and 1950 is the year that is conventionally treated as the present in radiocarbon dating. Short-term loss (\\(V_s(t+1)\\)): Most of the short-term, high-impact destruction of the archaeological record happens during the first year after deposition. Therefore, for this variable, we have considered the effect acting only once on the archaeological record, with an action, in any case, more agressive than the previous variable. Again, we can consider this a binomial distribution, where \\[V_s(t+1)\\sim Bi(W(t),\\theta_s)\\] where \\(\\theta_s\\) is the probability that the record survives. Again, this probability must be estimated according to previous information such as, for exemple, whether there has been human reoccupation or animal action at the site at \\(t +1\\). In this case, one good way to measure this is considering that for \\(\\theta_s = 0.5\\), half the record would be lost after this first year. All in all, if we put together these variables, then the remaining record for a site could be defined as \\[V(t)=V_l(t)+V_s(t+1)\\] There have been other quantitative approaches to the survival of the archaeological record, such as Surovell’s (2007, 2009) approaches. These are rather focused on the loss of complete sites, and not that much on the loss of specific records. And, as the authors acknowledge, they refer to specific geographic locations. However, with specific parameter combinations of our loss probabilities, we can obtain correlations higher than 0.98 with Surovell’s loss curves. In the case shown, for example, we have considered \\(\\theta_s=0.9\\) and \\(\\theta_l=0.9997\\) ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero 7.2 Implementation This layer of the model is less complex than the previous one. In this case, we only need to apply the loss of the record on the already created sample, according to the different loss probabilities. The functions for this are are short_loss and long loss. As before, to know more about these functions and their parameters you can look at the manual given in the package: ?short_loss ?long_loss Source You can check the source code for the short and long-term loss functions here: Show code Code ## Function 8. Short term taphonomic loss short_loss &lt;- function(x, theta_s){ res &lt;- c() for (i in 1:length(x)){ res[i] &lt;- rbinom(1, x[i], theta_s) } return(res) } ## Function 9. Long term taphonomic loss long_loss &lt;- function(x, theta_l, it){ t &lt;- it+1950 for (i in 1:ncol(x)){ prob &lt;- theta_l^(t-i) s &lt;- x[, i] for (k in 1:length(s)){ s[k] &lt;- rbinom(1, s[k], prob) } x[, i] &lt;- s } return(x) } 7.3 Usage With these functions, it is fairly easy to recompute the remaining record, and show it in the same format as before. Here we computed the record loss with \\(\\theta_s=0.9\\) and \\(\\theta_l=0.9997\\). Again, the user can play with these model and parameters using the online shiny app here. We use the Record generated in the previous chapter to which we apply the loss functions tlRec &lt;- apply(Rec, 2, short_loss, th_s) lRec &lt;- long_loss(tlRec, th_l, 7500) tlRec &lt;- t(lRec) We can then visualize the results of these lost using simple barplots: barplot(tlRec[c(1:4,77:79),], col = viridis(10), legend = colnames(Rec)[c(1:4,77:79)], xlab = &quot;Depth&quot;, ylab = &quot;nsamples&quot;, main = &quot;Distribution of specific years along depth with population&quot;) lines(Ps*300, col = &quot;darkred&quot;, lwd = 1.5) colnames(tlRec) &lt;- colnames(tRec) rownames(tlRec) &lt;- rownames(tRec) barplot(tRec, col = viridis(ncol(tRec)), #legend = colnames(Rec), xlab = &quot;Depth&quot;, ylab = &quot;nsamples&quot;, main = &quot;Distribution of samples for specific years&quot;) barplot(tlRec, col = viridis(ncol(tlRec)), #legend = colnames(Rec), xlab = &quot;Depth&quot;, ylab = &quot;nsamples&quot;, main = &quot;Distribution of samples for specific years after loss&quot;) "],["final-output.html", "8 Generate final archaeological record 8.1 Using sites size to generate deposit 8.2 Provide survey square and publicly available dates 8.3 Bonus maps", " 8 Generate final archaeological record Now that all the pieces are in place we need to combine them to generate files that can be shared with people in order for them to guess the original parameters. To summarize what we have so far: We simulated an artificial environment in Chapter 2. We localised sites on the map and defined a population growth process in Chapter 3. We defined rules to describe how population move and engage in conflict as they expand in Chapter 4. We simulated these process of growth, conflict and migration on the environement we designed, for extend periods of time in Chapter 5. We defined processes that use this simulated growth to generate an archaeological record in Chapter 6, We defined processes that transform this archaeological record using credible loss mechanisms 6. Here we will use all these previous steps to generate csvs that can be shared. In this chapter we will use the simulation run in the Chapter 5 ; if you want to generated new ‘fake’-dataset, different from the one that have been presented in the original challenge and that will be described in the chapter 9, you will want to run longer simulation, and choose among different run on the looks interesting/realistic. We provide a script ./scriptmini.R that to give some idea about how to automatize these run and how to choose among them. 8.1 Using sites size to generate deposit This has been done using file dateGeneration.R, we will describe it here now. First we get the rasters defining the environment (If you followed the book since the beginning they should be already in memory but in case you didn’t we we reload them here: height.ras &lt;- rast(file.path(&quot;data_toshare&quot;,&quot;dem_raster.tiff&quot;)) height.wat &lt;- height.ras height.wat[height.wat&gt;mean(height.wat[])] &lt;- NA Nts &lt;- readRDS(&quot;data_tmp/popSizeMatrix.RDS&quot;) plotMap(height.ras,height.wat) Using Rec_c , short_loss and long_loss, that we presented in the Chapter 6 &amp; 7, we generate deposit. This can take long time if the simulation included a lot of sites, spanning over long period of time. Here we first generate all deposit from the matrix Nts generated during the simulation of the Chapter 5 and then apply all loss functions together: #All in one: alldeposit=lapply(1:ncol(Nts),function(s)Rec_c(sapply(Nts[,s],A_rates), InitBP = 7500,ts=nrow(Nts),r = 0.2, max_bone_thickness = &quot;m&quot;)) allLosses=lapply(1:length(alldeposit),function(ind){ Rec=alldeposit[[ind]]; Rec=apply(Rec,2,short_loss,.6) #apply short loss Rec=long_loss(Rec,.9997,7500) #apply long loss return(Rec) }) rm(alldeposit) Once this is done, we have a list with all the potential dates available from all sites that have existed during our simulation. Realisticly, for any given site, not all the material will be recovered and even less of this material will be dated. To reflect that, we capped the maximum number of samples available per sites. To do so – and this was applied as is in the original challange presented in the Chapter 9, we assume that the biggest sites – the one with most media attention, most funding, will very likely get at most 30 dates. We thus subsampled the number of dates for smaller site given this ration of \\(\\frac{30}{n_{\\text{largest}}}\\). Here simulation have been greatly shortened, thus sites have way less samples. We thus cap the larges sites to have only 80% of the dates it generate, and sample the other sites using this ratio. maxSites=max(sapply(allLosses,sum)) nsample=round(sapply(allLosses,sum)*(30/maxSites)) allRemainingDates=lapply(seq_along(allLosses),function(r)extractDates(allLosses[[r]],n=nsample[r])) rm(allLosses) #object are big, need to free memory We then have a sample of date for every sites of our worlds; where the maximum number of dates is 30, for the biggest sites that existed in our simulation. Let’s looks at where these date are available, we first retrieve all sites saved in the simulation from 5: allsites=vect(file.path(foldtmp,&quot;allsites.shp&quot;)) if(class(allsites)[1] == &quot;PackedSpatVector&quot;) allsites=terra::vect(allsites) We can see where are the sites with more dates: expname=&quot;exn&quot; plotMap(height.ras,height.wat,paste0(&quot;final after losses&quot;)) plot(allsites,cex=3*(lengths(allRemainingDates)-1)/(29),pch=21,add=T,bg=rainbow(2,alpha=.6)[as.factor(allsites$culture[1:10])]) dates=unique(unlist(allRemainingDates)) dates=rev(sort(dates[!is.na(dates)])) plot(table(unlist(allRemainingDates))) totallDatesRemains=sapply(allRemainingDates,function(i)table(factor(i,levels=dates))) saveRDS(allRemainingDates,file=file.path(foldtmp,paste0(expname,&quot;_dates.RDS&quot;))) Figure 8.1: left: sites where dates are potentially available, size of the dot represent the number of dates, right: total number of dates by years Loading dates: dates=readRDS(file=file.path(foldtmp,paste0(expname,&quot;_dates.RDS&quot;))) Keep sites with foundable dates foundsites=allsites[lengths(dates)&gt;0,] foundsites$numdates=unlist(lengths(dates[lengths(dates)&gt;0])) founddates=dates[lengths(dates)&gt;0] Remove BP to the dates, add some random variation stdpool=c(20,30,40,50,60,80,100,120) founddates=lapply(founddates,sort) founddates=lapply(founddates,gsub,pattern=&quot; BP&quot;,replacement=&quot;&quot;) founddates=lapply(founddates,rev) founddates=lapply(founddates,function(i)paste0(i,&quot; ± &quot;,sample(stdpool,length(i),replace=T,prob=c(3,3,3,3,2,2,1,1)),&quot; BP&quot;)) foundsites$dates=sapply(founddates,paste0,collapse=&quot; | &quot;) plotMap(height.ras,height.wat) plot(foundsites,cex=foundsites$numdates/20+1,pch=21,bg=as.factor(foundsites$culture),add=T) 8.2 Provide survey square and publicly available dates To then share these date we choose to split the environment in grids and extract date by grid. For each grid, a csv is generated with the sites that have been found and for each site, their geolocalisation and the dates found at the site. leftdates &lt;- dates[lengths(dates)&gt;0] plotMap(height.ras,height.wat) plot(foundsites,cex=foundsites$numdates/20+.1,pch=21,bg=1,add=T) squares &lt;- st_make_grid(height.ras,.5) squares &lt;- st_bind_cols(squares,ID=1:length(squares)) ## Warning in st_bind_cols(squares, ID = 1:length(squares)): Use &#39;cbind&#39; instead when all arguments to be binded are of class sf. ## If you need to cbind a data.frame to an sf, use &#39;st_sf&#39; or &#39;dplyr::bind_cols&#39; (see the examples). plot(squares,add=T,col=adjustcolor(rainbow(nrow(squares)),.35)) text(st_coordinates(st_centroid(squares)),label=squares$ID,col=&quot;white&quot;) ## Warning: st_centroid assumes attributes are constant over geometries text(st_coordinates(st_centroid(squares)),label=squares$ID,col=&quot;white&quot;) ## Warning: st_centroid assumes attributes are constant over geometries Figure 8.2: The black dots represent the sites for which dates are available, the size of the circle reflect the amount of dates available. each square represent an area that can be selected by the users. In the archaoriddle original challenge we proposed to give the data from 4 grids that we selected oursleve to be sure the participants have a least some interesting information. Here we randomly pick up 4 squares with the only limits that they provide at least 5 settlements. You will want to change and manually population the vector selection in the chunk below to cover area with lot of sites and sites from differents cultures, from what can be seen in the Figure 8.2. plotMap(height.ras,height.wat) plot(foundsites,cex=foundsites$numdates/20+1,pch=21,bg=as.factor(foundsites$culture),add=T) selection=sample(1:nrow(squares),4) inter &lt;- st_intersection(st_as_sf(foundsites),squares[selection,]) ## Warning: attribute variables are assumed to be spatially constant throughout ## all geometries while(nrow(inter)&lt;5){ selection=sample(1:nrow(squares),4) inter &lt;- st_intersection(st_as_sf(foundsites),squares[selection,]) } plot(squares[selection,],add=T,col=adjustcolor(&quot;blue&quot;,.3)) plotMap(height.ras,height.wat) plot(st_geometry(inter),add=T,bg=rainbow(2,alpha=.6)[as.factor(inter$culture)],pch=21,cex=1+inter$numdates/10) Figure 8.3: Left: square publicly available ; right: sites surveyed within these publicly available squares. 8.2.1 Giving name to the publicly available data On the original challenge we used: fantasynamegenerators.com to choose nice, randomly generated name, to name the sites publicly available. You will see in the section 9.3.3 that we positionned them with cultural relevent group, although this has no impact on the results. Here we took all the name we had for the original challenge to randomly assignate some for you custom world. site_dist=st_distance(inter) potname=c(&quot;Farwallow&quot; ,&quot;Bearcall&quot; ,&quot;Dustscar&quot; ,&quot;Clearreach&quot; ,&quot;Rabbithole&quot;, &quot;Épibéliard&quot; ,&quot;Cololuçon&quot; ,&quot;Pulogne&quot; ,&quot;Haguemasse&quot; ,&quot;Auriteaux&quot; ,&quot;Bourville&quot; ,&quot;Banau&quot; ,&quot;Montnesse&quot; ,&quot;Bannet&quot; ,&quot;Alenlon&quot;, &quot;Roullac&quot; ,&quot;Genneville&quot; ,&quot;Vinlès&quot; ,&quot;Antonnet&quot; ,&quot;Courtou&quot; ,&quot;Beaulogne&quot; ,&quot;Coloville&quot; ,&quot;Sarsart&quot; ,&quot;Soilon&quot; ,&quot;Cololimar&quot;,&quot;Zava&quot; ,&quot;Catadrid&quot; ,&quot;Tegon&quot; ,&quot;Alicia&quot; ,&quot;Mulid&quot; ,&quot;Zararbella&quot; ,&quot;Malid&quot; ,&quot;Cásca&quot; ,&quot;Granalejos&quot; ,&quot;Segorez&quot; ,&quot;Terteixo&quot; ,&quot;Astumanca&quot; ,&quot;Galle&quot; ,&quot;Talona&quot; ,&quot;Girovega&quot; ,&quot;Albanada&quot; ,&quot;Nadoba&quot; ,&quot;Senca&quot; ,&quot;Vallanca&quot; ,&quot;Taville&quot;) u=0 while(length(potname)&lt;nrow(inter)){ u=u+1 potname=c(potname,paste0(potname,u)) } sitesnames=sample(potname,nrow(inter)) inter=cbind(inter,sitesnames) plotMap(height.ras,height.wat) plot(st_geometry(inter),add=T,bg=rainbow(2,alpha=.6)[as.factor(inter$culture)],pch=21,cex=1+inter$numdates/10) text(st_coordinates(inter),inter$sitesnames,cex=.8,pos=3) 8.2.2 Export and download dates Export csv and data for each public square for(g in selection){ curr=inter[inter$ID==g,] coords=st_coordinates(curr) write.csv(file=file.path(&quot;data_toshare&quot;,paste0(&quot;square_&quot;,g,&quot;.csv&quot;)),cbind.data.frame(sitename=curr$sitesnames,lon=coords[,1],lat=coords[,2],dates=curr$dates,economy=curr$culture)) } The csvs generate for this specific run of the bookdown can be download for each square: square 47 square 29 square 20 square 5 Export csv and data for all ever square allsites=st_intersection(st_as_sf(foundsites),squares[-selection,]) ## Warning: attribute variables are assumed to be spatially constant throughout ## all geometries for(g in (1:nrow(squares))[-selection]){ curr=allsites[allsites$ID==g,] coords=st_coordinates(curr) write.csv(file=file.path(&quot;data_tmp&quot;,paste0(&quot;square_&quot;,g,&quot;.csv&quot;)),cbind.data.frame(lon=coords[,1],lat=coords[,2],dates=curr$dates,economy=curr$culture)) } ld=strsplit(inter$dates,&quot; \\\\| &quot;) ld=lapply(ld,function(i)gsub(&quot; ± .*&quot;,&quot;&quot;,i)) inter$start=sapply(ld,max) inter$end=sapply(ld,min) The csvs generate for all squares can be downloaded here 8.3 Bonus maps Some maps we use and did for fake papers and social media etc… ## plotting oldschool map par(mar=c(0,0,0,0),oma=c(0,0,0,0)) plot(height.wat,col=adjustcolor(&quot;light blue&quot;,.4),reset=F,legend=F,axes=F) contour(height.ras,levels=seq(5,300,15),axes=F,ann=F,lwd=.1,labels=&quot;&quot;,bg=&quot;light blue&quot;,add=T) plot(st_geometry(inter),pch=4,add=T,lwd=.3) plot(st_geometry(inter),pch=20,add=T,lwd=.2,cex=.8,col=as.factor(inter$culture),alpha=.8) old=inter[inter$start&gt;7180,] ne=inter[inter$start&gt;6800 &amp; inter$end&lt;6900,] plot(height.wat,col=adjustcolor(&quot;light blue&quot;,.4),reset=F,legend=F,axes=F) contour(height.ras,levels=seq(5,300,15),axes=F,ann=F,lwd=.1,labels=&quot;&quot;,bg=&quot;light blue&quot;,add=T) plot(st_geometry(old),pch=4,add=T,lwd=.5,cex=3) plot(st_geometry(old),pch=20,add=T,lwd=.2,cex=1.8,col=as.factor(old$culture),alpha=.8) plot(height.wat,col=adjustcolor(&quot;light blue&quot;,.4),reset=F,legend=F,axes=F) contour(height.ras,levels=seq(5,300,15),axes=F,ann=F,lwd=.1,labels=&quot;&quot;,bg=&quot;light blue&quot;,add=T) plot(st_geometry(inter),pch=4,add=T,lwd=.5,cex=3) plot(st_geometry(inter),pch=20,add=T,lwd=.2,cex=1.8,col=as.factor(inter$culture),alpha=.8) plot(height.wat,col=adjustcolor(&quot;light blue&quot;,.4),reset=F,legend=F,axes=F) contour(height.ras,levels=seq(5,300,15),axes=F,ann=F,lwd=.1,labels=&quot;&quot;,bg=&quot;light blue&quot;,add=T) plot(st_geometry(inter),pch=4,add=T,lwd=.5,cex=3) plot(st_geometry(inter),pch=20,add=T,lwd=.2,cex=1.8,col=as.factor(inter$culture),alpha=.8) plot(height.wat,col=adjustcolor(&quot;light blue&quot;,.4),reset=F,legend=F,axes=F) contour(height.ras,levels=seq(5,300,15),axes=F,ann=F,lwd=.1,labels=&quot;&quot;,bg=&quot;light blue&quot;,add=T) plot(st_geometry(ne),pch=4,add=T,lwd=.5,cex=3) plot(st_geometry(ne),pch=20,add=T,lwd=.2,cex=1.8,col=as.factor(ne$culture),alpha=.8) More maps for twitter: plot(height.wat,col=adjustcolor(&quot;light blue&quot;,.4),reset=F,legend=F,axes=F) contour(height.ras,levels=seq(5,300,15),axes=F,ann=F,lwd=.2,labels=&quot;&quot;,bg=&quot;light blue&quot;,add=T) plot(st_geometry(inter),pch=4,add=T,lwd=.5,cex=3) plot(st_geometry(inter),pch=20,add=T,lwd=.2,cex=1.8,col=as.factor(inter$culture),alpha=.8) sel=inter[c(20,2,15,36),] text(st_coordinates(sel),sel$sitesnames,cex=.8,pos=c(1,2,2,4)) text(x = -1, y = .7, labels = &quot;❤️&quot;, adj = c(0, 0), cex = 3, col = &quot;black&quot;, font = 1) text(x = -1, y = 1.2, labels = &quot;🔥 &quot;, adj = c(0, 0), cex = 3, col = &quot;black&quot;, font = 1) text(x = -1, y = 1.2, labels = &quot;⚔️ &quot;, adj = c(0, 0), cex = 3, col = &quot;black&quot;, font = 1) text(x = -1.2, y = 1.8/2, labels = &quot;???? &quot;, adj=c(0,0), cex = 3, col = &quot;black&quot;, font = 1) More maps!! (the coolest) plotMap(height.ras,height.wat) contour(height.ras,levels=seq(5,300,15),axes=F,ann=F,lwd=.2,labels=&quot;&quot;,bg=&quot;light blue&quot;,add=T) plot(st_geometry(inter),pch=4,add=T,lwd=.5,cex=3) plot(st_geometry(inter),pch=20,add=T,lwd=.2,cex=1.8,col=as.factor(inter$culture),alpha=.8) coastline=st_cast(st_as_sf(as.polygons(height.ras&gt;mean(height.ras[])))[2,],&quot;MULTILINESTRING&quot;) plot(coastline,add=T,lwd=1.1,col=&quot;black&quot;) sel=inter[c(20,2,15,36),] text(st_coordinates(sel),sel$sitesnames,cex=.8,pos=c(1,2,2,4)) plotMap(height.ras,height.wat) contour(height.ras,levels=seq(5,300,15),axes=F,ann=F,lwd=.2,labels=&quot;&quot;,bg=&quot;light blue&quot;,add=T) plot(st_geometry(inter),pch=4,add=T,lwd=.5,cex=3) plot(st_geometry(inter),pch=20,add=T,lwd=.2,cex=1.8,col=as.factor(inter$culture),alpha=.8) coastline=st_cast(st_as_sf(as.polygons(height.ras&gt;mean(height.ras[])))[2,],&quot;MULTILINESTRING&quot;) plot(coastline,add=T,lwd=1.1,col=&quot;black&quot;) sel=inter[c(20,2,15,36),] text(st_coordinates(sel),sel$sitesnames,cex=.8,pos=c(1,2,2,4)) #text(x = -1, y = .7, labels = &quot;🍻&quot;, adj = c(0, 0), cex = 3, col = &quot;black&quot;, font = 1) text(x = -3.4, y = .7, labels = &quot;🍻 &quot;, adj = c(0, 0), cex = 10, col = &quot;black&quot;, font = 1) text(x = -1.8, y = .7, labels = &quot;👍 &quot;, adj = c(0, 0), cex = 8, col = &quot;black&quot;, font = 1) text(x = -.5, y = .7, labels = &quot;🍔 &quot;, adj = c(0, 0), cex = 6, col = &quot;black&quot;, font = 1) #text(x = -1, y = 1.2, labels = &quot;⚔️ &quot;, adj = c(0, 0), cex = 3, col = &quot;black&quot;, font = 1) #text(x = -1.2, y = 1.8/2, labels = &quot;???? &quot;, adj=c(0,0), cex = 3, col = &quot;black&quot;, font = 1) "],["original-challenge.html", "9 Archaeoriddle: the original challenge 9.1 Context 9.2 Implementation 9.3 Let’s rewind the tape 9.4 The proposals", " 9 Archaeoriddle: the original challenge In this chapter we will here go through the whole steps we presented throughout Chapter 2 to 8, but in a condensed way and with the parameters used to generate the data shared during the Archaeoriddle original challenge. But before that we will we retrace the journey from the first discussions in the courtyard of the McDonald Institute in Cambridge to the final workshop in Belfast. We will then end the chapater by briefly discuss the proposal submitted for the challenge. 9.1 Context Here a bit of how all that happen, and the majors steps. 9.1.1 Original idea and first reflections 9.1.2 Birth of a Challenge After discussion within the member of the CDAL, we decided that a challenge-like event, with a cash-prize, would be the best way to convince people to participate. After the B/A grant has been awarded to Simon Carrignon, Alfredo Cortell and Enrico Crema, we thus decided to announce this challenge through various social media video and a website: https://theia.arch.cam.ac.uk/archaeoriddle/ 9.1.3 The British Academy Grant – November 2022 9.1.4 CAA Oxford – June 2023 Paper ID: 295 Paper Title: Archaeoriddle: A collaborative game to improve archaeological inference Abstract: Archaeoriddle is a collaborative research project aimed to improve archaeological hypothesis building by comparing different methodological proposals within a crowd-sourced and interactive working framework. To achieve this, we created a virtual world through computational simulation, emulating an interaction process between a hypothetical group of incumbent hunter-gatherers and a group of spreading farmers. This virtual world and its associated virtual archaeological datasets have been developed by members of the Computational and Digital Archaeology Laboratory (CDAL) of the University of Cambridge and can be consulted here (https://acortell3.shinyapps.io/Archaeo_riddle/). Archaeologists are well aware of the problems and limitations posed by its data. Methodological and theoretical advances have been made to tackle these challenges over the years. These include a wide range of multidisciplinary approaches, where key insights from chemistry or geology, but also mathematics and data science, are employed in order to improve and transcend the current limitations posed by the archaeological record. In particular, techniques based on statistical modelling and computational simulation, have known an exponential growth during these last years. Nevertheless, the robustness of these techniques are rarely evaluated formally, and their effective ability to reconstruct the human past is unknown. The limited samples available, and the destructive process of archaeological excavation further limits our ability to properly evaluate confounding factors such as research design and post-depositional processes, all of this contributing to archaeological uncertainty. In short, archaeologists are constrained by the data they have, and are very rarely able to reproduce large scale studies with significant sample sizes, able to offer a data poll where the development of current formal inferential methods can be reliably applied and contrasted. But this awareness is not enough. If we want to take archaeological formal inference one step further, we must make sure that the methods that are currently being used and developed are effective within our specific research field. This is the key objective of archaeoriddle: to offer a virtual platform and a shared dataset where different methodologies can be compared against each other, with the unique opportunity to be able to assess their ability to reconstruct the human past. This collaborative enterprise will provide an unique chance to (1) help understand and compare different methods applied to archaeology, and (2) propose a general and necessary reflection on archaeological methodology, as a foundation for promoting solid and shared methodological standards. Since the virtual interaction process has been created virtually, we know the exact parameters behind the farming expansion. However, interested participants are given only a very limited and indirect set of data and information about what happened. Using their preferred methods, we propose a game, following ideas by Axelrod (1980) and Rendell et al. (2010), where participants have to answer a series of questions related to this virtual expansion process. In this way, archaeoriddle is thought of as a game, but which also seeks for international collaboration to understand how different current theoretical and methodological proposals behave under a controlled and testable environment. The project was initially presented in the past CAA (in Oxford) and, therefore, this paper aims at briefly updating the project during a 5 min talk. In this edition of the CAA, we intend to focus on how tactical simulation and collaborative research can help improving methodological approaches to archaeological uncertainty. References: Axelrod, R. (1980) “Effective choice in the Prisoner’s Dilemma”, Journal of Conflict Resolution, 24(1), pp. 3-25. DOI: https://doi.org/10.1177/002200278002400101 Rendell, L., Boyd, R., Cownden, D. Enquist, M., Eriksson, K., Feldman, M. W., Fogarty, L., Ghirlanda, S., Lillicrap, T., Laland, K. N. (2010). “Why copy others? Insights from the Social Learning Strategies tournament”, Science, 328(5975), pp. 208-213. DOI: http://doi.org/10.1126/science.1184719 Created on: Mon, 31 Oct 2022 18:04:39 GMT Last Modified: Mon, 12 Dec 2022 15:04:16 GMT Authors: - ac2320@cam.ac.uk - sc2297@cam.ac.uk - lmb211@cam.ac.uk - cbs41@cam.ac.uk - jl2094@cam.ac.uk - erc62@cam.ac.uk 9.1.5 The EAA call for proposal – August 2023 Dear colleague, We hope this e-mail finds you well. We are writing to you because we would like you to know on the Archaeoriddle project, a project directed to improve methods in Archaeology and Cultural Evolution. All of the information about it is here (https://theia.arch.cam.ac.uk/archaeoriddle/), but very briefly for now, in this project we have simulated an interaction process between groups of foragers and farmers and generated a “virtual” archaeological record and we now propose a game where participants will have to answer some of the questions proposed in the website with the data provided. Because we do know the actual answer, we believe this is a great way to test the inferential power of archaeological methods. This project will come to an end in a dedicated workshop in the next EAA Conference in Belfast (https://www.e-a-a.org/EAA2023) in late August - early September, where every participant will be able to present their results and proposed methods. We will finance £650 to the best ten proposals that we receive, with a non-extendable deadline on 16th June. This money can be used to attend to the conference or in any other way the ‘winner’ would prefer. The only requisite is that the winners of the award will have to present their proposal at the workshop. Finally, after the workshop, we will invite all participants (awardees and not) to contribute on a joint publication. You can have detailed information at the link above, under the tab “How to participate?”. Please, consider sending your proposal to us, either yourself, or by transmitting this to any other potential researcher who could be interested. We’d love to have you there and chat about how can we improve archaeological methods!! Best wishes Alfredo Cortell-Nicolau, Simon Carrignon, Xavier Rubio-Campillo and Enrico R. Crema On top of allowing participants to download the public dataset, and to select new square to survey, the website mentioned in the call listed a series of research questions aimed to guide participants in their endeavours. 9.1.5.1 Research Questions RQ1. The Poppy-chewers and Rabbit-skinners had a hostile relationship. RQ2. Poppy-chewers followed an exponential population growth, while Rabbit-Skinners eventually stalled, and as the Poppy-chewers population size started to increase, their population declined (Fig. 3). RQ3. The rate of dispersal varied across the landscape, with an average of ~0.62 Km/year, faster for the sea crossing (~1.11 Km/year) and a slight slow-down to ~0.57 Km/year for the Northeastern quadrant of the map. The website where these research question will disappear anytime soon, so here is a snapshot of the first page: But in case this disappear, the source to reproduce the website are also available in the github Archaeoriddle repository here. The call was issued in April 2023, after acceptance of the workshop at EAA, and 5 proposals were submitted. During the actual workshop, the whole project was summarised, the 5 proposal described by there respective authors, and the exact simulation reveiled. 9.2 Implementation The Challenge is basically built on top of a manually selected environment, generated using the tools describe in the previous chapter, on top of which a bunch of simulation have been run, and one has been selected. But as Simon Carrinon has done a poor job when he did the simulation he didn’t generate seeds, nor stored them, for each experiment; so we cannot re-run the original one EXACTLY as it happened. But he kept a lot of informations and output about it which, are stored in the folder general_results_selected_simu/ This should be enough to allow anyone to reconstruct most of what happened. Thus, a quick recap of the implementation of the original challenge: Using all the functions developed and described previously in this book, we : Generate an environment that look cools Choose a series of parameters and Run hundreds of simulations and choose the one who looked the coolest 😆. We used the script scriptmini.R to run one simulation, and then used a very simple and inelegant bash loop to parallelise muleiple run: for t in {1..5} ; do Rscript scriptmini.R newset_$t &gt; log_new_set$t 2&gt; log_new_set$t.err &amp; done One can see, on line 52 until the end of the scriptmini.R, that some graphes are saved. This will be used to quickly compare the different simulation and choose the good one. The simulation are also run with the parameter: visumin=TRUE, which allow to save images of population growth and settlement creation at every time step while the model is running, which can then be used to generate video of the whole process using simple script like: for i in test*_? ; do ffmpeg -i &quot;$i/map_000%3d.png&quot; &quot;${i}_out.mp4&quot; ; done #need better option for web display: for i in buffattack*.mp4 ; do ffmpeg -i $i -c:v libx264 -crf 23 -preset medium -vf format=yuv420p -c:a copy -movflags +faststart web_$i ; done where test* will the folder storing all output of simulation. We provide the simulation that were generated during this preliminary phase in the zenodo folder associated to the first paper published fomr the Archaeoriddle project in this archive (wrning, its 1.2GB). Here videos showing a selection of some of the scenario discarded while choosing the ultimate run: 9.3 Let’s rewind the tape Here we will summarise all the steps described in the book, but using the map, sites position and parameters used in the original Challenge. Load the original raster: original.ras &lt;- rast(&quot;data_original/east_narnia4x.tif&quot;) plot(original.ras, col=col_ramp(20), legend=F, reset=F) original.wat &lt;- original.ras original.wat[original.wat&gt;mean(original.wat[])] &lt;- NA plot(original.wat, col=&quot;lightblue&quot;, add=T, legend=F) original.groups &lt;- original.ras # taking back the raster created before original.groups[original.groups&lt;mean(original.groups[])] &lt;- NA #replace values that are below mean level (under water) original.groups[original.groups&lt;(maxh*.7)] &lt;- 1 # values below 70% of the maximum original are set as 1 original.groups[original.groups&gt;(maxh*.7)] &lt;- 200 # value above at 200 (high mountains) original.groups[is.na(original.groups)] &lt;- -1 # NA values (below waters) are set to -1 original.poly &lt;- as.polygons(original.groups) # convert original.groups as polygons that will be of three type: -1 =&gt; under water, 1, viable, and 200, high mountaines original.viable &lt;- makeValid(original.poly[2,]) # select polygon with &#39;1&#39; and make it a valid polygon as it may have some loop and problems # get only the non submerged actual land and plot it above_level &lt;- original.ras &gt; mean(original.ras[]) coastline &lt;- st_as_sf(as.polygons(above_level))[2,] plot(coastline, col=NA, bgc=adjustcolor(&quot;cyan&quot;, 0.1), add=T) Adding resources, using the original coordinates, which were manually selected to correspond to interesting parts of the map ##ressource geolocalisation set manually goodresources &lt;- vect( cbind( x=c(-0.2300711, -3.1455282, -0.5086485, -1.9639755, -0.4077843, 0.019688, -3.116710), y=c(3.6423000, -0.2551019, -0.7440748, 1.1303214, 1.0248567, 0.2194895, 2.0267718) ) ) #spread of resources areas &lt;- 4 * c(100, 2000, 200, 300, 100, 200, 400) #speed of ressource decay: ks &lt;- c(0.0002, 0.0001000, 0.0001600, 0.0001800, 0.00040, .0002, 0.0002)/4 crs(goodresources) &lt;- crs(original.ras) original.allres &lt;- lapply( seq_along(goodresources), function(i){ logisticdecay(goodresources[i], mask(original.ras, original.viable), x=areas[i], k=ks[i] ) } ) allna &lt;- sapply(original.allres, function(i)any(is.na(values(i)))) original.allres &lt;- original.allres[!allna] original.ress &lt;- original.allres[[1]] for(i in 2:length(original.allres)) original.ress &lt;- original.ress + original.allres[[i]] original.ress &lt;- mask(original.ress,original.viable) plot(original.ress) This, should also exactly correspond to what is stored in data_original/resources.tiff plot(original.ress) plot(rast(&quot;data_original/resources.tiff&quot;)) And this is Rabbithole home of the Poppychewer and Rabbitskinner! We can take back the sites’ positions, defined for the original Archaeoriddle and see all sites at the beginning of the simulation: original.sites=vect(&quot;data_original/sitesinitialposition/&quot;) plotMap(original.ras,original.wat,paste0(&quot;year &quot;, 0)) points(crds(original.sites), pch=21, bg=rainbow(2, alpha=0.6)[as.factor(original.sites$culture)],col=1) text(original.sites,pos=3) And below are the original set of parameters. Kbase=c(&quot;HG&quot;=35,&quot;F&quot;=120) #difference in K for the two cultures # spatial penality to extent: lower, bigger penality cul_ext &lt;- c(&quot;HG&quot;=7, &quot;F&quot;=6) # penality of occupational area: low, other sites can come close penal_cul &lt;- c(&quot;HG&quot;=4, &quot;F&quot;=5) # proba to give birth every year prob_birth &lt;- c(&quot;HG&quot;=0.3, &quot;F&quot;=0.5) # proba to die when pop &gt; K prob_survive &lt;- c(&quot;HG&quot;=0.8, &quot;F&quot;=0.65) # proba to create new settlement when Ne &gt; K prob_split &lt;- c(&quot;HG&quot;=0.2, &quot;F&quot;=0.6) # how big the group of migrant should be to create a new city vs # migrate to a existing one minimals &lt;- c(&quot;HG&quot;=0.14, &quot;F&quot;=0.20) # prob to migrate to existing settlement when Ne &gt; K prob_move &lt;- c(&quot;HG&quot;=0.2,&quot;F&quot;=0.1) We can play with that, and run one simple simulation on Rabbithole: ts &lt;- 100 print(paste0(&quot;Starting simulation &quot;,&quot;nan&quot;)) ## [1] &quot;Starting simulation nan&quot; onesimu &lt;- run_simulation( sites=original.sites, viable=original.viable, dem=original.ras, ressources=original.ress, water=original.wat, foldervid=&quot;nan&quot;, visu=F, visumin=F, ts=ts, #length of simulation in year Kbase=c(&quot;HG&quot;=35, &quot;F&quot;=110), #difference in K for the two cultures cul_ext=c(&quot;HG&quot;=7, &quot;F&quot;=6), #spatial penality to extent: lower, bigger penality penal_cul=c(&quot;HG&quot;=4, &quot;F&quot;=5), #penality of occupational area: low, other sites can cam close prob_birth=c(&quot;HG&quot;=0.3, &quot;F&quot;=0.5), #proba of giving birth every year prob_survive=c(&quot;HG&quot;=0.8, &quot;F&quot;=0.65), #proba of dying when pop &gt; K prob_split=c(&quot;HG&quot;=0.5, &quot;F&quot;=0.6), #proba of creating a new settlement when Ne &gt; K minimals=c(&quot;HG&quot;=0.14,&quot;F&quot;=0.20), #how big the group of migrant should be to create a new city vs migrate to a existing one bufferatack=300, #distance max around which settlement can fight prob_move=c(&quot;HG&quot;=0.2, &quot;F&quot;=0.1) #proba of migrating to existing settlement when Ne &gt; K ) And explore the output: Nts &lt;- onesimu$Nts # population at each timestep warcasualties &lt;- onesimu$warcasualties #death by war at each time step plot(sites, cex=(1+Nts[1,]/100), pch=21, bg=rainbow(2, alpha=0.6)[as.factor(original.sites$culture)]) plot(1, 1, type=&quot;n&quot;, xlim=c(0,i), ylim=c(0,max(Nts)), xlab=&quot;time&quot;, ylab=&quot;popsize&quot;) nill &lt;- lapply(1:ncol(Nts), function(i)lines(Nts[,i], col=rainbow(2)[as.factor(original.sites$culture)[i]])) plot(apply(Nts, 1, sum)[1:i], xlab=&quot;time&quot;, ylab=&quot;popsize&quot;) i &lt;- nrow(Nts) # Get the number of rows in Nts plot(apply(Nts[1:i, original.sites$culture==&quot;F&quot;], 1, sum), col=&quot;red&quot;, type=&quot;l&quot;, lwd=2, ylim=c(0, max(apply(Nts, 1, sum)))) # Plot sum of &#39;F&#39; culture values points(apply(Nts[1:i, original.sites$culture==&quot;HG&quot;], 1, sum), col=&quot;blue&quot;, lwd=2, type=&quot;l&quot;) # Add points for &#39;HG&#39; culture values plot(warcasualties[1:(i-1)], lwd=2, col=&quot;green&quot;, type=&quot;h&quot;, yaxt=&quot;n&quot;, ylab=&quot;&quot;) # Plot war casualties axis(4) # Add an axis on the right side par(new=T) # Allow a new plot to be drawn on the existing plot growF &lt;- apply(Nts[1:(i-1), original.sites$culture==&quot;F&quot;], 1, sum) # Sum of &#39;F&#39; culture values excluding last row growHG &lt;- apply(Nts[1:(i-1), original.sites$culture==&quot;HG&quot;], 1, sum) # Sum of &#39;HG&#39; culture values excluding last row plot(growF, col=&quot;red&quot;, type=&quot;l&quot;, lwd=2, ylim=c(0, max(growF, growHG))) # Plot growth of &#39;F&#39; culture points(growHG, col=&quot;blue&quot;, lwd=2, type=&quot;l&quot;) # Add points for growth of &#39;HG&#39; culture We could then apply the record deposit and the loss function as described in Chapter 8, and generate a fake archaeological record to share with others. This would give a fake archaeological record for one simulation, ran using the same initial conditions and parameters as those used during the original challenge. However, these results won’t exactly replicate what occurred during the original challenge. Unfortunately, as we briefly mentioned earlier, we can’t recreate exactly the same run from scratch because every interaction between settlements (migration, wars) and within settlements (birth and death) is stochastic. To replicate the same outcomes, we would need to use the same seed to ensure every random draw is identical. However, we can indeed run multiple simulations, as we did in Chapter 5.4, to estimate what typically happens under these conditions. Nonetheless, even if we can’t replicate it from scratch, we still have information about what happened step by step, stored in data_original/general_results_selected_simu/. We will leave a larger-scale exploration of these specific parameters as an exercise for those who would like to explore them in more detail during the original challenge. 9.3.1 Generate and share final simulation Let’s load again the maps, matrix storing all population changes and war casualites theoriginalchallenge=readRDS(&quot;data_original/general_results_selected_simu/buffattack300_K110_PSU065_3_all.RDS&quot;) Nts=theoriginalchallenge$Nts warcasualties=theoriginalchallenge$warcasualties We can look again at the site at the beginning sites.oc=vect(&quot;data_original/sitesinitialposition/&quot;) height.ras=rast(&quot;data_original/east_narnia4x.tif&quot;) height.wat=height.ras height.wat[height.wat&gt;mean(height.wat[])]=NA year=1 plotMap(height.ras,height.wat,paste0(&quot;year &quot;,year,&quot;, when it all begins&quot;)) plot(sites.oc,cex=(as.integer(Nts[year,]&gt;0)*0.5+Nts[year,]/100),pch=21,add=T,bg=rainbow(2,alpha=.6)[as.factor(sites.oc$culture)]) text(sites.oc[Nts[year,]&gt;0],pos=3) We can look at the war and the number of deaths for each battle for the first 1000 years And compare it to the total population growth of each culture: i=1000 par(mar=c(5,5,1,5)) plot(2:i,warcasualties[1:(i-1)],lwd=2,col=&quot;dark red&quot;,type=&quot;h&quot;,yaxt=&quot;n&quot;,ylab=&quot;&quot;,xlim=c(0,i),xlab=&quot;time&quot;) axis(4) mtext(&quot;# dead&quot;,4,2.5) par(new=T) growF=apply(Nts[1:i,sites.oc$culture==&quot;F&quot;],1,sum) growHG=apply(Nts[1:i,sites.oc$culture==&quot;HG&quot;],1,sum) plot(growF,col=&quot;orange&quot;,type=&quot;l&quot;,lwd=5,ylim=c(0,max(growF,growHG)),xlim=c(0,i),ylab=&quot;pop&quot;,xlab=&quot;&quot;) points(growHG,col=&quot;blue&quot;,lwd=5,type=&quot;l&quot;) If we look at the total population size, we can see how the clashes slowed down the population growth: par(mar=c(5,5,1,5)) plot(2:i,warcasualties[1:(i-1)],lwd=2,col=&quot;dark red&quot;,type=&quot;h&quot;,yaxt=&quot;n&quot;,ylab=&quot;&quot;,xlim=c(0,i),xlab=&quot;time&quot;) axis(4) mtext(&quot;# dead&quot;,4,2.5) par(new=T) growT=apply(Nts,1,sum) plot(growT,col=&quot;black&quot;,type=&quot;l&quot;,lwd=5,ylim=c(0,max(growT)),xlim=c(0,i),ylab=&quot;pop&quot;,xlab=&quot;&quot;) We can also look at how the growth rates change: plot(diff(growT,lag=100),type=&quot;l&quot;) And replaying all of that, on the map, will give this: Using Rec_c , short_loss and long_loss we generated deposit from the record of the run selected from the original challenge. This takes long time when applied for all sites for all the time of simulations but can be done: alldeposit=lapply(1:ncol(Nts),function(s)Rec_c(sapply(Nts[,s],A_rates), InitBP = 7500,ts=ncol(Nts),r = 0.2, Max_bone_thickness = &quot;m&quot;)) allLosses=lapply(1:length(alldeposit),function(ind){ print(paste0(&quot;settlement #&quot;,ind)); Rec=alldeposit[[ind]]; Rec=apply(Rec,2,short_loss,.6) #apply short loss Rec=long_loss(Rec,.9997,7500) #apply long loss return(Rec) }) rm(alldeposit) #we suggest you to remove the deposit object, which can be large and isn&#39;t used after We then limited the amount of sample available, with the assumption that if the biggest site was to be excavated as much as possible, we won’t have more than 30 dates from it. Thus we randomly sampled after having normalised by this max amount. maxSites=max(sapply(allLosses,sum)) nsample=round(sapply(allLosses,sum)*30/maxSites) allRemainingDates=lapply(seq_along(allLosses),function(r)extractDates(allLosses[[r]],n=nsample[r])) rm(allLosses) #object are big, need to free memory dates=unique(unlist(allRemainingDates)) #some dates are NA, not sure why, but need to get rid of these dates=rev(sort(dates[!is.na(dates)])) totallDatesRemains=sapply(allRemainingDates,function(i)table(factor(i,levels=dates))) saveRDS(allRemainingDates,file=paste0(expname,&quot;_dates.RDS&quot;)) Although the seed used for the sampling hasn’t be saved, the output of this steps been saved and get be loaded back. dates.oc=readRDS(&quot;data_original/general_results_selected_simu/buffattack300_K110_PSU065_3_dates.RDS&quot;) sites.oc=vect(&quot;data_original/general_results_selected_simu/buffattack300_K110_PSU065_3_sitesRast.RDS&quot;) After our previous sub-sampling, some sites wont have any date so we remove them from the list of available dates, that becomes: foundsites.oc=sites.oc[lengths(dates.oc)&gt;0,] foundsites.oc$numdates=unlist(lengths(dates.oc[lengths(dates.oc)&gt;0])) founddates.oc=dates.oc[lengths(dates.oc)&gt;0] We then make it a bit annoying by removing BP to the dates, we add useless and meaningless different standard deviation: stdpool=c(20,30,40,50,60,80,100,120) founddates.oc=lapply(founddates.oc,sort) founddates.oc=lapply(founddates.oc,gsub,pattern=&quot; BP&quot;,replacement=&quot;&quot;) founddates.oc=lapply(founddates.oc,rev) founddates.oc=lapply(founddates.oc,function(i)paste0(i,&quot; ± &quot;,sample(stdpool,length(i),replace=T,prob=c(3,3,3,3,2,2,1,1)),&quot; BP&quot;)) foundsites.oc$dates=sapply(founddates.oc,paste0,collapse=&quot; | &quot;) plotMap(height.ras,height.wat) plot(foundsites.oc,cex=foundsites.oc$numdates/20+1,pch=21,bg=as.factor(foundsites.oc$culture),add=T) Again, this is stochastic (the selection of the fake standard variation added is radom), so the exact results can’t be reproduced exactly, although in that case this steps doesn’t change anything, the outcome of this steps is available in the folder data_original/all_squares.oc/. 9.3.2 Grid generation and sharing Split the environment in grids and extract date for each grid leftdates=dates.oc[lengths(dates.oc)&gt;0] plotMap(height.ras,height.wat) plot(foundsites.oc,cex=foundsites.oc$numdates/20+.1,pch=21,bg=1,add=T) squares.oc=st_make_grid(height.ras,.5) plot(squares.oc,add=T,col=adjustcolor(rainbow(length(squares.oc)),.35)) text(st_coordinates(st_centroid(squares.oc)),label=1:length(squares.oc),col=&quot;white&quot;) text(st_coordinates(st_centroid(squares.oc)),label=1:length(squares.oc),col=&quot;white&quot;) The original selection was : plotMap(height.ras,height.wat) plot(foundsites.oc,cex=foundsites.oc$numdates/20+1,pch=21,bg=as.factor(foundsites.oc$culture),add=T) selection.oc=c(14,30,45,65,66) plot(squares.oc[selection.oc],add=T,col=adjustcolor(&quot;blue&quot;,.3)) inter.oc=st_intersection(st_as_sf(foundsites.oc),squares.oc[selection.oc]) ## Warning: attribute variables are assumed to be spatially constant throughout ## all geometries plotMap(height.ras,height.wat) plot(st_geometry(inter.oc),add=T,bg=rainbow(2,alpha=.6)[as.factor(inter.oc$culture)],pch=21,cex=1+inter.oc$numdates/10) 9.3.3 Giving name to the publicly available data For the original challange we wanted sites to have a name for the sites publicly available. To do so we used: fantasynamegenerators.com. We manually positionned to generate some culturally relevant group depending on their geographical position, in a way slightly inspired by the reality ; although this has no impact on the results. site_dist=st_distance(inter.oc) #min(site_dist[as.numeric(site_dist)&gt;units(0)]) sitesnames=1:nrow(inter.oc) sitesnames[c(1:4, 20)]=c(&quot;Farwallow&quot; ,&quot;Bearcall&quot; ,&quot;Dustscar&quot; ,&quot;Clearreach&quot; ,&quot;Rabbithole&quot;) fr=c(&quot;Épibéliard&quot; ,&quot;Cololuçon&quot; ,&quot;Pulogne&quot; ,&quot;Haguemasse&quot; ,&quot;Auriteaux&quot; ,&quot;Bourville&quot; ,&quot;Banau&quot; ,&quot;Montnesse&quot; ,&quot;Bannet&quot; ,&quot;Alenlon&quot;, &quot;Roullac&quot; ,&quot;Genneville&quot; ,&quot;Vinlès&quot; ,&quot;Antonnet&quot; ,&quot;Courtou&quot; ,&quot;Beaulogne&quot; ,&quot;Coloville&quot; ,&quot;Sarsart&quot; ,&quot;Soilon&quot; ,&quot;Cololimar&quot;) sitesnames[5:19]=fr[1:(19-4)] spain=c(&quot;Zava&quot; ,&quot;Catadrid&quot; ,&quot;Tegon&quot; ,&quot;Alicia&quot; ,&quot;Mulid&quot; ,&quot;Zararbella&quot; ,&quot;Malid&quot; ,&quot;Cásca&quot; ,&quot;Granalejos&quot; ,&quot;Segorez&quot; ,&quot;Terteixo&quot; ,&quot;Astumanca&quot; ,&quot;Galle&quot; ,&quot;Talona&quot; ,&quot;Girovega&quot; ,&quot;Albanada&quot; ,&quot;Nadoba&quot; ,&quot;Senca&quot; ,&quot;Vallanca&quot; ,&quot;Taville&quot;) sitesnames[21:length(sitesnames)]=spain[1:(length(sitesnames)-20)] inter.oc=cbind(inter.oc,sitesnames) plotMap(height.ras,height.wat) plot(st_geometry(inter.oc),add=T,bg=rainbow(2,alpha=.6)[as.factor(inter.oc$culture)],pch=21,cex=1+inter.oc$numdates/10) text(st_coordinates(inter.oc),inter.oc$sitesnames,cex=.8,pos=3) We can then export csvs and data for each public square, and the non-public one. We here simply follow what was presented in Chapter @ref(final-output#provide-survey-square-and-publicly-available-dates): inter.oc$sitesnames=sitesnames for(g in selection.oc){ curr=inter.oc[inter.oc$ID==g,] coords=st_coordinates(curr) write.csv(file=paste0(&quot;square_&quot;,g,&quot;.csv&quot;),cbind.data.frame(sitename=curr$sitesnames,lon=coords[,1],lat=coords[,2],dates=curr$dates,economy=curr$culture)) } remainingsites.oc=st_intersection(st_as_sf(foundsites.oc),squares.oc[-selection.oc]) for(g in (1:length(squares.oc))[-selection.oc]){ curr=remainingsites.oc[remainingsites.oc$ID==g,] coords=st_coordinates(curr) write.csv(file=paste0(&quot;square_&quot;,g,&quot;.csv&quot;),cbind.data.frame(lon=coords[,1],lat=coords[,2],dates=curr$dates,economy=curr$culture)) } 9.4 The proposals 9.4.1 P1 P1 (https://github.com/dpriss/Archaeoriddle_Kahlenberg_Priss) was based on agent-based modelling (ABM) combined with exploratory data analysis. It first studied the land of Rabbithole and calibrated the dates provided, which were then used to compute trajectories of dispersal and study site preference using ArcGIS pro and R. The suggested dispersal rates over land and water were close to the Archaeoriddle solution. The proposal then used the results of its analyses to fit the ABM, which was built using NetLogo. For the ABM, moving groups of hunter–gatherers and farmers, as well as the different settlements, were treated as agents, with starting values obtained from the exploratory data analysis and relevant literature. Additionally, it introduced behavioural rules, including movement and site preference, reaction to population threshold or reaction to interaction. After running the model several times with different parameters, it correctly predicted a hostile relation between the two groups and inferred an initial location of the Rabbit-skinners in the northeastern quadrant and the Poppy-chewers in the southwestern quadrant, while also detecting the latters’ northward movement. The expansion rate was not captured by the initial assumption of logistic growth in the areas already densely inhabited by Poppy-chewers, which makes sense considering the different population trajectories of the two groups. 9.4.2 P2 P2 (https://doi.org/10.5281/zenodo.12803445) used point-process modelling focused initially on the sampling process to build first-order models combining fitness and the available cells to predict potential occupation. It then focused on the question of whether there was conflict between the two groups. After building different archaeological phases, it computed the clustering patterns of the groups under the assumption that higher clustering could lead to higher conflict (Field, 2004). Following this, it computed the interaction distance between groups of settlements of hunter–gatherers and farmers through a multitype Strauss model. According to the results, hostilities increased over time. In the original model, the rules for hostility were not time dependent, but we have seen that, even if the rules for conflict did not change, as the population grew and the number of settlements increased, there was more probability of contact and thus more probability of conflict, which finally resulted in an increased mortality. 9.4.3 P3 P3 (https://doi.org/10.5281/zenodo.8260754) used species-distribution modelling in R to develop a four-stage research design. The first stage focused on determining which additional cells would result in a representative sample of the range variables (elevation and resource quality) present within the data. The second stage focused on data exploration to identify internal temporal, spatial and farmer and forager patterns. In the third stage, it generated summed probability distributions from the calibrated radiocarbon dates to generate relative estimates of population size through time for foragers and farmers. The fourth stage combined these data into a spatiotemporal species-distribution model, where both time and space were explicit predictors used to estimate the distribution of farmers and foragers in 100-year intervals. The spatiotemporal species distribution was successful in reproducing the directionality of the farming dispersal (from south to north) as well as the decline in hunter–gatherer populations. 9.4.4 P4 P4 (https://github.com/AlexesMes/Archeaoriddle_RabbitWorld) first developed a strategy to aid in the selection of additional data using a friction calculation that considered the distance from a putative origin region, the elevation of the region and its environmental suitability. Using R, analysis focused on capturing local complexity in the dispersal of Poppy-chewers in the study area. To track this, it used a hierarchical Bayesian phase model that was informed by all the selected settlements, both the ones held in common with other proposals and the ones obtained after calculations for additional sampling. This method allowed uncertainty to be introduced. It divided Rabbithole into 25 subareas and calculated the times of arrival of the Poppy-chewers for each area, including their high-probability density intervals, resulting in a successful approach to expansion rates. 9.4.5 P5 P5 used a qualitative approach (no code involved) that considered two cultures, A and B, initially established in regions R_A and R_B separated by a sea. Culture A eventually gained the ability to cross or bypass the barrier and to enter the region R_B and to make contact with B. Under the condition that all other parameters were equal (e.g. environmental fitness in both regions and no new pathogens or predators unknown to A), a significantly shorter settlement persistence of culture A in R_B, when compared with A’s settlement persistence assessed in other regions implies a hostile relation between A and B. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
