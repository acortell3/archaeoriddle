---
output:
  pdf_document: default
  html_document: default
  always_allow_html: true
---

```{r, echo = FALSE}
# We need to load again the functions and get the simulation results from chapter 1
# to be able to use them again in this chapter

# Function 1.
Gpd <- function(x, K, W_fert_age = c(10, 45), M_fert_age = c(15, 55),
                p_offspring = 0.3, prob = 0.8, ...){
  
  ### Process of having offspring
  
  # Female fertile population
  W <- x[x[,2]=="F", ]
  W_fert <- W[W$Age > W_fert_age[1] & W$Age < W_fert_age[2], ]
  # Male fertile population
  M <- x[x[,2]=="M", ]
  M_fert <- M[M$Age > M_fert_age[1] & M$Age < M_fert_age[2], ]
  
  ## Probability of having descendance per woman
  # Penalisation in case there are too few men
  pen <- round(nrow(M_fert)*2 / nrow(W_fert), 2) ## Assumes one man can have two women
  pen[pen>1] <- 1 ## The men penalisation can never multiply the birth rate per woman
  
  ## Probability of a woman having a son per year
  p_offspring <- p_offspring*pen
  
  # Aging process. They get one year older
  x$Age <- x$Age+1
  
  ### Births are new population that's added
  n_offspring <- sum(rbinom(nrow(W_fert), 1, p_offspring))
  new_pop <- data.frame(
    "Age" = rep(0, n_offspring),
    "Sex" = sample(c("M","F"), n_offspring, prob=c(0.5, 0.5), replace=TRUE))
  x <- rbind(x, new_pop)
  
  ### Process of dying
  vec_d <- apply(x, 1, death, ...)
  x <- x[vec_d==0, ]
  
  ## Apply carrying capacty restrictions
  x <- K_lim(x, K = K, prob = prob)
  
  return(x)
}


# Function 2.
death <- function(x, pd=data.frame("Age" = c(0:99),
                                   "P_d" = c(rep(0.14,1),
                                             rep(0.16,4),
                                             rep(0.05,5),
                                             rep(0.01,24),
                                             rep(0.03,14),
                                             rep(0.1,10),
                                             rep(0.3,42)))){
  age <- as.numeric(x[1])
  return(rbinom(1, 1, prob=pd[pd$Age==age, 2]))
}


# Function 3.
K_lim <- function(x, K, prob=0.8){
  p <- nrow(x)
  if (p>K){
    o <- rbinom(p-K, 1, prob)
    o <- sum(o[o==1])
    o[o==0] <- 1 ## Avoids problem eliminating all the df if remove == 0
    x <- x[-sample(1:nrow(x), o, replace=FALSE), ]
  }
  return(x)
}


# Function 4.
A_rates <- function(x,
                    kcalpers = 2,
                    kcalmeat_eat = 0.45,
                    kcalmeat_prod = 1.5,
                    in_camp_eat = 0.55,
                    in_camp_stay = 13,
                    kg = 0.07){
  
  
  if (is.data.frame(x) == TRUE){
    P <- nrow(x)
  } else if (length(x) == 1){
    P <- x
  } else {
    P <- length(x)
  }
  
  # Check variable values are within the defined ranges
  if (1.5 <= kcalpers & kcalpers <= 2.5){
    B <- kcalpers*365
  } else {
    stop('kcalpers must be within [1.5, 2.5]')
  }
  
  if (0 <= kcalmeat_eat & kcalmeat_eat <= 1) {
    M <- kcalmeat_eat
  } else {
    stop('kcalmeat_eat must be within [0, 1]')
  }
  
  if (1 <= kcalmeat_prod & kcalmeat_prod < 2.5) {
    R <- kcalmeat_prod
  } else {
    stop('kcalmeat_prod must be within [1, 25]')
  }
  
  if (0 <= in_camp_eat & in_camp_eat <= 1){
    S <- in_camp_eat
  } else {
    stop('in_camp_eat must be within [0, 1]')
  }
  
  if (in_camp_stay <= 52){
    O <- round(in_camp_stay/52,2)
  } else {
    stop('A year cannot have more than 52 weeks')
  }

  C <- B*M
  G <- (C * S) / R ## Quantity (in kg) of animal consumed per person in camp during year t
  A <- P * O * kg * G ## kilograms of meat consumed within a camp by the group
  W <- round((1000 * A) / 4) ## samples extracted from that meat
  
  return(W)
}


# Function 5.
D_along <- function(W_t, r, max_bone_thickness = c("m", 's', 'l', 'vl'),
                    prop_buried = .9999){
  
  # Define parameter r
  if(r > 0.5) stop("values > 0.5 are not accepted for param 'r'")
  r <- round(r, 1)
  # Constraints for parameter Pb
  if (prop_buried >= 1) stop("Pb must be lower than 1")
  
  # Define parameter Max_bone_thickness (L)
  max_bone_thickness = match.arg(max_bone_thickness)
  if (max_bone_thickness == 's'){
    L <- 2.5
  } else if (max_bone_thickness == 'm'){
    L <- 5
  } else if (max_bone_thickness == 'l'){
    L <- 10
  } else if (max_bone_thickness == 'vl'){
    L <- 20
  }
  
  # Define tmax
  tm <- L/r
  # Estimate lambda
  l <- -log(1 - prop_buried) / tm
  
  ss <- rep(0, round(tm)) ## Vector to distribute samples over
  tl <- 0 # Year where the sample is deposited
  tu <- 1 # Year when it is covered
  
  for (i in 1:tm){
    Wb <- W_t * (1 - exp(-l*(tu-tl))) # Apply formula for tu
    Wbprev <- W_t * (1 - exp(-l*((tu-1)-tl))) # Calculate for previous to tu
    ss[i] <- round(Wb - Wbprev) # Number of samples for each year
    tu <- tu + 1
  }
  
  return(ss)
}


# Function 6.
Pop_stoch <- function(pop_size, K, prob = 0.8, ts, ...){
  
  ## Create initial population
  pop_matrix <- data.frame(
    "Age" = sample(10:30, pop_size, 10:30, replace = TRUE),
    "Sex" = sample(c("M","F"), pop_size, prob = c(0.5,0.5), replace = TRUE)
  )
  
  # Initialize vector with population size for each year  
  pop <- vector(length=ts)
  pop[0] <- nrow(pop_matrix)
  ## Run stochastic process  
  for (i in 1:ts){
    pop_matrix <- Gpd(pop_matrix, K=K, prob=prob, ...)
    pop[i] <- nrow(pop_matrix)
  }
  
  return(pop)
}


## Function 7.
Rec_c <- function(x, persqm = FALSE, area, ts, InitBP, ...){
  
  ## Whether sqm division must be included or not
  if (persqm == TRUE){
    x <- x / area
  }
  
  ## Spread dates along different depths
  matdim <- length(x)
  mat <- matrix(nrow=matdim, ncol=matdim)
  
  for (i in 1:matdim){
    new <- D_along(x[i], ...)
    st <- i - 1
    pos <- c(rep(0, st), new)
    pos <- pos[1:matdim]
    mat[, i] <- pos
  }
  mat[is.na(mat)] <- 0
  
  ## Names for columns (each year)
  years <- seq(InitBP, InitBP-ts)
  nyears <- c()
  for (i in 1:matdim){
    nyears[i] <- paste0(years[i], " BP")
  }
  colnames(mat) <- nyears
  
  ## Names for rows (each depth)
  # Extract arguments as a list
  extract_args <- function(x, ...){ 
    extras <- list(...)
    return(list(extras=extras)) 
  }

  dr <- extract_args(D_along, ...)
  dr <- dr$extras$r
  
  d <- rev(cumsum(rep(dr, nrow(mat)))) ## computes depths
  rownames(mat) <- paste0("d = ", d, " cm")

  return(mat)
}


set.seed(1234)
# 1. Pop-protocol
Ps <- Pop_stoch(pop_size=100, ts=100, K=150)
# 2. A-protocol
samp <- unlist(lapply(Ps, A_rates)) ## Extract sample per time steps
# 3. D-protocol
Rec <- Rec_c(samp, InitBP = 7500, ts = 100, r = 0.2, max_bone_thickness = "m")
tRec <- t(Rec)

```


# Record loss

## Theoretical model

### General purpose

Our main interest is showing and testing the effect of post-depositional processes on the archaeological record. The most obvious consequence of these effects relates to the direct loss of the record, while other implications can result in vertical and horizontal movements. Such movements, in turn, distort our comprehension of an archaeological site. Although the horizontal disposition of archaeological waste is important to understand different aspects, such as the activities deployed in a site, the function of that site, or it can even bring information on demographic processes, it will not be taken into account here. The reason for this is that modelling horizontal post-depositional movements (1) requires specific types of information which are frequently not available for field researchers, (2) adds complexity layers, the payoff of which, given its cost in terms of computation and comprehension of the model, may be insufficient and (3) as the focus of this model is on the reliability of 14C dates, vertical movements, which can obscure the construction of consistent stratigraphies, have been given preference. 

However, we understand that horizontal movement is a key aspect of the record formation. For example, it is frequent that hydric, gullying or sloping processes concentrate the archaeological waste in specific parts of the site. This would bias our perception of record concentration both horizontally, but also vertically. This *record loss* model assumes and normalises this when $W(t)$ is retrieved from the previous *record formation* model. Nevertheless, this needs to be taken into account both if the user wants to introduce the values for $W(t)$, and for the analysis with real archaeological data. There is not a single solution to control for this, as it largely depends on the excavated area and whether this is representative for the complete site or not. For example, many surveys spread across a site could actually be representative of that site's record, whereas a single small survey in that same site is likely prone to present some specific bias due to post-depostional conditions. In this case, and in order to propose a correct variable, the user needs to make the proper calculations according to the site under study, but always bearing in mind that the model will work over the variable $W(t)$ (*waste produced*), and can be computed to be expressed by $m^2$.

### Model layers
Although in the future we will introduce additional layers which account for how the record moves vertically along the sequence, at this point, we are only considering the processes of record loss.

#### Loss of the record
The are two main components for this part of the model. As shown in the graph below, the first one is *time-dependent taphonomic loss*, usually referring to long term process affecting the record for every $t$ from the moment of its depositions to the present. The second one is *direct action*, which consider high-impact short term processes, such as human or animal action. These two variables can be applied together (advised), separately or not at all.

```{r, echo = FALSE,cache=FALSE}
library(DiagrammeR)

grViz("
digraph record_formation {

graph [layout = dot, ## Set title and layout
       label = 'Record loss process',
       labelloc = 't'
       fontname = Helvetica,
       fontsize = 30]

# node definitions with substituted label text
node [fontname = Helvetica, ## General node definition
      style = filled,
      color = MistyRose]
      
## Per node definition
a [label = '@@1',
   fillcolor = orange]
b [label = '@@2',
   fillcolor = orange]
c [label = '@@3',
   fillcolor = cadetblue,
   fixedsize = shape,
   width = 8.5,
   height = 1.5,
   fontsize = 30]

## General edge definition
edge [color = grey,
       arrowhead = vee,
       arrowtail = vee]

## Specific edges
a -> {c}
b -> {c}
}

## Label definition
[1]: 'Low impact - long term action'
[2]: 'High impact - short term action'
[3]: 'Remaining archaeological waste'

")
```

These variables are modelled as follows:

* **Long-term loss** ($V_l(t)$: Attending to the proxy we are considering (bones), we understand this variable as the processes which on the short term have little to none impact on bone preservation, but which can have severe consequences in the long term. These can refer, for example to the pH of the soil of the site, temperature and humidity changes, etc, and must be considered by the researcher or the user. We can subsume this information as a single probability $\theta_l$ within a binomial distribution. Then, the number of archaeological samples currently surviving from all the $W(t)$ generated at a time $t$ follows the distribution

$$V_l(t) \sim Bi(W(t), \theta_l) \tag{1}$$
Although mathematically this computation is straightforward, it can be quite expensive computationally. Therefore, because we can assume independence of the effect altering the sample for each $t$, and for computational efficiency, we compute $\theta_l$ as

$$\theta_l = \theta^{t+1950} \tag{2}$$

Where $\theta_l$ is the considered probability, $t$ is the time period before present where the sample was deposited and 1950 is the year that is conventionally treated as *the present* in radiocarbon dating.

* **Short-term loss** ($V_s(t+1)$): Most of the short-term, high-impact destruction of the archaeological record happens during the first year after deposition. Therefore, for this variable, we have considered the effect acting only once on the archaeological record, with an action, in any case, more agressive than the previous variable. Again, we can consider this a binomial distribution, where

$$V_s(t+1)\sim Bi(W(t),\theta_s)$$

where $\theta_s$ is the probability that the record survives. Again, this probability must be estimated according to previous information such as, for exemple, whether there has been human reoccupation or animal action at the site at $t +1$. In this case, one good way to measure this is considering that for $\theta_s = 0.5$, half the record would be lost after this first year.

All in all, if we put together these variables, then the remaining record for a site could be defined as

$$V(t)=V_l(t)+V_s(t+1)$$
There have been other quantitative approaches to the survival of the archaeological record, such as Surovell's (2007, 2009) approaches. These are rather focused on the loss of complete sites, and not that much on the loss of specific records. And, as the authors acknowledge, they refer to specific geographic locations. However, with specific parameter combinations of our loss probabilities, we can obtain correlations higher than 0.98 with Surovell's loss curves. In the case shown, for example, we have considered $\theta_s=0.9$ and $\theta_l=0.9997$

```{r, echo = FALSE, message=FALSE, results=FALSE}


### COMPARISON OF SUROVELL AGAINST BINOMIAL

Ns <- 10000 ## Number of sites (proxies)
t <- 40000 ## Time span

## Surovell proposal
# Constants
a <- 5.726442e06  
b <- 2176.4
c <- -1.3925309
# Surovell's sites
Ss <- rep(Ns, t)
for (i in 2:t){
  tm <- a*((c(0,i)+b)^c)
  tm <- round((tm[2]/tm[1])*Ns)
  Ss[i] <- tm
}


## 1 - Binomial proposal
th_l = 0.9997
Bs <- rep(Ns, t)
for (i in 2:t){
  Ns <- rbinom(1, Ns, th_l)
  Bs[i] <- Ns 
}
cor.test(Ss, Bs)
summary(lm(Ss ~ Bs))


## 2- binomials proposal
#  1 for short term, followed by long term
# th_l <- 0.9995
# th_l <- 0.99964
th_s <- 0.9

# Initial short term loss
Ls <- rbinom(1, Ns, th_s)
# Initialize time-series:
# t0 <- Initial # of sites
# t1 <- After short term-loss 
# We add a placeholder for the rest, that will be calculated
# using the long-term loss
Th_ss <- c(Ns, Ls, rep(1, t-2))

for (i in 3:t){
  Ls <- rbinom(1, Ls, th_l)
  Th_ss[i] <- Ls
}
cor.test(Th_ss, Bs)
summary(lm(Th_ss ~ Bs))

plot(Ss, type = "l", lwd = 2, col = "darkred", main = "Different proposals of record loss",
    ylab = "Nsamples", xlab = "t")
lines(Bs, lwd = 1.5, col = "darkgreen")
lines(Th_ss, lwd = 2, col = "darkblue")
legend("topright", legend = c("Surovell", "1 Binomial", "2 Binomial"),
       lty = c(1,1,1), col = c("darkred", "darkgreen", "darkblue"))

cor.test(Ss,Th_ss)

```

## Implementation
This layer of the model is less complex than the previous one. In this case, we only need to apply the loss of the record on the already created sample, according to the different loss probabilities. The functions for this are as follows

```{r}

## Function 8. Short term taphonomic loss
#' @title Short-term taphonomic loss
#' Returns a data frame in the same form that the inserted data frame with 
#' reduced values for short-term taphonomic loss process
#' @param x: A vector with the amount of samples per depth, at a specific years.
#' If used for many years, use with apply function, where the rows of the data
#' frame are the depths and the columns the years.
#' @param theta_s: Probability of the record surviving after the first year after deposition
#' @export
short_loss <- function(x, theta_s){
  res <- c()
  for (i in 1:length(x)){
    res[i] <- rbinom(1, x[i], theta_s)
  }
  return(res)
}

## Function 9. Long term taphonomic loss
#' @title Long-term taphonomic loss
#' Returns a data frame in the same form that the inserted data frame with 
#' reduced values for short-term taphonomic loss process
#' @param x: A vector with the amount of sample per depth. If used for many years, use
#' with apply function, where the rows of the data frame are the depths and the columns 
#' the years.
#' @param theta_l: Probability of the record surviving after the first year after deposition
#' @param it: Initial time. Initial year of occupation in BP.
#' @export
long_loss <- function(x, theta_l, it){
  t <- it+1950
  for (i in 1:ncol(x)){
    prob <- theta_l^(t-i)
    s <- x[, i]
      for (k in 1:length(s)){
        s[k] <- rbinom(1, s[k], prob)
               
      }
    x[, i] <- s  
  }
  return(x)
}
```


Once this is done, it is fairly easy to recompute the remaining record, and show it in the same terms as before. In this case, we have computed the record loss with $\theta_s=0.9$ and $\theta_l=0.9997$. Again, the user can heck how the model work and its results [here](https://acortell3.shinyapps.io/archaeo_record_model). 

```{r}
tlRec <- apply(Rec, 2, short_loss, th_s)
lRec <- long_loss(tlRec, th_l, 7500)
tlRec <- t(lRec)
```


```{r, echo = FALSE}
library(viridis)
barplot(tlRec[c(1:4,77:79),], col = viridis(10), legend = colnames(Rec)[c(1:4,77:79)],
        xlab = "Depth", ylab = "nsamples", main = "Distribution of specific years along depth with population")
lines(Ps*300, col = "darkred", lwd = 1.5)

colnames(tlRec) <- colnames(tRec)
rownames(tlRec) <- rownames(tRec)

barplot(tRec, col = viridis(ncol(tRec)), #legend = colnames(Rec),
        xlab = "Depth", ylab = "nsamples", main = "Distribution of samples for specific years")

barplot(tlRec, col = viridis(ncol(tlRec)), #legend = colnames(Rec),
        xlab = "Depth", ylab = "nsamples", main = "Distribution of samples for specific years after loss")

```

