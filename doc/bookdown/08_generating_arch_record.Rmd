# Generate final archaeological record {#final-output}

Now that all the pieces are in place we need to combine them to generate files that can be shared with people in order for them to guess the original parameters.

To summarize what we have so far:

1. We simulated an artificial environment in Chapter&nbsp;\@ref(env).
2. We localised sites on the map and defined a population growth process in Chapter&nbsp;\@ref(popgrowth).
3. We defined rules to describe how population move and engage in conflict as they expand in Chapter&nbsp;\@ref(conflict).
4. We simulated these processes of growth, conflict and migration on the environment we designed, for extended periods of time in Chapter&nbsp;\@ref(main-simu).
5. We defined processes that use this simulated growth to generate an archaeological record in Chapter&nbsp;\@ref(recgen),
6. We defined processes that transform this archaeological record using credible loss mechanisms&nbsp;\@ref(recgen).

Here we will use all these previous steps to generate csvs that can be shared. In this chapter we will use the simulation run in the Chapter&nbsp;\@ref(main-simu) ; if you want to generate a new artificial dataset, different from the one that has been presented in the original challenge and that will be described in the chapter&nbsp;\@ref(original-challenge), you will want to run a longer simulation, and choose among different runs the ones that look interesting/realistic. We provide a script `./scriptmini.R` to give some idea about how to automatize these runs and how to choose among them.



## Using sites size to generate deposit

This has been done using file `dateGeneration.R`, and we will describe it here now.

First, we get the rasters defining the environment (If you followed the book since the beginning, they should be already in memory but in case you didn't we reload them here):

```{r getRaster-ex,eval=T,cache=F}
height.ras <- rast(file.path("data_toshare","dem_raster.tiff"))
height.wat <- height.ras
height.wat[height.wat>mean(height.wat[])] <- NA
Nts <- readRDS("data_tmp/popSizeMatrix.RDS")
plotMap(height.ras,height.wat)
```

Using `Rec_c` , `short_loss` and `long_loss`, that we presented in the Chapter&nbsp;\@ref(recgen) &&nbsp;\@ref(recloss), we generate the deposit. This can take a long time if the simulation includes a lot of sites spanning over a long period of time. Here, we first generate all the deposit from the matrix `Nts` generated during the simulation of the Chapter&nbsp;\@ref(main-simu) and then apply all loss functions together:


```{r doloss-ex,eval=T}
#All in one:
alldeposit=lapply(1:ncol(Nts),function(s)Rec_c(sapply(Nts[,s],A_rates), InitBP = 7500,ts=nrow(Nts),r = 0.2, max_bone_thickness = "m"))
allLosses=lapply(1:length(alldeposit),function(ind){
    Rec=alldeposit[[ind]];                    
    Rec=apply(Rec,2,short_loss,.6)  #apply short loss
    Rec=long_loss(Rec,.9997,7500) #apply long loss
    return(Rec)
})
rm(alldeposit)
```

Once this is done, we will have a list of all the potential dates available from all sites during our simulation. Realistically, for any given site, not _all_  the material will be recovered, and even less will be dated. To reflect that, we capped the maximum number of samples available per sites. To do so -- and this was applied as is in the original challenge presented in the Chapter&nbsp;\@ref(original-challenge), we assume that the biggest sites -- the ones with most media attention, most funding, will very likely get at most 30 dates. We thus subsampled the number of dates for smaller sites given this ratio of $\frac{30}{n_{\text{largest}}}$. Here the simulation has been greatly shortened, thus sites have way less samples. We thus cap the large sites to have only 80% of the dates they generate and sample from the other sites using this ratio.

```{r removesmall-ex, eval=T}
maxSites=max(sapply(allLosses,sum))
nsample=round(sapply(allLosses,sum)*(30/maxSites))
allRemainingDates=lapply(seq_along(allLosses),function(r)extractDates(allLosses[[r]],n=nsample[r]))
rm(allLosses) #object are big, need to free memory
```


We then have date samples for every site in our worlds; where the maximum number of dates is 30 for the biggest sites that existed in our simulation. 

Let's look at where these date are available, we first retrieve all sites saved in the simulation from&nbsp;\@ref(main-simu):

```{r getfakeresults-ex,cache=F}

allsites=vect(file.path(foldtmp,"allsites.shp"))
if(class(allsites)[1] == "PackedSpatVector")
    allsites=terra::vect(allsites)
```

We can see where are the sites with more dates:

```{r getadates-ex,eval=T,out.width="45%",fig.show="hold",fig.cap="left: sites where dates are potentially available, size of the dot represent the number of dates, right: total number of dates by years", }
expname="exn"
plotMap(height.ras,height.wat,paste0("final after losses"))
plot(allsites,cex=3*(lengths(allRemainingDates)-1)/(29),pch=21,add=T,bg=rainbow(2,alpha=.6)[as.factor(allsites$culture[1:10])])

dates=unique(unlist(allRemainingDates))
dates=rev(sort(dates[!is.na(dates)]))
plot(table(unlist(allRemainingDates)))
totallDatesRemains=sapply(allRemainingDates,function(i)table(factor(i,levels=dates)))
saveRDS(allRemainingDates,file=file.path(foldtmp,paste0(expname,"_dates.RDS")))
```



Loading dates:

```{r getfakedates-ex,cache=F}
dates=readRDS(file=file.path(foldtmp,paste0(expname,"_dates.RDS")))
```

Keep sites with foundable dates

```{r extractdates-ex,cache=F}
foundsites=allsites[lengths(dates)>0,]
foundsites$numdates=unlist(lengths(dates[lengths(dates)>0]))
founddates=dates[lengths(dates)>0]
```

Remove BP to the dates, add some random variation

```{r addstdev-ex,cache=F}
stdpool=c(20,30,40,50,60,80,100,120)
founddates=lapply(founddates,sort)
founddates=lapply(founddates,gsub,pattern=" BP",replacement="")
founddates=lapply(founddates,rev)
founddates=lapply(founddates,function(i)paste0(i," ± ",sample(stdpool,length(i),replace=T,prob=c(3,3,3,3,2,2,1,1))," BP"))
foundsites$dates=sapply(founddates,paste0,collapse=" | ")
plotMap(height.ras,height.wat)
plot(foundsites,cex=foundsites$numdates/20+1,pch=21,bg=as.factor(foundsites$culture),add=T)
```

## Provide survey square and publicly available dates {#public-data}

To then share these dates, we choose to split the environment into grids and extract dates by grid. For each grid, a csv file is generated with data about the sites that have been found, i.e. their geolocalisation and the dates found at the site.

```{r plotallfoundiste-ex,cache=F,fig.cap="The black dots represent the sites for which dates are available, the size of the circle reflects the amount of dates available. Each square represents an area that can be selected by the users."}

leftdates <- dates[lengths(dates)>0]
plotMap(height.ras,height.wat)
plot(foundsites,cex=foundsites$numdates/20+.1,pch=21,bg=1,add=T)
squares <- st_make_grid(height.ras,.5)
squares <- st_bind_cols(squares,ID=1:length(squares))
plot(squares,add=T,col=adjustcolor(rainbow(nrow(squares)),.35))
text(st_coordinates(st_centroid(squares)),label=squares$ID,col="white")
text(st_coordinates(st_centroid(squares)),label=squares$ID,col="white")
```

In the original archaoriddle challenge we proposed to give the data from 4 grids that we selected ourselves to be sure the participants have a least _some_  interesting information. Here we choose randomly four squares with the only limits that they provide at least five settlements. You will want to change and manually the vector `selection` in the chunk below to cover areas with a lot of sites and sites from different cultures, from what can be seen in the Figure&nbsp;\@ref(fig:plotallfoundiste-ex).

```{r getpublicsele-ex,cache=F,out.width="45%",fig.show="hold",fig.cap="Left: square publicly available ; right: sites surveyed within these publicly available squares."}
plotMap(height.ras,height.wat)
plot(foundsites,cex=foundsites$numdates/20+1,pch=21,bg=as.factor(foundsites$culture),add=T)
selection=sample(1:nrow(squares),4)
inter <- st_intersection(st_as_sf(foundsites),squares[selection,])
while(nrow(inter)<5){
    selection=sample(1:nrow(squares),4)
    inter <- st_intersection(st_as_sf(foundsites),squares[selection,])
}
plot(squares[selection,],add=T,col=adjustcolor("blue",.3))
plotMap(height.ras,height.wat)
plot(st_geometry(inter),add=T,bg=rainbow(2,alpha=.6)[as.factor(inter$culture)],pch=21,cex=1+inter$numdates/10)
```

### Giving name to the publicly available data

In the original challenge we used: [fantasynamegenerators.com](https://www.fantasynamegenerators.com/fantasy-town-names.php) to choose nice, randomly generated names, to name the sites publicly available. You will see in the section&nbsp;\@ref(naming) that we positioned them with culturally relevant groups, although this has no impact on the results. Here, we took all the names we had for the original challenge to randomly assign some for your custom world.

```{r namesites-ex,cache=F}

site_dist=st_distance(inter)
potname=c("Farwallow" ,"Bearcall" ,"Dustscar" ,"Clearreach" ,"Rabbithole",
"Épibéliard" ,"Cololuçon" ,"Pulogne" ,"Haguemasse" ,"Auriteaux" ,"Bourville" ,"Banau" ,"Montnesse" ,"Bannet" ,"Alenlon", "Roullac" ,"Genneville" ,"Vinlès" ,"Antonnet" ,"Courtou" ,"Beaulogne" ,"Coloville" ,"Sarsart" ,"Soilon" ,"Cololimar","Zava" ,"Catadrid" ,"Tegon" ,"Alicia" ,"Mulid" ,"Zararbella" ,"Malid" ,"Cásca" ,"Granalejos" ,"Segorez" ,"Terteixo" ,"Astumanca" ,"Galle" ,"Talona" ,"Girovega" ,"Albanada" ,"Nadoba" ,"Senca" ,"Vallanca" ,"Taville")

u=0
while(length(potname)<nrow(inter)){
    u=u+1
    potname=c(potname,paste0(potname,u))
}
sitesnames=sample(potname,nrow(inter))

inter=cbind(inter,sitesnames)
plotMap(height.ras,height.wat)
plot(st_geometry(inter),add=T,bg=rainbow(2,alpha=.6)[as.factor(inter$culture)],pch=21,cex=1+inter$numdates/10)
text(st_coordinates(inter),inter$sitesnames,cex=.8,pos=3)
```

### Export and download dates

Export csv and data for each public square

```{r exportpublic-ex,eval=T}

for(g in selection){
    curr=inter[inter$ID==g,]
    coords=st_coordinates(curr)
    write.csv(file=file.path("data_toshare",paste0("square_",g,".csv")),cbind.data.frame(sitename=curr$sitesnames,lon=coords[,1],lat=coords[,2],dates=curr$dates,economy=curr$culture))
}

```

The csvs generated for this specific run of the bookdown can be downloaded for each square:

```{r create-links,echo=FALSE,results="asis"}
for (g in selection) cat(paste0("- [square ",g,"](data_toshare/square_",g,".csv)\r\n"))
```

Export CSV and data for every square

```{r exportprivate-ex,eval=T}

allsites=st_intersection(st_as_sf(foundsites),squares[-selection,])

for(g in (1:nrow(squares))[-selection]){
    curr=allsites[allsites$ID==g,]
    coords=st_coordinates(curr)
    write.csv(file=file.path("data_tmp",paste0("square_",g,".csv")),cbind.data.frame(lon=coords[,1],lat=coords[,2],dates=curr$dates,economy=curr$culture))
}

ld=strsplit(inter$dates," \\| ")
ld=lapply(ld,function(i)gsub(" ± .*","",i))
inter$start=sapply(ld,max)
inter$end=sapply(ld,min)

```

The CSVs generate for all squares can be [downloaded here](data_tmp/)


## Bonus maps


Some maps we use and did for fake papers and social media etc... 


```{r randommaps-ex,out.width="30%",fig.show="hold"}

## plotting oldschool map

par(mar=c(0,0,0,0),oma=c(0,0,0,0))

plot(height.wat,col=adjustcolor("light blue",.4),reset=F,legend=F,axes=F)
contour(height.ras,levels=seq(5,300,15),axes=F,ann=F,lwd=.1,labels="",bg="light blue",add=T)
plot(st_geometry(inter),pch=4,add=T,lwd=.3)
plot(st_geometry(inter),pch=20,add=T,lwd=.2,cex=.8,col=as.factor(inter$culture),alpha=.8)

old=inter[inter$start>7180,]
ne=inter[inter$start>6800 & inter$end<6900,]

plot(height.wat,col=adjustcolor("light blue",.4),reset=F,legend=F,axes=F)
contour(height.ras,levels=seq(5,300,15),axes=F,ann=F,lwd=.1,labels="",bg="light blue",add=T)
plot(st_geometry(old),pch=4,add=T,lwd=.5,cex=3)
plot(st_geometry(old),pch=20,add=T,lwd=.2,cex=1.8,col=as.factor(old$culture),alpha=.8)


plot(height.wat,col=adjustcolor("light blue",.4),reset=F,legend=F,axes=F)
contour(height.ras,levels=seq(5,300,15),axes=F,ann=F,lwd=.1,labels="",bg="light blue",add=T)
plot(st_geometry(inter),pch=4,add=T,lwd=.5,cex=3)
plot(st_geometry(inter),pch=20,add=T,lwd=.2,cex=1.8,col=as.factor(inter$culture),alpha=.8)


plot(height.wat,col=adjustcolor("light blue",.4),reset=F,legend=F,axes=F)
contour(height.ras,levels=seq(5,300,15),axes=F,ann=F,lwd=.1,labels="",bg="light blue",add=T)
plot(st_geometry(inter),pch=4,add=T,lwd=.5,cex=3)
plot(st_geometry(inter),pch=20,add=T,lwd=.2,cex=1.8,col=as.factor(inter$culture),alpha=.8)

plot(height.wat,col=adjustcolor("light blue",.4),reset=F,legend=F,axes=F)
contour(height.ras,levels=seq(5,300,15),axes=F,ann=F,lwd=.1,labels="",bg="light blue",add=T)
plot(st_geometry(ne),pch=4,add=T,lwd=.5,cex=3)
plot(st_geometry(ne),pch=20,add=T,lwd=.2,cex=1.8,col=as.factor(ne$culture),alpha=.8)
```

More maps for twitter:

```{r randommaps2-ex}
plot(height.wat,col=adjustcolor("light blue",.4),reset=F,legend=F,axes=F)
contour(height.ras,levels=seq(5,300,15),axes=F,ann=F,lwd=.2,labels="",bg="light blue",add=T)
plot(st_geometry(inter),pch=4,add=T,lwd=.5,cex=3)
plot(st_geometry(inter),pch=20,add=T,lwd=.2,cex=1.8,col=as.factor(inter$culture),alpha=.8)
sel=inter[c(20,2,15,36),]
text(st_coordinates(sel),sel$sitesnames,cex=.8,pos=c(1,2,2,4))
text(x = -1, y = .7, labels = "❤️", adj = c(0, 0), cex = 3, col = "black", font = 1)
text(x = -1, y = 1.2, labels = "🔥 ", adj = c(0, 0), cex = 3, col = "black", font = 1)
text(x = -1, y = 1.2, labels = "⚔️ ", adj = c(0, 0), cex = 3, col = "black", font = 1)
text(x = -1.2, y = 1.8/2, labels = "???? ", adj=c(0,0), cex = 3, col = "black", font = 1)
```
More maps!! (the coolest)


```{r randommaps3-ex}
plotMap(height.ras,height.wat)
contour(height.ras,levels=seq(5,300,15),axes=F,ann=F,lwd=.2,labels="",bg="light blue",add=T)
plot(st_geometry(inter),pch=4,add=T,lwd=.5,cex=3)
plot(st_geometry(inter),pch=20,add=T,lwd=.2,cex=1.8,col=as.factor(inter$culture),alpha=.8)
coastline=st_cast(st_as_sf(as.polygons(height.ras>mean(height.ras[])))[2,],"MULTILINESTRING")
plot(coastline,add=T,lwd=1.1,col="black")
sel=inter[c(20,2,15,36),]
text(st_coordinates(sel),sel$sitesnames,cex=.8,pos=c(1,2,2,4))

```


```{r}

plotMap(height.ras,height.wat)
contour(height.ras,levels=seq(5,300,15),axes=F,ann=F,lwd=.2,labels="",bg="light blue",add=T)
plot(st_geometry(inter),pch=4,add=T,lwd=.5,cex=3)
plot(st_geometry(inter),pch=20,add=T,lwd=.2,cex=1.8,col=as.factor(inter$culture),alpha=.8)
coastline=st_cast(st_as_sf(as.polygons(height.ras>mean(height.ras[])))[2,],"MULTILINESTRING")
plot(coastline,add=T,lwd=1.1,col="black")
sel=inter[c(20,2,15,36),]
text(st_coordinates(sel),sel$sitesnames,cex=.8,pos=c(1,2,2,4))
#text(x = -1, y = .7, labels = "🍻", adj = c(0, 0), cex = 3, col = "black", font = 1)
text(x = -3.4, y = .7, labels = "🍻 ", adj = c(0, 0), cex = 10, col = "black", font = 1)
text(x = -1.8, y = .7, labels = "👍 ", adj = c(0, 0), cex = 8, col = "black", font = 1)
text(x = -.5, y = .7, labels = "🍔 ", adj = c(0, 0), cex = 6, col = "black", font = 1)
#text(x = -1, y = 1.2, labels = "⚔️ ", adj = c(0, 0), cex = 3, col = "black", font = 1)
#text(x = -1.2, y = 1.8/2, labels = "???? ", adj=c(0,0), cex = 3, col = "black", font = 1)


```
