# Archaeoriddle: the original challenge {#original-challenge}

In this chapter we will here go through the whole steps we presented throughout Chapter&nbsp;\@ref(env) to&nbsp;\@ref(final-output), but in a condensed way and with the parameters used to generate the data shared during the Archaeoriddle original challenge. But before that we will we retrace the journey from the first discussions in the courtyard of the McDonald Institute in Cambridge to the final workshop in Belfast.

We will then end the chapater by briefly  discuss  the proposal submitted for the challenge.

## Context 

Here a bit of how all that happen, and the majors steps.

### Original idea and first reflections


### Birth of a Challenge

After discussion within the member of the CDAL, we decided that a challenge-like event, with a cash-prize, would be the best way to convince people to participate. After the B/A grant has been awarded to Simon Carrignon, Alfredo Cortell and Enrico Crema, we thus decided to announce this challenge through various social media video and a website: https://theia.arch.cam.ac.uk/archaeoriddle/

### The British Academy Grant -- November 2022

### CAA Oxford -- June 2023

> Paper ID: 295
> Paper Title: Archaeoriddle: A collaborative game to improve archaeological inference
> Abstract:
> Archaeoriddle is a collaborative research project aimed to improve archaeological hypothesis building by comparing different methodological proposals within a crowd-sourced and interactive working framework. To achieve this, we created a virtual world through computational simulation, emulating an interaction process between a hypothetical group of incumbent hunter-gatherers and a group of spreading farmers. This virtual world and its associated virtual archaeological datasets have been developed by members of the Computational and Digital Archaeology Laboratory (CDAL) of the University of Cambridge and can be consulted here (https://acortell3.shinyapps.io/Archaeo_riddle/).
> Archaeologists are well aware of the problems and limitations posed by its data.  Methodological and theoretical advances have been made to tackle these challenges over the years. These include a wide range of multidisciplinary approaches, where key insights from chemistry or geology, but also mathematics and data science, are employed in order to improve and transcend the current limitations posed by the archaeological record. In particular, techniques based on statistical modelling and computational simulation, have known an exponential growth during these last years. Nevertheless, the robustness of these techniques are rarely evaluated formally, and their effective ability to reconstruct the human past is unknown. The limited samples available, and the destructive process of archaeological excavation further limits our ability to properly evaluate confounding factors such as research design and post-depositional processes, all of this contributing to archaeological uncertainty.
> In short, archaeologists are constrained by the data they have, and are very rarely able to reproduce large scale studies with significant sample sizes, able to offer a data poll where the development of current formal inferential methods can be reliably applied and contrasted.
> But this awareness is not enough. If we want to take archaeological formal inference one step further, we must make sure that the methods that are currently being used and developed are effective within our specific research field. This is the key objective of archaeoriddle: to offer a virtual platform and a shared dataset where different methodologies can be compared against each other, with the unique opportunity to be able to assess their ability to reconstruct the human past.
> This collaborative enterprise will provide an unique chance to (1) help understand and compare different methods applied to archaeology, and (2) propose a general and necessary reflection on archaeological methodology, as a foundation for promoting solid and shared methodological standards.
> Since the virtual interaction process has been created virtually, we know the exact parameters behind the farming expansion. However, interested participants are given only a very limited and indirect set of data and information about what happened. Using their preferred methods, we propose a game, following ideas by Axelrod (1980) and Rendell et al. (2010), where participants have to answer a series of questions related to this virtual expansion process. In this way, archaeoriddle is thought of as a game, but which also seeks for international collaboration to understand how different current theoretical and methodological proposals behave under a controlled and testable environment.
> The project was initially presented in the past CAA (in Oxford) and, therefore, this paper aims at briefly updating the project during a 5 min talk. In this edition of the CAA, we intend to focus on how tactical simulation and collaborative research can help improving methodological approaches to archaeological uncertainty.
> 
>  References:
> Axelrod, R. (1980) “Effective choice in the Prisoner’s Dilemma”, Journal of Conflict
> Resolution, 24(1), pp. 3-25. DOI: https://doi.org/10.1177/002200278002400101
> Rendell, L., Boyd, R., Cownden, D. Enquist, M., Eriksson, K., Feldman, M. W.,
> Fogarty, L., Ghirlanda, S., Lillicrap, T., Laland, K. N. (2010). “Why copy others?
> Insights from the Social Learning Strategies tournament”, Science, 328(5975), pp.
> 208-213. DOI: http://doi.org/10.1126/science.1184719  
> 
> Created on: Mon, 31 Oct 2022 18:04:39 GMT
> 
> Last Modified: Mon, 12 Dec 2022 15:04:16 GMT
> 
> Authors:
> 
>      - ac2320@cam.ac.uk 
>      - sc2297@cam.ac.uk 
>      - lmb211@cam.ac.uk 
>      - cbs41@cam.ac.uk 
>      - jl2094@cam.ac.uk 
>      - erc62@cam.ac.uk 


### The EAA call for proposal -- August 2023

> Dear colleague,
> 
> We hope this e-mail finds you well. We are writing to you because we would like you to know on the Archaeoriddle project, a project directed to improve methods in Archaeology and Cultural Evolution. All of the information about it is here (https://theia.arch.cam.ac.uk/archaeoriddle/), but very briefly for now, in this project we have simulated an interaction process between groups of foragers and farmers and generated a "virtual" archaeological record and we now propose a game where participants will have to answer some of the questions proposed in the website with the data provided. Because we do know the actual answer, we believe this is a great way to test the inferential power of archaeological methods.
> 
> This project will come to an end in a dedicated workshop in the next EAA Conference in Belfast (https://www.e-a-a.org/EAA2023) in late August - early September, where every participant will be able to present their results and proposed methods. We will finance £650 to the best ten proposals that we receive, with a non-extendable deadline on 16th June. This money can be used to attend to the conference or in any other way the ‘winner’ would prefer. The only requisite is that the winners of the award will have to present their proposal at the workshop. Finally, after the workshop, we will invite all participants (awardees and not) to contribute on a joint publication. You can have detailed information at the link above, under the tab “How to participate?”. Please, consider sending your proposal to us, either yourself, or by transmitting this to any other potential researcher who could be interested. We’d love to have you there and chat about how can we improve archaeological methods!!
> 
> Best wishes
> Alfredo Cortell-Nicolau, Simon Carrignon, Xavier Rubio-Campillo and Enrico R. Crema
 

On top of allowing participants to download the public dataset, and to select new square to survey, the website mentioned in the call listed a series of research questions aimed to guide participants in their endeavours.

#### Research Questions

- *RQ1.*  The Poppy-chewers and Rabbit-skinners had a hostile relationship.

- *RQ2.*  Poppy-chewers followed an exponential population growth, while Rabbit-Skinners eventually stalled, and as the Poppy-chewers population size started to increase, their population declined (Fig. 3).
- *RQ3.*  The rate of dispersal varied across the landscape, with an average of ~0.62 Km/year, faster for the sea crossing (~1.11 Km/year) and a slight slow-down to ~0.57 Km/year for the Northeastern quadrant of the map.


The website where these research question will disappear anytime soon, so here is a snapshot of the first page:


But in case this disappear, the source to reproduce the website are also available in the github Archaeoriddle repository [here](https://github.com/acortell3/archaeoriddle/tree/main/doc/shinyapp).

The call was issued in April 2023, after acceptance of the workshop at EAA, and 5 proposals were submitted. 

During the actual workshop, the whole project was summarised, the 5 proposal described by there respective authors, and the exact simulation reveiled.

## Implementation

The Challenge is basically built on top of a manually selected environment, generated using the tools describe in the previous chapter, on top of which a bunch of simulation have been run, and one has been selected.

But as [Simon Carrinon](sc.fram.io) has done a poor job when he did the simulation he didn't generate seeds, nor stored them, for each experiment; so we cannot re-run the original one EXACTLY as it happened. But he kept a lot of informations and output about it which, are stored in the folder `general_results_selected_simu/`

This should be enough to allow anyone to reconstruct most of what happened. Thus, a quick recap of the implementation of the original challenge:

Using all the functions developed and described previously in this book, we :

1. Generate an environment that look cools
2. Choose a series of parameters and
3. Run hundreds of simulations and choose the one who looked the coolest 😆.
  
We used the script [scriptmini.R](https://github.com/acortell3/archaeoriddle/blob/main/doc/bookdown/scriptmini.R) to run one simulation, and then used a very simple and inelegant bash loop to parallelise muleiple run:

```bash
for t in {1..5} ; do Rscript scriptmini.R newset_$t > log_new_set$t 2> log_new_set$t.err &  done
```

One can see, on line 52 until the end of the `scriptmini.R`, that some graphes are saved. This will be used to quickly compare the different simulation and choose the good one.

The simulation are also run with the parameter: `visumin=TRUE `, which allow to save images of population growth and settlement creation at every time step while the model is running, which can then be used to generate video of the whole process using simple script like:

```bash
for i in test*_? ; do ffmpeg -i "$i/map_000%3d.png" "${i}_out.mp4" ; done
```

## Let's rewind the tape

Here we will summarise all the previous steps described in the book, but using the map, sites position and parameters used in the original Challenge.

Load the original raster:

```{r originalrast,fig.show="hold"}

original.ras <- rast("data_original/east_narnia4x.tif")
plot(original.ras, col=col_ramp(20), legend=F, reset=F)
original.wat <- original.ras
original.wat[original.wat>mean(original.wat[])] <- NA
plot(original.wat, col="lightblue", add=T, legend=F)

original.groups <- original.ras # taking back the raster created before
original.groups[original.groups<mean(original.groups[])] <- NA #replace values that are below mean level (under water)
original.groups[original.groups<(maxh*.7)] <- 1 # values below 70% of the maximum original are set as 1
original.groups[original.groups>(maxh*.7)] <- 200 # value above at 200 (high mountains)
original.groups[is.na(original.groups)] <- -1 #  NA values (below waters) are set to -1
original.poly <- as.polygons(original.groups) # convert original.groups as polygons that will be of three type: -1 => under water, 1, viable, and 200, high mountaines
original.viable <- makeValid(original.poly[2,]) # select polygon with '1' and make it a valid polygon as it may have some loop and problems
# get only the non submerged actual land and plot it
above_level <- original.ras > mean(original.ras[])
coastline <- st_as_sf(as.polygons(above_level))[2,]
plot(coastline, col=NA, bgc=adjustcolor("cyan", 0.1), add=T)
```

Adding resources, using the original coordinates, which were manually selected to correspond to interesting parts of the map 

```{r originalress,fig.show="hold"}
##ressource geolocalisation set manually
goodresources <- vect(
  cbind(
    x=c(-0.2300711, -3.1455282, -0.5086485, -1.9639755,
        -0.4077843, 0.019688, -3.116710),
    y=c(3.6423000, -0.2551019, -0.7440748, 1.1303214,
        1.0248567, 0.2194895, 2.0267718)
  )
)
#spread of resources
areas <- 4 * c(100, 2000, 200, 300, 100, 200, 400)
#speed of ressource decay:
ks <- c(0.0002, 0.0001000, 0.0001600, 0.0001800, 0.00040, .0002, 0.0002)/4

crs(goodresources) <- crs(original.ras)

original.allres <- lapply(
  seq_along(goodresources),
  function(i){
    logisticdecay(goodresources[i], mask(original.ras, original.viable),
                  x=areas[i], k=ks[i]
    )
  }
)

allna <- sapply(original.allres, function(i)any(is.na(values(i))))
original.allres <- original.allres[!allna]
original.ress <- original.allres[[1]]
for(i in 2:length(original.allres))
    original.ress <- original.ress + original.allres[[i]]
original.ress <- mask(original.ress,original.viable)
plot(original.ress)
```

This, should also exactly correspond to what is stored in `data_original/resources.tiff`

```{r comptiff,out.width="45%",fig.show="gold" }
plot(original.ress)
plot(rast("data_original/resources.tiff"))
```

And this is `Rabbithole`  home of the Poppychewer and Rabbitskinner!
We can take back the sites' positions, defined for the original Archaeoriddle and see all sites at the beginning of the simulation:

```{r rabbithole}
original.sites=vect("data_original/sitesinitialposition/")
plotMap(original.ras,original.wat,paste0("year ", 0))
points(crds(original.sites), pch=21, bg=rainbow(2, alpha=0.6)[as.factor(original.sites$culture)],col=1)
text(original.sites,pos=3)
```

And below are the original set of parameters.

```{r originalinit}

Kbase=c("HG"=35,"F"=120) #difference in K for the two cultures
# spatial penality to extent: lower, bigger penality
cul_ext <- c("HG"=7, "F"=6)
# penality of occupational area: low, other sites can come close
penal_cul <- c("HG"=4, "F"=5)
# proba to give birth every year
prob_birth <- c("HG"=0.3, "F"=0.5)
# proba to die when pop > K
prob_survive <- c("HG"=0.8, "F"=0.65)
# proba to create new settlement when Ne > K
prob_split <- c("HG"=0.2, "F"=0.6)
# how big the group of migrant should be to create a new city vs
# migrate to a existing one
minimals <- c("HG"=0.14, "F"=0.20)
# prob to migrate to existing settlement when Ne > K
prob_move <- c("HG"=0.2,"F"=0.1)
```

We can play with that, and run one simple simulation on Rabbithole:

```{r exsimu-oc,eval=T}

ts <- 100

print(paste0("Starting simulation ","nan"))

onesimu <- run_simulation(
  sites=original.sites, viable=original.viable, dem=original.ras,
  ressources=original.ress,
  water=original.wat,
  foldervid="nan",
  visu=F, visumin=F,
  ts=ts, #length of simulation in year
  Kbase=c("HG"=35, "F"=110), #difference in K for the two cultures
  cul_ext=c("HG"=7, "F"=6), #spatial penality to extent: lower, bigger penality
  penal_cul=c("HG"=4, "F"=5), #penality of occupational area: low, other sites can cam close
  prob_birth=c("HG"=0.3, "F"=0.5), #proba of giving birth every year
  prob_survive=c("HG"=0.8, "F"=0.65), #proba of dying when pop > K
  prob_split=c("HG"=0.5, "F"=0.6), #proba of creating a new settlement when Ne > K
  minimals=c("HG"=0.14,"F"=0.20), #how big the group of migrant should be to create a new city vs migrate to a existing one 
  bufferatack=300, #distance max around which settlement can fight
  prob_move=c("HG"=0.2, "F"=0.1) #proba of migrating to existing settlement when Ne > K
)
```

And explore the output:

```{r plotsimu-oc,fig.show="hold",out.width="45%"}
Nts <- onesimu$Nts # population at each timestep
warcasualties <- onesimu$warcasualties #death by war at each time step
plot(sites, cex=(1+Nts[1,]/100), pch=21, bg=rainbow(2, alpha=0.6)[as.factor(original.sites$culture)])

plot(1, 1, type="n", xlim=c(0,i), ylim=c(0,max(Nts)),
     xlab="time", ylab="popsize")
nill  <- lapply(1:ncol(Nts),
       function(i)lines(Nts[,i], col=rainbow(2)[as.factor(original.sites$culture)[i]]))
plot(apply(Nts, 1, sum)[1:i], xlab="time", ylab="popsize")

i <- nrow(Nts)  # Get the number of rows in Nts
plot(apply(Nts[1:i, original.sites$culture=="F"], 1, sum), col="red", type="l",
     lwd=2, ylim=c(0, max(apply(Nts, 1, sum))))  # Plot sum of 'F' culture values
points(apply(Nts[1:i, original.sites$culture=="HG"], 1, sum),
       col="blue", lwd=2, type="l")  # Add points for 'HG' culture values
plot(warcasualties[1:(i-1)], lwd=2, col="green", type="h", yaxt="n", ylab="")  # Plot war casualties
axis(4)  # Add an axis on the right side
par(new=T)  # Allow a new plot to be drawn on the existing plot
growF <- apply(Nts[1:(i-1), original.sites$culture=="F"], 1, sum)  # Sum of 'F' culture values excluding last row
growHG <- apply(Nts[1:(i-1), original.sites$culture=="HG"], 1, sum)  # Sum of 'HG' culture values excluding last row
plot(growF, col="red", type="l", lwd=2, ylim=c(0, max(growF, growHG)))  # Plot growth of 'F' culture
points(growHG, col="blue", lwd=2, type="l")  # Add points for growth of 'HG' culture
```

We could then apply the record deposit and the loss function as described in Chapter&nbsp;\@ref(final-output), and generate a fake archaeological record to share with others. This would give a fake archaeological record for one simulation, ran using the same initial conditions and parameters as those used during the original challenge. However, these results won't exactly replicate what occurred during the original challenge.

Unfortunately, as we briefly mentioned earlier, we can't recreate _exactly_ the same run from scratch because every interaction between settlements (migration, wars) and within settlements (birth and death) is stochastic. To replicate the same outcomes, we would need to use the same seed to ensure every random draw is identical. However, we can indeed run _multiple_ simulations, as we did in Chapter \@ref(explore-simulation), to estimate what typically happens under these conditions.

Nonetheless, even if we can't replicate it from scratch, we still have information about what happened step by step, stored in `data_original/general_results_selected_simu/`. We will leave a larger-scale exploration of these specific parameters as an exercise for those who would like to explore them in more detail during the original challenge.

### Generate and share final simulation


Let's load again the maps, matrix storing all population changes and war casualites


```{r loadressources-oc}
theoriginalchallenge=readRDS("data_original/general_results_selected_simu/buffattack300_K110_PSU065_3_all.RDS")
Nts=theoriginalchallenge$Nts
warcasualties=theoriginalchallenge$warcasualties
```

We can look again at the site at the beginning

```{r plotinitialcondition-oc,cache=F}

sites.oc=vect("data_original/sites.ocinitialposition/")
height.ras=rast("data_original/east_narnia4x.tif")
height.wat=height.ras
height.wat[height.wat>mean(height.wat[])]=NA
year=1 
plotMap(height.ras,height.wat,paste0("year ",year,", when it all begins"))
plot(sites.oc,cex=(as.integer(Nts[year,]>0)*0.5+Nts[year,]/100),pch=21,add=T,bg=rainbow(2,alpha=.6)[as.factor(sites.oc$culture)])
text(sites.oc[Nts[year,]>0],pos=3)

```

We can look at the war and the number of deaths for each battle for the first 1000 years And compare it to the total population growth of each culture:

```{r plotwar-oc}
i=1000
par(mar=c(5,5,1,5))
plot(2:i,warcasualties[1:(i-1)],lwd=2,col="dark red",type="h",yaxt="n",ylab="",xlim=c(0,i),xlab="time")
axis(4)
mtext("# dead",4,2.5)
par(new=T)
growF=apply(Nts[1:i,sites.oc$culture=="F"],1,sum)
growHG=apply(Nts[1:i,sites.oc$culture=="HG"],1,sum)
plot(growF,col="orange",type="l",lwd=5,ylim=c(0,max(growF,growHG)),xlim=c(0,i),ylab="pop",xlab="")
points(growHG,col="blue",lwd=5,type="l")
```

If we look at the total population size, we can see how the clashes slowed down the population growth:

```{r plotwarTotgrowths-oc}
par(mar=c(5,5,1,5))
plot(2:i,warcasualties[1:(i-1)],lwd=2,col="dark red",type="h",yaxt="n",ylab="",xlim=c(0,i),xlab="time")
axis(4)
mtext("# dead",4,2.5)
par(new=T)
growT=apply(Nts,1,sum)
plot(growT,col="black",type="l",lwd=5,ylim=c(0,max(growT)),xlim=c(0,i),ylab="pop",xlab="")
```

We can also look at how the growth rates change:

```{r rategrowth}
plot(diff(growT,lag=100),type="l")
```

And replaying  all of that, on the map, will give this:

```{r finalvideo,echo=F,results="asis"}
cat('<video controls> <source  src="interfinal.mp4" type="video/mp4"></video>')
```

Using `Rec_c` , `short_loss` and `long_loss` we generated deposit from the record of the run selected from the original challenge.  This takes long time when applied for all sites for all the time of simulations but can be done:


```{r doloss-oc,eval=F}

alldeposit=lapply(1:ncol(Nts),function(s)Rec_c(sapply(Nts[,s],A_rates), InitBP = 7500,ts=ncol(Nts),r = 0.2, Max_bone_thickness = "m"))
allLosses=lapply(1:length(alldeposit),function(ind){
    print(paste0("settlement #",ind));
    Rec=alldeposit[[ind]];                    
    Rec=apply(Rec,2,short_loss,.6)  #apply short loss
    Rec=long_loss(Rec,.9997,7500) #apply long loss
    return(Rec)
})
rm(alldeposit) #we suggest you to remove the deposit object, which can be large and isn't used after
```

We then limited the amount of sample available, with the assumption that if the biggest site was to be excavated as much as possible, we won't have more than 30 dates from it. Thus we randomly sampled after having normalised by this max amount.

```{r removesmall, eval=F}
maxSites=max(sapply(allLosses,sum))
nsample=round(sapply(allLosses,sum)*30/maxSites)
allRemainingDates=lapply(seq_along(allLosses),function(r)extractDates(allLosses[[r]],n=nsample[r]))
rm(allLosses) #object are big, need to free memory
dates=unique(unlist(allRemainingDates)) #some dates are NA, not sure why, but need to get rid of these
dates=rev(sort(dates[!is.na(dates)]))
totallDatesRemains=sapply(allRemainingDates,function(i)table(factor(i,levels=dates)))
saveRDS(allRemainingDates,file=paste0(expname,"_dates.RDS"))
```

Although the seed used for the sampling hasn't be saved, the output of this steps been saved and get be loaded back.

```{r getfakedates-oc}
dates.oc=readRDS("data_original/general_results_selected_simu/buffattack300_K110_PSU065_3_dates.RDS")
```

After our previous sub-sampling, some sites wont have any date  so we remove them from the list of available dates, that becomes:

```{r extractdates-oc,cache=F}
foundsites.oc=sites.oc[lengths(dates)>0,]
foundsites.oc$numdates=unlist(lengths(dates[lengths(dates)>0]))
founddates.oc=dates.oc[lengths(dates.oc)>0]
```

We then make it a bit annoying by removing BP to the dates, we add useless and meaningless different standard deviation:

```{r addstdev-oc}
stdpool=c(20,30,40,50,60,80,100,120)
founddates.oc=lapply(founddates.oc,sort)
founddates.oc=lapply(founddates.oc,gsub,pattern=" BP",replacement="")
founddates.oc=lapply(founddates.oc,rev)
founddates.oc=lapply(founddates.oc,function(i)paste0(i," ± ",sample(stdpool,length(i),replace=T,prob=c(3,3,3,3,2,2,1,1))," BP"))
foundsites.oc$dates=sapply(founddates.oc,paste0,collapse=" | ")
plotMap(height.ras,height.wat)
plot(foundsites.oc,cex=foundsites.oc$numdates/20+1,pch=21,bg=as.factor(foundsites.oc$culture),add=T)
```

Again, this is stochastic (the selection of the fake standard variation added is radom), so the exact results can't be reproduced exactly, although in that case this steps doesn't change anything, the outcome of this steps is available in the folder [data_original/all_squares/](./data_original/all_squares/).

### Grid generation and sharing

Split the environment in grids and extract date for each grid

```{r plotallfoundiste-oc,cache=F}
leftdates=dates.oc[lengths(dates.oc)>0]
plotMap(height.ras,height.wat)
plot(foundsites.oc,cex=foundsites.oc$numdates/20+.1,pch=21,bg=1,add=T)
squares=st_make_grid(height.ras,.5)
plot(squares,add=T,col=adjustcolor(rainbow(length(squares)),.35))
text(st_coordinates(st_centroid(squares)),label=1:length(squares),col="white")
text(st_coordinates(st_centroid(squares)),label=1:length(squares),col="white")
```

The original selection was :

```{r getpublicsele-oc}
plotMap(height.ras,height.wat)
plot(foundsites.oc,cex=foundsites.oc$numdates/20+1,pch=21,bg=as.factor(foundsites.oc$culture),add=T)
selection=c(14,30,45,65,66)
plot(squares[selection],add=T,col=adjustcolor("blue",.3))
inter=st_intersection(st_as_sf(foundsites.oc),squares[selection])
plotMap(height.ras,height.wat)
plot(st_geometry(inter),add=T,bg=rainbow(2,alpha=.6)[as.factor(inter$culture)],pch=21,cex=1+inter$numdates/10)
```

### Giving name to the publicly available data {#naming}

For the original challange we wanted sites to have a name for the sites publicly available. To do so we used: [fantasynamegenerators.com](https://www.fantasynamegenerators.com/fantasy-town-names.php). We manually positionned to generate some culturally relevant group depending on their geographical position, in a way slightly inspired by the reality ;  although this has no impact on the results. 

```{r namesites,cache=F}

site_dist=st_distance(inter)
#min(site_dist[as.numeric(site_dist)>units(0)])

sitesnames=1:nrow(inter)

sitesnames[c(1:4, 20)]=c("Farwallow" ,"Bearcall" ,"Dustscar" ,"Clearreach" ,"Rabbithole")

fr=c("Épibéliard" ,"Cololuçon" ,"Pulogne" ,"Haguemasse" ,"Auriteaux" ,"Bourville" ,"Banau" ,"Montnesse" ,"Bannet" ,"Alenlon", "Roullac" ,"Genneville" ,"Vinlès" ,"Antonnet" ,"Courtou" ,"Beaulogne" ,"Coloville" ,"Sarsart" ,"Soilon" ,"Cololimar")
sitesnames[5:19]=fr[1:(19-4)]
spain=c("Zava" ,"Catadrid" ,"Tegon" ,"Alicia" ,"Mulid" ,"Zararbella" ,"Malid" ,"Cásca" ,"Granalejos" ,"Segorez" ,"Terteixo" ,"Astumanca" ,"Galle" ,"Talona" ,"Girovega" ,"Albanada" ,"Nadoba" ,"Senca" ,"Vallanca" ,"Taville")

sitesnames[21:length(sitesnames)]=spain[1:(length(sitesnames)-20)]
inter=cbind(inter,sitesnames)
plotMap(height.ras,height.wat)
plot(st_geometry(inter),add=T,bg=rainbow(2,alpha=.6)[as.factor(inter$culture)],pch=21,cex=1+inter$numdates/10)
text(st_coordinates(inter),inter$sitesnames,cex=.8,pos=3)
```

We can then export `csv`s and data for each public square, and the non-public one. We here simply follow what was presented in Chapter&nbsp;\@ref(final-output#provide-survey-square-and-publicly-available-dates):

```{r exportpublic,eval=F}

inter$sitesnames=sitesnames
for(g in selection){
	curr=inter[inter$ID==g,]
	coords=st_coordinates(curr)
	write.csv(file=paste0("square_",g,".csv"),cbind.data.frame(sitename=curr$sitesnames,lon=coords[,1],lat=coords[,2],dates=curr$dates,economy=curr$culture))
}

remainingsites.oc=st_intersection(st_as_sf(foundsites.oc),squares[-selection])

for(g in (1:length(squares))[-selection]){
	curr=remainingsites.oc[remainingsites.oc$ID==g,]
	coords=st_coordinates(curr)
	write.csv(file=paste0("square_",g,".csv"),cbind.data.frame(lon=coords[,1],lat=coords[,2],dates=curr$dates,economy=curr$culture))
}

```

## The proposals

### P1 

P1 (https://github.com/dpriss/Archaeoriddle_Kahlenberg_Priss) was based on agent-based modelling (ABM) combined with exploratory data analysis. It first studied the land of Rabbithole and calibrated the dates provided, which were then used to compute trajectories of dispersal and study site preference using ArcGIS pro and R. The suggested dispersal rates over land and water were close to the Archaeoriddle solution. The proposal then used the results of its analyses to fit the ABM, which was built using NetLogo. For the ABM, moving groups of hunter–gatherers and farmers, as well as the different settlements, were treated as agents, with starting values obtained from the exploratory data analysis and relevant literature. Additionally, it introduced behavioural rules, including movement and site preference, reaction to population threshold or reaction to interaction. After running the model several times with different parameters, it correctly predicted a hostile relation between the two groups and inferred an initial location of the Rabbit-skinners in the northeastern quadrant and the Poppy-chewers in the southwestern quadrant, while also detecting the latters’ northward movement. The expansion rate was not captured by the initial assumption of logistic growth in the areas already densely inhabited by Poppy-chewers, which makes sense considering the different population trajectories of the two groups. 


### P2 

P2 (https://doi.org/10.5281/zenodo.12803445) used point-process modelling focused initially on the sampling process to build first-order models combining fitness and the available cells to predict potential occupation. It then focused on the question of whether there was conflict between the two groups. After building different archaeological phases, it computed the clustering patterns of the groups under the assumption that higher clustering could lead to higher conflict (Field, 2004). Following this, it computed the interaction distance between groups of settlements of hunter–gatherers and farmers through a multitype Strauss model. According to the results, hostilities increased over time. In the original model, the rules for hostility were not time dependent, but we have seen that, even if the rules for conflict did not change, as the population grew and the number of settlements increased, there was more probability of contact and thus more probability of conflict, which finally resulted in an increased mortality.

### P3 

P3 (https://doi.org/10.5281/zenodo.8260754) used species-distribution modelling in R to develop a four-stage research design. The first stage focused on determining which additional cells would result in a representative sample of the range variables (elevation and resource quality) present within the data. The second stage focused on data exploration to identify internal temporal, spatial and farmer and forager patterns. In the third stage, it generated summed probability distributions from the calibrated radiocarbon dates to generate relative estimates of population size through time for foragers and farmers. The fourth stage combined these data into a spatiotemporal species-distribution model, where both time and space were explicit predictors used to estimate the distribution of farmers and foragers in 100-year intervals. The spatiotemporal species distribution was successful in reproducing the directionality of the farming dispersal (from south to north) as well as the decline in hunter–gatherer populations.


### P4

P4 (https://github.com/AlexesMes/Archeaoriddle_RabbitWorld) first developed a strategy to aid in the selection of additional data using a friction calculation that considered the distance from a putative origin region, the elevation of the region and its environmental suitability. Using R, analysis focused on capturing local complexity in the dispersal of Poppy-chewers in the study area. To track this, it used a hierarchical Bayesian phase model that was informed by all the selected settlements, both the ones held in common with other proposals and the ones obtained after calculations for additional sampling. This method allowed uncertainty to be introduced. It divided Rabbithole into 25 subareas and calculated the times of arrival of the Poppy-chewers for each area, including their high-probability density intervals, resulting in a successful approach to expansion rates. 


### P5 

P5 used a qualitative approach (no code involved) that considered two cultures, A and B, initially established in regions R_A and R_B separated by a sea. Culture A eventually gained the ability to cross or bypass the barrier and to enter the region R_B and to make contact with B. Under the condition that all other parameters were equal (e.g. environmental fitness in both regions and no new pathogens or predators unknown to A), a significantly shorter settlement persistence of culture A in R_B, when compared with A’s settlement persistence assessed in other regions implies a hostile relation between A and B.
